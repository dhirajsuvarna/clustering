{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dhirajsuvarna/clustering/blob/master/notebook/Clustering_3D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7itsMS6SY8A"
   },
   "source": [
    "# Clustering on 3D Dataset \n",
    "\n",
    "This notebook is used to run the python scripts for \n",
    "1. Generate **latent vectors** using autoencoders \n",
    "1. Perform clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "sZN3vPwZSLf9",
    "outputId": "7a4d0f23-49b5-470f-e4bf-642dec0795fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fWk6POGuUUYy",
    "outputId": "f4cbad9c-4125-432a-e671-8bac7f9d4a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Projects/clustering/clustering_3d_data\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Projects/clustering/clustering_3d_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "colab_type": "code",
    "id": "9JZW8J_NUrVM",
    "outputId": "9eed628d-14fb-43d2-f0b2-bd613f3e4ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (4/4), done.\n",
      "From https://github.com/dhirajsuvarna/clustering\n",
      "   1c2eb77..8240578  master     -> origin/master\n",
      "Updating 1c2eb77..8240578\n",
      "Fast-forward\n",
      " clustering_3d_data/train.py | 1 \u001b[32m+\u001b[m\n",
      " 1 file changed, 1 insertion(+)\n"
     ]
    }
   ],
   "source": [
    "#! git clone https://github.com/dhirajsuvarna/clustering.git\n",
    "#! git config --global user.email \"dhiraj.suvarna@gmail.com\"\n",
    "#! git stash\n",
    "! git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STz5BoTeXvW5"
   },
   "source": [
    "## Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "colab_type": "code",
    "id": "svk1TrdZUsTm",
    "outputId": "b58fbcc0-260d-4cc4-bd20-e36a23dbcb81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open3d==0.9.0 in /usr/local/lib/python3.6/dist-packages (0.9.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from open3d==0.9.0) (1.18.5)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from open3d==0.9.0) (5.2.2)\n",
      "Requirement already satisfied: widgetsnbextension in /usr/local/lib/python3.6/dist-packages (from open3d==0.9.0) (3.5.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from open3d==0.9.0) (7.5.1)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (4.3.3)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (2.11.2)\n",
      "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (4.5.3)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (4.6.3)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (5.6.1)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (5.3.4)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (4.10.1)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (5.0.7)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->open3d==0.9.0) (0.2.0)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->open3d==0.9.0) (5.5.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->open3d==0.9.0) (1.12.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->open3d==0.9.0) (4.4.2)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->open3d==0.9.0) (0.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->open3d==0.9.0) (1.1.1)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (0.6.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (2.1.3)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (3.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (0.8.4)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (0.4.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->open3d==0.9.0) (1.4.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->open3d==0.9.0) (19.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->open3d==0.9.0) (2.8.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->open3d==0.9.0) (2.6.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d==0.9.0) (4.8.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d==0.9.0) (47.3.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d==0.9.0) (1.0.18)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d==0.9.0) (0.7.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d==0.9.0) (0.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->open3d==0.9.0) (20.4)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->open3d==0.9.0) (0.5.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->open3d==0.9.0) (0.2.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->notebook->open3d==0.9.0) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install open3d==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7u99rGunZGhA",
    "outputId": "c66ae572-3f02-4c50-9017-89665274cc20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /usr/local/lib/python3.6/dist-packages (1.10.0.post1)\n"
     ]
    }
   ],
   "source": [
    "! pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "48T7PgQrYrSF",
    "outputId": "278b4908-c8e6-464f-cd30-e4b42fe24ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "colab_type": "code",
    "id": "FZ3aDuT0myLf",
    "outputId": "864904ee-d21c-48a4-d625-34b67a25e99b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.12.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (47.3.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.6.0.post3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.17.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard) (1.18.5)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard) (1.6.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (2020.4.5.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.24.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-paxNdU0Z-J7"
   },
   "source": [
    "## Create Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Opg1xhCSaBAD",
    "outputId": "ece664ff-0e1d-4ca5-f444-1e0c1eb21bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Projects/pointnet/pointnet.pytorch/dataset_utils/pcd_utils\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My Drive/Projects/pointnet/pointnet.pytorch/dataset_utils/pcd_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "055OS3c3aSJc",
    "outputId": "d5c4b5c7-afc1-4094-f62f-9e5a90724537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:Namespace(ext='.pcd', input='/content/drive/My Drive/Projects/dataset/shapenet_pcd', labeled_split=False, split_percent=99)\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/airplane\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/bag\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/cap\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/car\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/chair\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/earphone\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/guitar\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/knife\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/lamp\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/laptop\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/motorbike\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/mug\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/pistol\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/rocket\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/skateboard\n",
      "Creating split of : /content/drive/My Drive/Projects/dataset/shapenet_pcd/table\n"
     ]
    }
   ],
   "source": [
    "! python create_train_test_shuffle.py --input /content/drive/My\\ Drive/Projects/dataset/shapenet_pcd --split_percent 99 --ext .pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUwOmrEdaTKN"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VGVCYaXlX1PX",
    "outputId": "2c1e2b1b-99a9-4c7c-9f21-58af4f2030c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Projects/clustering/clustering_3d_data\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/My\\ Drive/Projects/clustering/clustering_3d_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4RQfNmiT4XMJ",
    "outputId": "bc652e4f-1055-47b5-fcdd-fc339900af6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch \n",
      "import argparse\n",
      "import os\n",
      "import random\n",
      "from torch.utils.data import DataLoader\n",
      "from torch import optim\n",
      "\n",
      "from dataprep.dataset import PointCloudDataset\n",
      "from model.model import PCAutoEncoder\n",
      "from model.model_fxia22 import PointNetAE\n",
      "from torch.utils.tensorboard import SummaryWriter\n",
      "import numpy as np\n",
      "from util import pointutil\n",
      "import clustering\n",
      "import runmanager\n",
      "import json\n",
      "\n",
      "if torch.cuda.is_available():\n",
      "    from chamfer_distance.chamfer_distance_gpu import ChamferDistance # https://github.com/chrdiller/pyTorchChamferDistance\n",
      "else:\n",
      "    from chamfer_distance.chamfer_distance_cpu import ChamferDistance # https://github.com/chrdiller/pyTorchChamferDistance\n",
      "\n",
      "#########################################################################\n",
      "# SHOULD BE STRICTLY REFACTORED - this is not acceptable here\n",
      "def trimfilenames(iFileName):\n",
      "    # \"F:\\projects\\ai\\pointnet\\dataset\\DMUNet_OBJ_format\\dataset_PCD_5000\\Switch\\Switch_4.pcd\" -> \"Switch\\Switch_4.pcd\"\n",
      "    pathComps = os.path.normpath(iFileName).split(os.sep)[-2:]\n",
      "    trimPath = os.sep.join(pathComps)\n",
      "    return trimPath\n",
      "\n",
      "#########################################################################\n",
      "\n",
      "def create_network(iModelType, iNumPoints, iModelPath = ''):\n",
      "    point_dim = 3\n",
      "    if iModelType == 'dhiraj':\n",
      "        autoencoder = PCAutoEncoder(point_dim, iNumPoints)\n",
      "    elif iModelType == 'fxia':\n",
      "        autoencoder = PointNetAE(iNumPoints)\n",
      "\n",
      "    if iModelPath != '':\n",
      "        autoencoder.load_state_dict(torch.load(ip_options.load_saved_model))\n",
      "\n",
      "    # It is recommented to move the model to GPU before constructing optimizers for it. \n",
      "    # This link discusses this point in detail - https://discuss.pytorch.org/t/effect-of-calling-model-cuda-after-constructing-an-optimizer/15165/8\n",
      "    # Moving the Network model to GPU\n",
      "    autoencoder.to(device)\n",
      "    return autoencoder\n",
      "\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "\n",
      "parser.add_argument(\"--batch_size\", type=int, default=32, help=\"input batch size\")\n",
      "parser.add_argument(\"--num_points\", type=int, required=True, help=\"Number of Points to sample\")\n",
      "parser.add_argument(\"--num_workers\", type=int, default=4, help=\"Number Multiprocessing Workers\")\n",
      "parser.add_argument(\"--dataset_path\", required=True, help=\"Path to Dataset\")\n",
      "parser.add_argument(\"--nepoch\", type=int, required=True, help=\"Number of Epochs to train for\")\n",
      "parser.add_argument(\"--load_saved_model\", default='', help=\"load an saved model\")\n",
      "parser.add_argument(\"--start_epoch_from\", default=0, help=\"usually used with load model\")\n",
      "parser.add_argument(\"--model_type\", required=True, choices=['dhiraj', 'fxia'], help=\"Model Types\")\n",
      "parser.add_argument(\"--only_clustering\", action=\"store_true\", help=\"Perform only Clustering\")\n",
      "parser.add_argument(\"--latent_vector\", required=True, default=\"\", help=\"Path to saved latent vector\")\n",
      "parser.add_argument(\"--filenames\", required=True, default=\"\", help=\"Path to saved filenames vector\")\n",
      "\n",
      "\n",
      "ip_options = parser.parse_args()\n",
      "print(f\"Input Arguments : {ip_options}\")\n",
      "\n",
      "# Seed the Randomness\n",
      "manualSeed = random.randint(1, 10000)  # fix seed\n",
      "print(\"Random Seed: \", manualSeed)\n",
      "random.seed(manualSeed)\n",
      "torch.manual_seed(manualSeed) #later: \n",
      "\n",
      "# Create instance of SummaryWriter \n",
      "writer = SummaryWriter('runs/' + ip_options.model_type)\n",
      "# create folder for trained models to be saved\n",
      "os.makedirs('saved_models', exist_ok=True)\n",
      "\n",
      "# determine the device to run the network on\n",
      "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
      "\n",
      "if not ip_options.only_clustering:\n",
      "    # Creating Dataset\n",
      "    train_ds = PointCloudDataset(ip_options.dataset_path, ip_options.num_points, 'train')\n",
      "    test_ds = PointCloudDataset(ip_options.dataset_path, ip_options.num_points, 'test')\n",
      "\n",
      "    # Creating DataLoader \n",
      "    train_dl = DataLoader(train_ds, batch_size=ip_options.batch_size, shuffle=True, num_workers= ip_options.num_workers)\n",
      "    test_dl = DataLoader(test_ds, batch_size=ip_options.batch_size, shuffle=True, num_workers= ip_options.num_workers)\n",
      "\n",
      "    # Output of the dataloader is a tensor reprsenting\n",
      "    # [batch_size, num_channels, height, width]\n",
      "\n",
      "    # Creating Model\n",
      "    autoencoder = create_network(ip_options.model_type, ip_options.num_points, ip_options.load_saved_model)\n",
      "\n",
      "    # Setting up Optimizer - https://pytorch.org/docs/stable/optim.html \n",
      "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
      "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
      "\n",
      "    # create instance of Chamfer Distance Loss Instance\n",
      "    chamfer_dist = ChamferDistance()\n",
      "\n",
      "    ##########################################################################################\n",
      "    # START TRAINING OF THE NETWORK\n",
      "    print(\"Start Training\")\n",
      "    m = runmanager.RunManager(autoencoder, train_dl, writer)\n",
      "    for epoch in range(int(ip_options.start_epoch_from), ip_options.nepoch):\n",
      "        m.begin_epoch()\n",
      "        for i, data in enumerate(train_dl):\n",
      "            points = data[0]\n",
      "            filenames = list(data[1])\n",
      "\n",
      "            points = points.transpose(2, 1)\n",
      "            \n",
      "            points = points.to(device)\n",
      "\n",
      "            optimizer.zero_grad()   # Reseting the gradients\n",
      "\n",
      "            reconstructed_points, latent_vector = autoencoder(points) # perform training\n",
      "\n",
      "            points = points.transpose(1,2)\n",
      "            reconstructed_points = reconstructed_points.transpose(1,2)\n",
      "            dist1, dist2 = chamfer_dist(points, reconstructed_points)   # calculate loss\n",
      "            train_loss = (torch.mean(dist1)) + (torch.mean(dist2))\n",
      "\n",
      "            print(f\"Epoch: {epoch}, Iteration: {i}, Loss: {train_loss}\")\n",
      "            \n",
      "            train_loss.backward() # Calculate the gradients using Back Propogation\n",
      "\n",
      "            optimizer.step() # Update the weights and biases \n",
      "\n",
      "            m.track_loss(train_loss)\n",
      "\n",
      "        scheduler.step()\n",
      "        m.end_epoch()\n",
      "\n",
      "    ##########################################################################################\n",
      "    # find the best performing epoch and run the network to get latent vectors\n",
      "    print(\"Generate the Best Latent Vectors\")\n",
      "    with torch.no_grad():\n",
      "        best_latent_vector = torch.Tensor().to(device)\n",
      "        best_filenames = list()\n",
      "        autoencoder_eval = create_network(ip_options.model_type, ip_options.num_points)\n",
      "        state_dict = torch.load('saved_models/network_%d.pth' %m.best_epoch_id, map_location=device)\n",
      "        autoencoder_eval.load_state_dict(state_dict)\n",
      "        autoencoder_eval.eval() # set the network in evaluation mode\n",
      "        for itrid, data in enumerate(train_dl):\n",
      "            print(f\"Evaluating Batch: {itrid}\")\n",
      "            filenames = list(data[1])\n",
      "\n",
      "            points = data[0]\n",
      "            points = points.transpose(2, 1)        \n",
      "            points = points.to(device)\n",
      "\n",
      "            reconstructed_points, latent_vector = autoencoder_eval(points) # perform training\n",
      "            best_latent_vector = torch.cat((best_latent_vector, latent_vector), 0) \n",
      "            best_filenames.extend(filenames)\n",
      "\n",
      "        # add embedding for t-sne visualiztion\n",
      "        trimmedFiles = list(map(trimfilenames, best_filenames))\n",
      "        writer.add_embedding(best_latent_vector, metadata=trimmedFiles, global_step=m.best_epoch_id, tag=\"Latent_Vectors\")\n",
      "\n",
      "        # serialize the best latent vector\n",
      "        torch.save(best_latent_vector.detach(), 'saved_models/best_latent_vector_%d.pth' %m.best_epoch_id)\n",
      "        torch.save(best_filenames, 'saved_models/best_filenames_%d.pth' %m.best_epoch_id)\n",
      "        best_latent_vector = best_latent_vector.cpu().data.detach().numpy()\n",
      "else: \n",
      "    best_latent_vector = ip_options.latent_vector\n",
      "    best_filenames = ip_options.filenames\n",
      "\n",
      "    best_latent_vector = torch.load(best_latent_vector)\n",
      "    best_filenames = torch.load(best_filenames)\n",
      "\n",
      "#######################################################\n",
      "# PERFORM CLUSTERING \n",
      "print(\"Performing Clustering...\")\n",
      "n_clusters = 2\n",
      "clusteringAlgo = clustering.get_clusteringAlgo(\"dbscan\")\n",
      "clusteringAlgo.performClustering(best_latent_vector)\n",
      "clusteringAlgo.save()\n",
      "\n",
      "cluster_map = dict() # create a map of cluster id and filenames \n",
      "for index, label in enumerate(clusteringAlgo.algo.labels_):\n",
      "    label = \"Cluster-\" + str(label)\n",
      "    cluster_map.setdefault(label, []).append(best_filenames[index])\n",
      "\n",
      "with open(\"clustering.json\", 'w') as clusterout:\n",
      "    json.dump(cluster_map, clusterout)\n",
      "    print(\"Clusters file generated\")\n",
      "\n",
      "\n",
      "print(\"Writing to Tensorboard\")\n",
      "import open3d as o3d\n",
      "color = (0, 0, 0)\n",
      "for k, v in cluster_map.items():\n",
      "    batchedPoints = torch.Tensor()\n",
      "    batchedColors = torch.Tensor()\n",
      "    for i, filepath in enumerate(v):\n",
      "        pointCloud = o3d.io.read_point_cloud(filepath)\n",
      "        points = np.asarray(pointCloud.points)\n",
      "\n",
      "        points = pointutil.random_n_points(points, ip_options.num_points)\n",
      "        points = pointutil.normalize(points)\n",
      "\n",
      "        colors = np.full(points.shape, color)\n",
      "        points = torch.as_tensor(points)#.unsqueeze(0)\n",
      "        colors = torch.as_tensor(colors)#.unsqueeze(0)\n",
      "        if i == 0:\n",
      "            batchedPoints = points.unsqueeze(0)\n",
      "            batchedColors = colors.unsqueeze(0)\n",
      "        else:\n",
      "            batchedPoints = torch.cat((batchedPoints, points.unsqueeze(0)))\n",
      "            batchedColors = torch.cat((batchedColors, colors.unsqueeze(0)))\n",
      "        #torch.stack(colors)\n",
      "    writer.add_mesh(\"Cluster-\" + str(k), batchedPoints, batchedColors)\n",
      "\n",
      "print(\"the end.\")\n",
      "\n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! cat train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "M3lwERDmX91R",
    "outputId": "d9767d83-1bd0-41da-d107-3009bdf12307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch: 132, Iteration: 249, Loss: 0.0017618241254240274\n",
      "Epoch: 132, Iteration: 250, Loss: 0.0018679737113416195\n",
      "Epoch: 132, Iteration: 251, Loss: 0.001681943191215396\n",
      "Epoch: 132, Iteration: 252, Loss: 0.001750099123455584\n",
      "Epoch: 132, Iteration: 253, Loss: 0.0019669514149427414\n",
      "Epoch: 132, Iteration: 254, Loss: 0.0018086706986650825\n",
      "Epoch: 132, Iteration: 255, Loss: 0.0017529085744172335\n",
      "Epoch: 132, Iteration: 256, Loss: 0.0020318590104579926\n",
      "Epoch: 132, Iteration: 257, Loss: 0.0018791410839185119\n",
      "Epoch: 132, Iteration: 258, Loss: 0.0018744241679087281\n",
      "Epoch: 132, Iteration: 259, Loss: 0.001916710170917213\n",
      "Epoch: 132, Iteration: 260, Loss: 0.0018355955835431814\n",
      "Epoch: 132, Iteration: 261, Loss: 0.0019541974179446697\n",
      "Epoch: 132, Iteration: 262, Loss: 0.0018978482112288475\n",
      "Epoch: 132, Iteration: 263, Loss: 0.0018578553572297096\n",
      "Epoch: 132, Iteration: 264, Loss: 0.0017857670318335295\n",
      "Epoch: 132, Iteration: 265, Loss: 0.0017355906311422586\n",
      "Epoch: 132, Iteration: 266, Loss: 0.00202151108533144\n",
      "Epoch: 132, Iteration: 267, Loss: 0.0018447241745889187\n",
      "Epoch: 132, Iteration: 268, Loss: 0.001802057959139347\n",
      "Epoch: 132, Iteration: 269, Loss: 0.0020225895568728447\n",
      "Epoch: 132, Iteration: 270, Loss: 0.001824791543185711\n",
      "Epoch: 132, Iteration: 271, Loss: 0.0020342606585472822\n",
      "Epoch: 132, Iteration: 272, Loss: 0.0016927614342421293\n",
      "Epoch: 132, Iteration: 273, Loss: 0.001866933424025774\n",
      "Epoch: 132, Iteration: 274, Loss: 0.0017673701513558626\n",
      "Epoch: 132 Loss: 0.0018852690966348461\n",
      "Epoch: 133, Iteration: 0, Loss: 0.0020059202797710896\n",
      "Epoch: 133, Iteration: 1, Loss: 0.002055402612313628\n",
      "Epoch: 133, Iteration: 2, Loss: 0.0018257880583405495\n",
      "Epoch: 133, Iteration: 3, Loss: 0.0015937199350446463\n",
      "Epoch: 133, Iteration: 4, Loss: 0.0017735084984451532\n",
      "Epoch: 133, Iteration: 5, Loss: 0.0019115573959425092\n",
      "Epoch: 133, Iteration: 6, Loss: 0.0020907509606331587\n",
      "Epoch: 133, Iteration: 7, Loss: 0.0019214469939470291\n",
      "Epoch: 133, Iteration: 8, Loss: 0.0018772042822092772\n",
      "Epoch: 133, Iteration: 9, Loss: 0.0019391055684536695\n",
      "Epoch: 133, Iteration: 10, Loss: 0.0018046130426228046\n",
      "Epoch: 133, Iteration: 11, Loss: 0.001697817351669073\n",
      "Epoch: 133, Iteration: 12, Loss: 0.0018870161147788167\n",
      "Epoch: 133, Iteration: 13, Loss: 0.0020021810196340084\n",
      "Epoch: 133, Iteration: 14, Loss: 0.002178556751459837\n",
      "Epoch: 133, Iteration: 15, Loss: 0.0019347644411027431\n",
      "Epoch: 133, Iteration: 16, Loss: 0.0018158648163080215\n",
      "Epoch: 133, Iteration: 17, Loss: 0.0016853527631610632\n",
      "Epoch: 133, Iteration: 18, Loss: 0.0016776103293523192\n",
      "Epoch: 133, Iteration: 19, Loss: 0.0020577269606292248\n",
      "Epoch: 133, Iteration: 20, Loss: 0.0016777771525084972\n",
      "Epoch: 133, Iteration: 21, Loss: 0.0021979627199470997\n",
      "Epoch: 133, Iteration: 22, Loss: 0.0018221938516944647\n",
      "Epoch: 133, Iteration: 23, Loss: 0.0019590980373322964\n",
      "Epoch: 133, Iteration: 24, Loss: 0.0017235669074580073\n",
      "Epoch: 133, Iteration: 25, Loss: 0.0018583982018753886\n",
      "Epoch: 133, Iteration: 26, Loss: 0.0016956403851509094\n",
      "Epoch: 133, Iteration: 27, Loss: 0.001979371067136526\n",
      "Epoch: 133, Iteration: 28, Loss: 0.0018317126668989658\n",
      "Epoch: 133, Iteration: 29, Loss: 0.0017352860886603594\n",
      "Epoch: 133, Iteration: 30, Loss: 0.0018699744250625372\n",
      "Epoch: 133, Iteration: 31, Loss: 0.0016561378724873066\n",
      "Epoch: 133, Iteration: 32, Loss: 0.001908269478008151\n",
      "Epoch: 133, Iteration: 33, Loss: 0.0016962565714493394\n",
      "Epoch: 133, Iteration: 34, Loss: 0.0019103281665593386\n",
      "Epoch: 133, Iteration: 35, Loss: 0.0019664443098008633\n",
      "Epoch: 133, Iteration: 36, Loss: 0.0018347622826695442\n",
      "Epoch: 133, Iteration: 37, Loss: 0.001927034230902791\n",
      "Epoch: 133, Iteration: 38, Loss: 0.002010516356676817\n",
      "Epoch: 133, Iteration: 39, Loss: 0.0017118313116952777\n",
      "Epoch: 133, Iteration: 40, Loss: 0.0019222715636715293\n",
      "Epoch: 133, Iteration: 41, Loss: 0.0017645596526563168\n",
      "Epoch: 133, Iteration: 42, Loss: 0.0019552530720829964\n",
      "Epoch: 133, Iteration: 43, Loss: 0.0020081214606761932\n",
      "Epoch: 133, Iteration: 44, Loss: 0.0016887628007680178\n",
      "Epoch: 133, Iteration: 45, Loss: 0.0016384359914809465\n",
      "Epoch: 133, Iteration: 46, Loss: 0.0019606146961450577\n",
      "Epoch: 133, Iteration: 47, Loss: 0.0019767247140407562\n",
      "Epoch: 133, Iteration: 48, Loss: 0.002100041601806879\n",
      "Epoch: 133, Iteration: 49, Loss: 0.0019175736233592033\n",
      "Epoch: 133, Iteration: 50, Loss: 0.0018802923150360584\n",
      "Epoch: 133, Iteration: 51, Loss: 0.0019167507998645306\n",
      "Epoch: 133, Iteration: 52, Loss: 0.0017881561070680618\n",
      "Epoch: 133, Iteration: 53, Loss: 0.002085266634821892\n",
      "Epoch: 133, Iteration: 54, Loss: 0.0019425462232902646\n",
      "Epoch: 133, Iteration: 55, Loss: 0.0019854558631777763\n",
      "Epoch: 133, Iteration: 56, Loss: 0.0019759112037718296\n",
      "Epoch: 133, Iteration: 57, Loss: 0.0018811158370226622\n",
      "Epoch: 133, Iteration: 58, Loss: 0.0017094300128519535\n",
      "Epoch: 133, Iteration: 59, Loss: 0.002439430681988597\n",
      "Epoch: 133, Iteration: 60, Loss: 0.0018541730241850019\n",
      "Epoch: 133, Iteration: 61, Loss: 0.001667837263084948\n",
      "Epoch: 133, Iteration: 62, Loss: 0.001953172730281949\n",
      "Epoch: 133, Iteration: 63, Loss: 0.001951182377524674\n",
      "Epoch: 133, Iteration: 64, Loss: 0.0019365631742402911\n",
      "Epoch: 133, Iteration: 65, Loss: 0.0019036659505218267\n",
      "Epoch: 133, Iteration: 66, Loss: 0.0016663700807839632\n",
      "Epoch: 133, Iteration: 67, Loss: 0.0018404187867417932\n",
      "Epoch: 133, Iteration: 68, Loss: 0.0020537981763482094\n",
      "Epoch: 133, Iteration: 69, Loss: 0.001733284443616867\n",
      "Epoch: 133, Iteration: 70, Loss: 0.0021261300425976515\n",
      "Epoch: 133, Iteration: 71, Loss: 0.0019440281903371215\n",
      "Epoch: 133, Iteration: 72, Loss: 0.0017836993793025613\n",
      "Epoch: 133, Iteration: 73, Loss: 0.001676359330303967\n",
      "Epoch: 133, Iteration: 74, Loss: 0.002040793187916279\n",
      "Epoch: 133, Iteration: 75, Loss: 0.001992788165807724\n",
      "Epoch: 133, Iteration: 76, Loss: 0.0018970557721331716\n",
      "Epoch: 133, Iteration: 77, Loss: 0.0018682493828237057\n",
      "Epoch: 133, Iteration: 78, Loss: 0.0019893310964107513\n",
      "Epoch: 133, Iteration: 79, Loss: 0.001768193207681179\n",
      "Epoch: 133, Iteration: 80, Loss: 0.0016558649949729443\n",
      "Epoch: 133, Iteration: 81, Loss: 0.001712765428237617\n",
      "Epoch: 133, Iteration: 82, Loss: 0.0019325617467984557\n",
      "Epoch: 133, Iteration: 83, Loss: 0.0021193563006818295\n",
      "Epoch: 133, Iteration: 84, Loss: 0.001902589458040893\n",
      "Epoch: 133, Iteration: 85, Loss: 0.0018547836225479841\n",
      "Epoch: 133, Iteration: 86, Loss: 0.0017961114645004272\n",
      "Epoch: 133, Iteration: 87, Loss: 0.0018083472969010472\n",
      "Epoch: 133, Iteration: 88, Loss: 0.0017811572179198265\n",
      "Epoch: 133, Iteration: 89, Loss: 0.001781345228664577\n",
      "Epoch: 133, Iteration: 90, Loss: 0.002122791949659586\n",
      "Epoch: 133, Iteration: 91, Loss: 0.001870294683612883\n",
      "Epoch: 133, Iteration: 92, Loss: 0.0017954539507627487\n",
      "Epoch: 133, Iteration: 93, Loss: 0.0017276371363550425\n",
      "Epoch: 133, Iteration: 94, Loss: 0.0019115449395030737\n",
      "Epoch: 133, Iteration: 95, Loss: 0.002008212963119149\n",
      "Epoch: 133, Iteration: 96, Loss: 0.001642424613237381\n",
      "Epoch: 133, Iteration: 97, Loss: 0.0017205080948770046\n",
      "Epoch: 133, Iteration: 98, Loss: 0.0019483671057969332\n",
      "Epoch: 133, Iteration: 99, Loss: 0.0020527783781290054\n",
      "Epoch: 133, Iteration: 100, Loss: 0.0019451288972049952\n",
      "Epoch: 133, Iteration: 101, Loss: 0.0017915950156748295\n",
      "Epoch: 133, Iteration: 102, Loss: 0.0018666696269065142\n",
      "Epoch: 133, Iteration: 103, Loss: 0.0018174268770962954\n",
      "Epoch: 133, Iteration: 104, Loss: 0.002126565668731928\n",
      "Epoch: 133, Iteration: 105, Loss: 0.0019708306062966585\n",
      "Epoch: 133, Iteration: 106, Loss: 0.0017052657203748822\n",
      "Epoch: 133, Iteration: 107, Loss: 0.0017756285378709435\n",
      "Epoch: 133, Iteration: 108, Loss: 0.001833748072385788\n",
      "Epoch: 133, Iteration: 109, Loss: 0.0019108253763988614\n",
      "Epoch: 133, Iteration: 110, Loss: 0.0016539494972676039\n",
      "Epoch: 133, Iteration: 111, Loss: 0.0019596454221755266\n",
      "Epoch: 133, Iteration: 112, Loss: 0.0019804029725492\n",
      "Epoch: 133, Iteration: 113, Loss: 0.0017319064354524016\n",
      "Epoch: 133, Iteration: 114, Loss: 0.0020592098589986563\n",
      "Epoch: 133, Iteration: 115, Loss: 0.0019619017839431763\n",
      "Epoch: 133, Iteration: 116, Loss: 0.00202186219394207\n",
      "Epoch: 133, Iteration: 117, Loss: 0.0019333114614710212\n",
      "Epoch: 133, Iteration: 118, Loss: 0.0019675670191645622\n",
      "Epoch: 133, Iteration: 119, Loss: 0.0017858899664133787\n",
      "Epoch: 133, Iteration: 120, Loss: 0.002089437562972307\n",
      "Epoch: 133, Iteration: 121, Loss: 0.00198828405700624\n",
      "Epoch: 133, Iteration: 122, Loss: 0.0017643352039158344\n",
      "Epoch: 133, Iteration: 123, Loss: 0.0019335122779011726\n",
      "Epoch: 133, Iteration: 124, Loss: 0.0018125631613656878\n",
      "Epoch: 133, Iteration: 125, Loss: 0.0017062723636627197\n",
      "Epoch: 133, Iteration: 126, Loss: 0.0020403682719916105\n",
      "Epoch: 133, Iteration: 127, Loss: 0.001871118787676096\n",
      "Epoch: 133, Iteration: 128, Loss: 0.0017563177971169353\n",
      "Epoch: 133, Iteration: 129, Loss: 0.0018213128205388784\n",
      "Epoch: 133, Iteration: 130, Loss: 0.001740592299029231\n",
      "Epoch: 133, Iteration: 131, Loss: 0.0018259139033034444\n",
      "Epoch: 133, Iteration: 132, Loss: 0.0020232696551829576\n",
      "Epoch: 133, Iteration: 133, Loss: 0.0018340416718274355\n",
      "Epoch: 133, Iteration: 134, Loss: 0.0020281325560063124\n",
      "Epoch: 133, Iteration: 135, Loss: 0.001836699666455388\n",
      "Epoch: 133, Iteration: 136, Loss: 0.00180781539529562\n",
      "Epoch: 133, Iteration: 137, Loss: 0.0016561753582209349\n",
      "Epoch: 133, Iteration: 138, Loss: 0.0016372131649404764\n",
      "Epoch: 133, Iteration: 139, Loss: 0.0019966703839600086\n",
      "Epoch: 133, Iteration: 140, Loss: 0.002009193878620863\n",
      "Epoch: 133, Iteration: 141, Loss: 0.0019599515944719315\n",
      "Epoch: 133, Iteration: 142, Loss: 0.001807733322493732\n",
      "Epoch: 133, Iteration: 143, Loss: 0.002014552941545844\n",
      "Epoch: 133, Iteration: 144, Loss: 0.0019403441110625863\n",
      "Epoch: 133, Iteration: 145, Loss: 0.002349439077079296\n",
      "Epoch: 133, Iteration: 146, Loss: 0.0018890106584876776\n",
      "Epoch: 133, Iteration: 147, Loss: 0.0017849572468549013\n",
      "Epoch: 133, Iteration: 148, Loss: 0.001998184947296977\n",
      "Epoch: 133, Iteration: 149, Loss: 0.001842353492975235\n",
      "Epoch: 133, Iteration: 150, Loss: 0.001786085544154048\n",
      "Epoch: 133, Iteration: 151, Loss: 0.0018561894539743662\n",
      "Epoch: 133, Iteration: 152, Loss: 0.002025663387030363\n",
      "Epoch: 133, Iteration: 153, Loss: 0.002001785673201084\n",
      "Epoch: 133, Iteration: 154, Loss: 0.0023081013932824135\n",
      "Epoch: 133, Iteration: 155, Loss: 0.0019834269769489765\n",
      "Epoch: 133, Iteration: 156, Loss: 0.002060210332274437\n",
      "Epoch: 133, Iteration: 157, Loss: 0.0015088646905496716\n",
      "Epoch: 133, Iteration: 158, Loss: 0.001689668046310544\n",
      "Epoch: 133, Iteration: 159, Loss: 0.001784865977242589\n",
      "Epoch: 133, Iteration: 160, Loss: 0.001831325818784535\n",
      "Epoch: 133, Iteration: 161, Loss: 0.002001083455979824\n",
      "Epoch: 133, Iteration: 162, Loss: 0.001765428576618433\n",
      "Epoch: 133, Iteration: 163, Loss: 0.0017066081054508686\n",
      "Epoch: 133, Iteration: 164, Loss: 0.0019039245089516044\n",
      "Epoch: 133, Iteration: 165, Loss: 0.0018077971180900931\n",
      "Epoch: 133, Iteration: 166, Loss: 0.001821412006393075\n",
      "Epoch: 133, Iteration: 167, Loss: 0.001847696490585804\n",
      "Epoch: 133, Iteration: 168, Loss: 0.0019943728111684322\n",
      "Epoch: 133, Iteration: 169, Loss: 0.0020034294575452805\n",
      "Epoch: 133, Iteration: 170, Loss: 0.0019177505746483803\n",
      "Epoch: 133, Iteration: 171, Loss: 0.002036278136074543\n",
      "Epoch: 133, Iteration: 172, Loss: 0.001924476120620966\n",
      "Epoch: 133, Iteration: 173, Loss: 0.0016976636834442616\n",
      "Epoch: 133, Iteration: 174, Loss: 0.00174951390363276\n",
      "Epoch: 133, Iteration: 175, Loss: 0.001957072876393795\n",
      "Epoch: 133, Iteration: 176, Loss: 0.0019548465497791767\n",
      "Epoch: 133, Iteration: 177, Loss: 0.0019318279810249805\n",
      "Epoch: 133, Iteration: 178, Loss: 0.001716658123768866\n",
      "Epoch: 133, Iteration: 179, Loss: 0.001799649908207357\n",
      "Epoch: 133, Iteration: 180, Loss: 0.001788573688827455\n",
      "Epoch: 133, Iteration: 181, Loss: 0.0018648554105311632\n",
      "Epoch: 133, Iteration: 182, Loss: 0.0020499725360423326\n",
      "Epoch: 133, Iteration: 183, Loss: 0.0020386315882205963\n",
      "Epoch: 133, Iteration: 184, Loss: 0.002107340609654784\n",
      "Epoch: 133, Iteration: 185, Loss: 0.0020567162428051233\n",
      "Epoch: 133, Iteration: 186, Loss: 0.0016257876995950937\n",
      "Epoch: 133, Iteration: 187, Loss: 0.001865198602899909\n",
      "Epoch: 133, Iteration: 188, Loss: 0.0020860356744378805\n",
      "Epoch: 133, Iteration: 189, Loss: 0.0020324275828897953\n",
      "Epoch: 133, Iteration: 190, Loss: 0.002046794630587101\n",
      "Epoch: 133, Iteration: 191, Loss: 0.001956335734575987\n",
      "Epoch: 133, Iteration: 192, Loss: 0.0018667296972125769\n",
      "Epoch: 133, Iteration: 193, Loss: 0.0017001463565975428\n",
      "Epoch: 133, Iteration: 194, Loss: 0.001920404378324747\n",
      "Epoch: 133, Iteration: 195, Loss: 0.002069564536213875\n",
      "Epoch: 133, Iteration: 196, Loss: 0.001818438176997006\n",
      "Epoch: 133, Iteration: 197, Loss: 0.0018296571215614676\n",
      "Epoch: 133, Iteration: 198, Loss: 0.0016846172511577606\n",
      "Epoch: 133, Iteration: 199, Loss: 0.0018710177391767502\n",
      "Epoch: 133, Iteration: 200, Loss: 0.001830537337809801\n",
      "Epoch: 133, Iteration: 201, Loss: 0.0018982049077749252\n",
      "Epoch: 133, Iteration: 202, Loss: 0.001783975400030613\n",
      "Epoch: 133, Iteration: 203, Loss: 0.0018855913076549768\n",
      "Epoch: 133, Iteration: 204, Loss: 0.00174548604991287\n",
      "Epoch: 133, Iteration: 205, Loss: 0.0021987303625792265\n",
      "Epoch: 133, Iteration: 206, Loss: 0.002230672398582101\n",
      "Epoch: 133, Iteration: 207, Loss: 0.0020308629609644413\n",
      "Epoch: 133, Iteration: 208, Loss: 0.0019858358427882195\n",
      "Epoch: 133, Iteration: 209, Loss: 0.0018586735241115093\n",
      "Epoch: 133, Iteration: 210, Loss: 0.0017331368289887905\n",
      "Epoch: 133, Iteration: 211, Loss: 0.0018827758030965924\n",
      "Epoch: 133, Iteration: 212, Loss: 0.0018682109657675028\n",
      "Epoch: 133, Iteration: 213, Loss: 0.0020389726851135492\n",
      "Epoch: 133, Iteration: 214, Loss: 0.001762164058163762\n",
      "Epoch: 133, Iteration: 215, Loss: 0.0018094154074788094\n",
      "Epoch: 133, Iteration: 216, Loss: 0.001934874802827835\n",
      "Epoch: 133, Iteration: 217, Loss: 0.0017898243386298418\n",
      "Epoch: 133, Iteration: 218, Loss: 0.0021764677949249744\n",
      "Epoch: 133, Iteration: 219, Loss: 0.001924112206324935\n",
      "Epoch: 133, Iteration: 220, Loss: 0.001836680807173252\n",
      "Epoch: 133, Iteration: 221, Loss: 0.0018007673788815737\n",
      "Epoch: 133, Iteration: 222, Loss: 0.0018273573368787766\n",
      "Epoch: 133, Iteration: 223, Loss: 0.0017641580197960138\n",
      "Epoch: 133, Iteration: 224, Loss: 0.0018112515099346638\n",
      "Epoch: 133, Iteration: 225, Loss: 0.0019723589066416025\n",
      "Epoch: 133, Iteration: 226, Loss: 0.0019933192525058985\n",
      "Epoch: 133, Iteration: 227, Loss: 0.0019191296305507421\n",
      "Epoch: 133, Iteration: 228, Loss: 0.001954219304025173\n",
      "Epoch: 133, Iteration: 229, Loss: 0.0019305291352793574\n",
      "Epoch: 133, Iteration: 230, Loss: 0.0020216042175889015\n",
      "Epoch: 133, Iteration: 231, Loss: 0.0018194119911640882\n",
      "Epoch: 133, Iteration: 232, Loss: 0.0016637378139421344\n",
      "Epoch: 133, Iteration: 233, Loss: 0.002175280824303627\n",
      "Epoch: 133, Iteration: 234, Loss: 0.0018706240225583315\n",
      "Epoch: 133, Iteration: 235, Loss: 0.001559457741677761\n",
      "Epoch: 133, Iteration: 236, Loss: 0.0020332355052232742\n",
      "Epoch: 133, Iteration: 237, Loss: 0.0017532145138829947\n",
      "Epoch: 133, Iteration: 238, Loss: 0.0018388095777481794\n",
      "Epoch: 133, Iteration: 239, Loss: 0.001749902730807662\n",
      "Epoch: 133, Iteration: 240, Loss: 0.0019115593750029802\n",
      "Epoch: 133, Iteration: 241, Loss: 0.0019585303962230682\n",
      "Epoch: 133, Iteration: 242, Loss: 0.001847578096203506\n",
      "Epoch: 133, Iteration: 243, Loss: 0.0019070964772254229\n",
      "Epoch: 133, Iteration: 244, Loss: 0.0018027386395260692\n",
      "Epoch: 133, Iteration: 245, Loss: 0.0021080730948597193\n",
      "Epoch: 133, Iteration: 246, Loss: 0.0018915022956207395\n",
      "Epoch: 133, Iteration: 247, Loss: 0.0017937941011041403\n",
      "Epoch: 133, Iteration: 248, Loss: 0.0017379609635099769\n",
      "Epoch: 133, Iteration: 249, Loss: 0.0018850562628358603\n",
      "Epoch: 133, Iteration: 250, Loss: 0.001761479303240776\n",
      "Epoch: 133, Iteration: 251, Loss: 0.001820660661906004\n",
      "Epoch: 133, Iteration: 252, Loss: 0.001963632181286812\n",
      "Epoch: 133, Iteration: 253, Loss: 0.001909047132357955\n",
      "Epoch: 133, Iteration: 254, Loss: 0.0017297319136559963\n",
      "Epoch: 133, Iteration: 255, Loss: 0.0019878821913152933\n",
      "Epoch: 133, Iteration: 256, Loss: 0.0018648621626198292\n",
      "Epoch: 133, Iteration: 257, Loss: 0.0020058276131749153\n",
      "Epoch: 133, Iteration: 258, Loss: 0.001975205261260271\n",
      "Epoch: 133, Iteration: 259, Loss: 0.0019481162307783961\n",
      "Epoch: 133, Iteration: 260, Loss: 0.0020326818339526653\n",
      "Epoch: 133, Iteration: 261, Loss: 0.0017971144989132881\n",
      "Epoch: 133, Iteration: 262, Loss: 0.0019409467931836843\n",
      "Epoch: 133, Iteration: 263, Loss: 0.0016509124543517828\n",
      "Epoch: 133, Iteration: 264, Loss: 0.0015494994586333632\n",
      "Epoch: 133, Iteration: 265, Loss: 0.0019759731367230415\n",
      "Epoch: 133, Iteration: 266, Loss: 0.0019763028249144554\n",
      "Epoch: 133, Iteration: 267, Loss: 0.001862695557065308\n",
      "Epoch: 133, Iteration: 268, Loss: 0.0018683267990127206\n",
      "Epoch: 133, Iteration: 269, Loss: 0.001893026172183454\n",
      "Epoch: 133, Iteration: 270, Loss: 0.002009616931900382\n",
      "Epoch: 133, Iteration: 271, Loss: 0.0017619002610445023\n",
      "Epoch: 133, Iteration: 272, Loss: 0.0015628701075911522\n",
      "Epoch: 133, Iteration: 273, Loss: 0.0017145179444923997\n",
      "Epoch: 133, Iteration: 274, Loss: 0.0019068283727392554\n",
      "Epoch: 133 Loss: 0.0018854453108497872\n",
      "Epoch: 134, Iteration: 0, Loss: 0.0019321334548294544\n",
      "Epoch: 134, Iteration: 1, Loss: 0.001873473054729402\n",
      "Epoch: 134, Iteration: 2, Loss: 0.0019757733680307865\n",
      "Epoch: 134, Iteration: 3, Loss: 0.001944902935065329\n",
      "Epoch: 134, Iteration: 4, Loss: 0.0020608447957783937\n",
      "Epoch: 134, Iteration: 5, Loss: 0.002119761426001787\n",
      "Epoch: 134, Iteration: 6, Loss: 0.0017569740302860737\n",
      "Epoch: 134, Iteration: 7, Loss: 0.0018475763499736786\n",
      "Epoch: 134, Iteration: 8, Loss: 0.0018669790588319302\n",
      "Epoch: 134, Iteration: 9, Loss: 0.001753984484821558\n",
      "Epoch: 134, Iteration: 10, Loss: 0.00178026023786515\n",
      "Epoch: 134, Iteration: 11, Loss: 0.0018471002113074064\n",
      "Epoch: 134, Iteration: 12, Loss: 0.0018180578481405973\n",
      "Epoch: 134, Iteration: 13, Loss: 0.0017274529673159122\n",
      "Epoch: 134, Iteration: 14, Loss: 0.0018435686361044645\n",
      "Epoch: 134, Iteration: 15, Loss: 0.0019163209944963455\n",
      "Epoch: 134, Iteration: 16, Loss: 0.001988734118640423\n",
      "Epoch: 134, Iteration: 17, Loss: 0.0020516065414994955\n",
      "Epoch: 134, Iteration: 18, Loss: 0.0019322644220665097\n",
      "Epoch: 134, Iteration: 19, Loss: 0.0018012078944593668\n",
      "Epoch: 134, Iteration: 20, Loss: 0.0017404076643288136\n",
      "Epoch: 134, Iteration: 21, Loss: 0.0020137906540185213\n",
      "Epoch: 134, Iteration: 22, Loss: 0.0019946859683841467\n",
      "Epoch: 134, Iteration: 23, Loss: 0.002002747729420662\n",
      "Epoch: 134, Iteration: 24, Loss: 0.001782985171303153\n",
      "Epoch: 134, Iteration: 25, Loss: 0.0019388177897781134\n",
      "Epoch: 134, Iteration: 26, Loss: 0.0019702017307281494\n",
      "Epoch: 134, Iteration: 27, Loss: 0.0019610985182225704\n",
      "Epoch: 134, Iteration: 28, Loss: 0.0018591666594147682\n",
      "Epoch: 134, Iteration: 29, Loss: 0.001750214141793549\n",
      "Epoch: 134, Iteration: 30, Loss: 0.00216178921982646\n",
      "Epoch: 134, Iteration: 31, Loss: 0.001894687651656568\n",
      "Epoch: 134, Iteration: 32, Loss: 0.001909569837152958\n",
      "Epoch: 134, Iteration: 33, Loss: 0.0018362193368375301\n",
      "Epoch: 134, Iteration: 34, Loss: 0.0020374059677124023\n",
      "Epoch: 134, Iteration: 35, Loss: 0.0018136580474674702\n",
      "Epoch: 134, Iteration: 36, Loss: 0.0017739181639626622\n",
      "Epoch: 134, Iteration: 37, Loss: 0.0018629910191521049\n",
      "Epoch: 134, Iteration: 38, Loss: 0.0018761814571917057\n",
      "Epoch: 134, Iteration: 39, Loss: 0.0020607199985533953\n",
      "Epoch: 134, Iteration: 40, Loss: 0.0019981754012405872\n",
      "Epoch: 134, Iteration: 41, Loss: 0.002136289607733488\n",
      "Epoch: 134, Iteration: 42, Loss: 0.0018466219771653414\n",
      "Epoch: 134, Iteration: 43, Loss: 0.0018961579771712422\n",
      "Epoch: 134, Iteration: 44, Loss: 0.0018394063226878643\n",
      "Epoch: 134, Iteration: 45, Loss: 0.0017501097172498703\n",
      "Epoch: 134, Iteration: 46, Loss: 0.0019816383719444275\n",
      "Epoch: 134, Iteration: 47, Loss: 0.0019127079285681248\n",
      "Epoch: 134, Iteration: 48, Loss: 0.0017373659648001194\n",
      "Epoch: 134, Iteration: 49, Loss: 0.002193233696743846\n",
      "Epoch: 134, Iteration: 50, Loss: 0.0018954521510750055\n",
      "Epoch: 134, Iteration: 51, Loss: 0.0017086125444620848\n",
      "Epoch: 134, Iteration: 52, Loss: 0.0019547580741345882\n",
      "Epoch: 134, Iteration: 53, Loss: 0.0019579757936298847\n",
      "Epoch: 134, Iteration: 54, Loss: 0.0017504366114735603\n",
      "Epoch: 134, Iteration: 55, Loss: 0.0016434374265372753\n",
      "Epoch: 134, Iteration: 56, Loss: 0.0017928602173924446\n",
      "Epoch: 134, Iteration: 57, Loss: 0.0019194592023268342\n",
      "Epoch: 134, Iteration: 58, Loss: 0.0019304027082398534\n",
      "Epoch: 134, Iteration: 59, Loss: 0.0017172177322208881\n",
      "Epoch: 134, Iteration: 60, Loss: 0.0018875461537390947\n",
      "Epoch: 134, Iteration: 61, Loss: 0.001881869975477457\n",
      "Epoch: 134, Iteration: 62, Loss: 0.0018906628247350454\n",
      "Epoch: 134, Iteration: 63, Loss: 0.0019123991951346397\n",
      "Epoch: 134, Iteration: 64, Loss: 0.0018092432292178273\n",
      "Epoch: 134, Iteration: 65, Loss: 0.0021937943529337645\n",
      "Epoch: 134, Iteration: 66, Loss: 0.002089658286422491\n",
      "Epoch: 134, Iteration: 67, Loss: 0.0020651286467909813\n",
      "Epoch: 134, Iteration: 68, Loss: 0.0016820105956867337\n",
      "Epoch: 134, Iteration: 69, Loss: 0.001765887951478362\n",
      "Epoch: 134, Iteration: 70, Loss: 0.001897428184747696\n",
      "Epoch: 134, Iteration: 71, Loss: 0.0016905360389500856\n",
      "Epoch: 134, Iteration: 72, Loss: 0.001904892036691308\n",
      "Epoch: 134, Iteration: 73, Loss: 0.001904497854411602\n",
      "Epoch: 134, Iteration: 74, Loss: 0.001790286274626851\n",
      "Epoch: 134, Iteration: 75, Loss: 0.0020131287164986134\n",
      "Epoch: 134, Iteration: 76, Loss: 0.0018678371561691165\n",
      "Epoch: 134, Iteration: 77, Loss: 0.0016751290531829\n",
      "Epoch: 134, Iteration: 78, Loss: 0.0017952756024897099\n",
      "Epoch: 134, Iteration: 79, Loss: 0.0020594007801264524\n",
      "Epoch: 134, Iteration: 80, Loss: 0.0018158021848648787\n",
      "Epoch: 134, Iteration: 81, Loss: 0.0020162409637123346\n",
      "Epoch: 134, Iteration: 82, Loss: 0.0018842510180547833\n",
      "Epoch: 134, Iteration: 83, Loss: 0.0018544874619692564\n",
      "Epoch: 134, Iteration: 84, Loss: 0.0019266799790784717\n",
      "Epoch: 134, Iteration: 85, Loss: 0.001615531393326819\n",
      "Epoch: 134, Iteration: 86, Loss: 0.0018154422286897898\n",
      "Epoch: 134, Iteration: 87, Loss: 0.0016957784537225962\n",
      "Epoch: 134, Iteration: 88, Loss: 0.0017796388128772378\n",
      "Epoch: 134, Iteration: 89, Loss: 0.0019595345947891474\n",
      "Epoch: 134, Iteration: 90, Loss: 0.0017652441747486591\n",
      "Epoch: 134, Iteration: 91, Loss: 0.0018758834339678288\n",
      "Epoch: 134, Iteration: 92, Loss: 0.0018296174239367247\n",
      "Epoch: 134, Iteration: 93, Loss: 0.0019337745616212487\n",
      "Epoch: 134, Iteration: 94, Loss: 0.0019988385029137135\n",
      "Epoch: 134, Iteration: 95, Loss: 0.0018934737890958786\n",
      "Epoch: 134, Iteration: 96, Loss: 0.0018318123184144497\n",
      "Epoch: 134, Iteration: 97, Loss: 0.0017757564783096313\n",
      "Epoch: 134, Iteration: 98, Loss: 0.0016884123906493187\n",
      "Epoch: 134, Iteration: 99, Loss: 0.0021224934607744217\n",
      "Epoch: 134, Iteration: 100, Loss: 0.001956884516403079\n",
      "Epoch: 134, Iteration: 101, Loss: 0.001824244624003768\n",
      "Epoch: 134, Iteration: 102, Loss: 0.0016413468401879072\n",
      "Epoch: 134, Iteration: 103, Loss: 0.0017485966673120856\n",
      "Epoch: 134, Iteration: 104, Loss: 0.001983877271413803\n",
      "Epoch: 134, Iteration: 105, Loss: 0.0017765334341675043\n",
      "Epoch: 134, Iteration: 106, Loss: 0.0019284766167402267\n",
      "Epoch: 134, Iteration: 107, Loss: 0.0020072406623512506\n",
      "Epoch: 134, Iteration: 108, Loss: 0.0017958536045625806\n",
      "Epoch: 134, Iteration: 109, Loss: 0.001935463398694992\n",
      "Epoch: 134, Iteration: 110, Loss: 0.001851879176683724\n",
      "Epoch: 134, Iteration: 111, Loss: 0.0018727548886090517\n",
      "Epoch: 134, Iteration: 112, Loss: 0.001608769642189145\n",
      "Epoch: 134, Iteration: 113, Loss: 0.0018018207047134638\n",
      "Epoch: 134, Iteration: 114, Loss: 0.0018720133230090141\n",
      "Epoch: 134, Iteration: 115, Loss: 0.0017794870072975755\n",
      "Epoch: 134, Iteration: 116, Loss: 0.0019476448651403189\n",
      "Epoch: 134, Iteration: 117, Loss: 0.0018358505330979824\n",
      "Epoch: 134, Iteration: 118, Loss: 0.00186887476593256\n",
      "Epoch: 134, Iteration: 119, Loss: 0.0021606278605759144\n",
      "Epoch: 134, Iteration: 120, Loss: 0.0018349627498537302\n",
      "Epoch: 134, Iteration: 121, Loss: 0.001787892309948802\n",
      "Epoch: 134, Iteration: 122, Loss: 0.0019499484915286303\n",
      "Epoch: 134, Iteration: 123, Loss: 0.001761842519044876\n",
      "Epoch: 134, Iteration: 124, Loss: 0.0019192849285900593\n",
      "Epoch: 134, Iteration: 125, Loss: 0.0018209705594927073\n",
      "Epoch: 134, Iteration: 126, Loss: 0.001788966590538621\n",
      "Epoch: 134, Iteration: 127, Loss: 0.0018309277947992086\n",
      "Epoch: 134, Iteration: 128, Loss: 0.001932351617142558\n",
      "Epoch: 134, Iteration: 129, Loss: 0.0021277270279824734\n",
      "Epoch: 134, Iteration: 130, Loss: 0.0018208972178399563\n",
      "Epoch: 134, Iteration: 131, Loss: 0.001898346934467554\n",
      "Epoch: 134, Iteration: 132, Loss: 0.001941762981005013\n",
      "Epoch: 134, Iteration: 133, Loss: 0.0019524737726897001\n",
      "Epoch: 134, Iteration: 134, Loss: 0.0019005269277840853\n",
      "Epoch: 134, Iteration: 135, Loss: 0.001978756859898567\n",
      "Epoch: 134, Iteration: 136, Loss: 0.001691358513198793\n",
      "Epoch: 134, Iteration: 137, Loss: 0.0018476536497473717\n",
      "Epoch: 134, Iteration: 138, Loss: 0.001929802936501801\n",
      "Epoch: 134, Iteration: 139, Loss: 0.001937509747222066\n",
      "Epoch: 134, Iteration: 140, Loss: 0.002089251298457384\n",
      "Epoch: 134, Iteration: 141, Loss: 0.0018865292659029365\n",
      "Epoch: 134, Iteration: 142, Loss: 0.0018967358628287911\n",
      "Epoch: 134, Iteration: 143, Loss: 0.0020496980287134647\n",
      "Epoch: 134, Iteration: 144, Loss: 0.0020554533693939447\n",
      "Epoch: 134, Iteration: 145, Loss: 0.0019231658661738038\n",
      "Epoch: 134, Iteration: 146, Loss: 0.0017597726546227932\n",
      "Epoch: 134, Iteration: 147, Loss: 0.0018379195826128125\n",
      "Epoch: 134, Iteration: 148, Loss: 0.001969706965610385\n",
      "Epoch: 134, Iteration: 149, Loss: 0.0019528568955138326\n",
      "Epoch: 134, Iteration: 150, Loss: 0.0020887970458716154\n",
      "Epoch: 134, Iteration: 151, Loss: 0.0019014067947864532\n",
      "Epoch: 134, Iteration: 152, Loss: 0.001972715836018324\n",
      "Epoch: 134, Iteration: 153, Loss: 0.0016725757159292698\n",
      "Epoch: 134, Iteration: 154, Loss: 0.001821644720621407\n",
      "Epoch: 134, Iteration: 155, Loss: 0.0019862125627696514\n",
      "Epoch: 134, Iteration: 156, Loss: 0.0019865205977112055\n",
      "Epoch: 134, Iteration: 157, Loss: 0.001903465948998928\n",
      "Epoch: 134, Iteration: 158, Loss: 0.0017142434371635318\n",
      "Epoch: 134, Iteration: 159, Loss: 0.002011889358982444\n",
      "Epoch: 134, Iteration: 160, Loss: 0.0017013688338920474\n",
      "Epoch: 134, Iteration: 161, Loss: 0.0019257481908425689\n",
      "Epoch: 134, Iteration: 162, Loss: 0.0017491044709458947\n",
      "Epoch: 134, Iteration: 163, Loss: 0.001803779974579811\n",
      "Epoch: 134, Iteration: 164, Loss: 0.0018461646977812052\n",
      "Epoch: 134, Iteration: 165, Loss: 0.001814038958400488\n",
      "Epoch: 134, Iteration: 166, Loss: 0.0019760620780289173\n",
      "Epoch: 134, Iteration: 167, Loss: 0.0018065714975818992\n",
      "Epoch: 134, Iteration: 168, Loss: 0.0020164931192994118\n",
      "Epoch: 134, Iteration: 169, Loss: 0.0020086211152374744\n",
      "Epoch: 134, Iteration: 170, Loss: 0.0019621190149337053\n",
      "Epoch: 134, Iteration: 171, Loss: 0.0019240793772041798\n",
      "Epoch: 134, Iteration: 172, Loss: 0.0014942853013053536\n",
      "Epoch: 134, Iteration: 173, Loss: 0.001763119362294674\n",
      "Epoch: 134, Iteration: 174, Loss: 0.0018948453944176435\n",
      "Epoch: 134, Iteration: 175, Loss: 0.0018295893678441644\n",
      "Epoch: 134, Iteration: 176, Loss: 0.0022327913902699947\n",
      "Epoch: 134, Iteration: 177, Loss: 0.0018194454023614526\n",
      "Epoch: 134, Iteration: 178, Loss: 0.0017790313577279449\n",
      "Epoch: 134, Iteration: 179, Loss: 0.002005991991609335\n",
      "Epoch: 134, Iteration: 180, Loss: 0.001936306245625019\n",
      "Epoch: 134, Iteration: 181, Loss: 0.001789570553228259\n",
      "Epoch: 134, Iteration: 182, Loss: 0.0020545050501823425\n",
      "Epoch: 134, Iteration: 183, Loss: 0.0018829378532245755\n",
      "Epoch: 134, Iteration: 184, Loss: 0.0016529576387256384\n",
      "Epoch: 134, Iteration: 185, Loss: 0.0019406659994274378\n",
      "Epoch: 134, Iteration: 186, Loss: 0.0019875001162290573\n",
      "Epoch: 134, Iteration: 187, Loss: 0.0018776350189000368\n",
      "Epoch: 134, Iteration: 188, Loss: 0.0019300124840810895\n",
      "Epoch: 134, Iteration: 189, Loss: 0.0018852164503186941\n",
      "Epoch: 134, Iteration: 190, Loss: 0.0021417001262307167\n",
      "Epoch: 134, Iteration: 191, Loss: 0.001978697255253792\n",
      "Epoch: 134, Iteration: 192, Loss: 0.0018026510952040553\n",
      "Epoch: 134, Iteration: 193, Loss: 0.001972401048988104\n",
      "Epoch: 134, Iteration: 194, Loss: 0.00189644331112504\n",
      "Epoch: 134, Iteration: 195, Loss: 0.0020698602311313152\n",
      "Epoch: 134, Iteration: 196, Loss: 0.0019112166482955217\n",
      "Epoch: 134, Iteration: 197, Loss: 0.0016658224631100893\n",
      "Epoch: 134, Iteration: 198, Loss: 0.0017952953930944204\n",
      "Epoch: 134, Iteration: 199, Loss: 0.0018854693043977022\n",
      "Epoch: 134, Iteration: 200, Loss: 0.0019627753645181656\n",
      "Epoch: 134, Iteration: 201, Loss: 0.0018727665301412344\n",
      "Epoch: 134, Iteration: 202, Loss: 0.0017703105695545673\n",
      "Epoch: 134, Iteration: 203, Loss: 0.0018954253755509853\n",
      "Epoch: 134, Iteration: 204, Loss: 0.0018893179949373007\n",
      "Epoch: 134, Iteration: 205, Loss: 0.0019666445441544056\n",
      "Epoch: 134, Iteration: 206, Loss: 0.0019095756579190493\n",
      "Epoch: 134, Iteration: 207, Loss: 0.001760366722010076\n",
      "Epoch: 134, Iteration: 208, Loss: 0.001943325623869896\n",
      "Epoch: 134, Iteration: 209, Loss: 0.0017269825330004096\n",
      "Epoch: 134, Iteration: 210, Loss: 0.002224158961325884\n",
      "Epoch: 134, Iteration: 211, Loss: 0.0019047923851758242\n",
      "Epoch: 134, Iteration: 212, Loss: 0.0019449590472504497\n",
      "Epoch: 134, Iteration: 213, Loss: 0.0018242383375763893\n",
      "Epoch: 134, Iteration: 214, Loss: 0.00174048263579607\n",
      "Epoch: 134, Iteration: 215, Loss: 0.0017575389938428998\n",
      "Epoch: 134, Iteration: 216, Loss: 0.002009450690820813\n",
      "Epoch: 134, Iteration: 217, Loss: 0.0019271073397248983\n",
      "Epoch: 134, Iteration: 218, Loss: 0.0021269137505441904\n",
      "Epoch: 134, Iteration: 219, Loss: 0.0020164039451628923\n",
      "Epoch: 134, Iteration: 220, Loss: 0.002326294779777527\n",
      "Epoch: 134, Iteration: 221, Loss: 0.0019172907341271639\n",
      "Epoch: 134, Iteration: 222, Loss: 0.0017819281201809645\n",
      "Epoch: 134, Iteration: 223, Loss: 0.00184214161708951\n",
      "Epoch: 134, Iteration: 224, Loss: 0.0018596079898998141\n",
      "Epoch: 134, Iteration: 225, Loss: 0.0018860097043216228\n",
      "Epoch: 134, Iteration: 226, Loss: 0.0017601324943825603\n",
      "Epoch: 134, Iteration: 227, Loss: 0.0020519255194813013\n",
      "Epoch: 134, Iteration: 228, Loss: 0.001792387804016471\n",
      "Epoch: 134, Iteration: 229, Loss: 0.0018077751155942678\n",
      "Epoch: 134, Iteration: 230, Loss: 0.0019103772938251495\n",
      "Epoch: 134, Iteration: 231, Loss: 0.0017144214361906052\n",
      "Epoch: 134, Iteration: 232, Loss: 0.0018663245718926191\n",
      "Epoch: 134, Iteration: 233, Loss: 0.0015997608425095677\n",
      "Epoch: 134, Iteration: 234, Loss: 0.0020670085214078426\n",
      "Epoch: 134, Iteration: 235, Loss: 0.0019785736221820116\n",
      "Epoch: 134, Iteration: 236, Loss: 0.001954915001988411\n",
      "Epoch: 134, Iteration: 237, Loss: 0.0018372954800724983\n",
      "Epoch: 134, Iteration: 238, Loss: 0.0019325681496411562\n",
      "Epoch: 134, Iteration: 239, Loss: 0.0019309665076434612\n",
      "Epoch: 134, Iteration: 240, Loss: 0.0017298446036875248\n",
      "Epoch: 134, Iteration: 241, Loss: 0.0018550275126472116\n",
      "Epoch: 134, Iteration: 242, Loss: 0.0018331296741962433\n",
      "Epoch: 134, Iteration: 243, Loss: 0.0020779361948370934\n",
      "Epoch: 134, Iteration: 244, Loss: 0.0017792690778151155\n",
      "Epoch: 134, Iteration: 245, Loss: 0.0017696113791316748\n",
      "Epoch: 134, Iteration: 246, Loss: 0.0016831611283123493\n",
      "Epoch: 134, Iteration: 247, Loss: 0.0021369096357375383\n",
      "Epoch: 134, Iteration: 248, Loss: 0.001997204264625907\n",
      "Epoch: 134, Iteration: 249, Loss: 0.0019102965015918016\n",
      "Epoch: 134, Iteration: 250, Loss: 0.0021708502899855375\n",
      "Epoch: 134, Iteration: 251, Loss: 0.0018523202743381262\n",
      "Epoch: 134, Iteration: 252, Loss: 0.0017444166587665677\n",
      "Epoch: 134, Iteration: 253, Loss: 0.0018440443091094494\n",
      "Epoch: 134, Iteration: 254, Loss: 0.0021156887523829937\n",
      "Epoch: 134, Iteration: 255, Loss: 0.0018876909743994474\n",
      "Epoch: 134, Iteration: 256, Loss: 0.0016229161992669106\n",
      "Epoch: 134, Iteration: 257, Loss: 0.0017306009540334344\n",
      "Epoch: 134, Iteration: 258, Loss: 0.0019450053805485368\n",
      "Epoch: 134, Iteration: 259, Loss: 0.0018715434707701206\n",
      "Epoch: 134, Iteration: 260, Loss: 0.0017718616873025894\n",
      "Epoch: 134, Iteration: 261, Loss: 0.0019207852892577648\n",
      "Epoch: 134, Iteration: 262, Loss: 0.0016589290462434292\n",
      "Epoch: 134, Iteration: 263, Loss: 0.0018688401905819774\n",
      "Epoch: 134, Iteration: 264, Loss: 0.0018588470993563533\n",
      "Epoch: 134, Iteration: 265, Loss: 0.0018679352942854166\n",
      "Epoch: 134, Iteration: 266, Loss: 0.0017449399456381798\n",
      "Epoch: 134, Iteration: 267, Loss: 0.0018713942263275385\n",
      "Epoch: 134, Iteration: 268, Loss: 0.0018202816136181355\n",
      "Epoch: 134, Iteration: 269, Loss: 0.0017287121154367924\n",
      "Epoch: 134, Iteration: 270, Loss: 0.0017256711144000292\n",
      "Epoch: 134, Iteration: 271, Loss: 0.0017573523800820112\n",
      "Epoch: 134, Iteration: 272, Loss: 0.0020388243719935417\n",
      "Epoch: 134, Iteration: 273, Loss: 0.0021291484590619802\n",
      "Epoch: 134, Iteration: 274, Loss: 0.001778043108060956\n",
      "Epoch: 134 Loss: 0.0018858043919014326\n",
      "Epoch: 135, Iteration: 0, Loss: 0.0017513944767415524\n",
      "Epoch: 135, Iteration: 1, Loss: 0.0019543408416211605\n",
      "Epoch: 135, Iteration: 2, Loss: 0.0017145640449598432\n",
      "Epoch: 135, Iteration: 3, Loss: 0.0020110434852540493\n",
      "Epoch: 135, Iteration: 4, Loss: 0.0018255510367453098\n",
      "Epoch: 135, Iteration: 5, Loss: 0.0016450800467282534\n",
      "Epoch: 135, Iteration: 6, Loss: 0.002064047148451209\n",
      "Epoch: 135, Iteration: 7, Loss: 0.0018273560563102365\n",
      "Epoch: 135, Iteration: 8, Loss: 0.0022021266631782055\n",
      "Epoch: 135, Iteration: 9, Loss: 0.0018105783965438604\n",
      "Epoch: 135, Iteration: 10, Loss: 0.0017289725365117192\n",
      "Epoch: 135, Iteration: 11, Loss: 0.0019400293240323663\n",
      "Epoch: 135, Iteration: 12, Loss: 0.00198181439191103\n",
      "Epoch: 135, Iteration: 13, Loss: 0.002052975818514824\n",
      "Epoch: 135, Iteration: 14, Loss: 0.001963213784620166\n",
      "Epoch: 135, Iteration: 15, Loss: 0.0018515180563554168\n",
      "Epoch: 135, Iteration: 16, Loss: 0.002018793486058712\n",
      "Epoch: 135, Iteration: 17, Loss: 0.001648678444325924\n",
      "Epoch: 135, Iteration: 18, Loss: 0.0021566934883594513\n",
      "Epoch: 135, Iteration: 19, Loss: 0.0021703883539885283\n",
      "Epoch: 135, Iteration: 20, Loss: 0.0019274225924164057\n",
      "Epoch: 135, Iteration: 21, Loss: 0.001859957817941904\n",
      "Epoch: 135, Iteration: 22, Loss: 0.0018475106917321682\n",
      "Epoch: 135, Iteration: 23, Loss: 0.0018683768576011062\n",
      "Epoch: 135, Iteration: 24, Loss: 0.0018346422584727407\n",
      "Epoch: 135, Iteration: 25, Loss: 0.002021758584305644\n",
      "Epoch: 135, Iteration: 26, Loss: 0.0016880217008292675\n",
      "Epoch: 135, Iteration: 27, Loss: 0.001930803875438869\n",
      "Epoch: 135, Iteration: 28, Loss: 0.0020873267203569412\n",
      "Epoch: 135, Iteration: 29, Loss: 0.0018337771762162447\n",
      "Epoch: 135, Iteration: 30, Loss: 0.0018918734276667237\n",
      "Epoch: 135, Iteration: 31, Loss: 0.0015982715412974358\n",
      "Epoch: 135, Iteration: 32, Loss: 0.0019033881835639477\n",
      "Epoch: 135, Iteration: 33, Loss: 0.0017235238337889314\n",
      "Epoch: 135, Iteration: 34, Loss: 0.0015917050186544657\n",
      "Epoch: 135, Iteration: 35, Loss: 0.002072389703243971\n",
      "Epoch: 135, Iteration: 36, Loss: 0.001931603066623211\n",
      "Epoch: 135, Iteration: 37, Loss: 0.001963687129318714\n",
      "Epoch: 135, Iteration: 38, Loss: 0.0016861167969182134\n",
      "Epoch: 135, Iteration: 39, Loss: 0.0022593606263399124\n",
      "Epoch: 135, Iteration: 40, Loss: 0.0017098402604460716\n",
      "Epoch: 135, Iteration: 41, Loss: 0.0017958921380341053\n",
      "Epoch: 135, Iteration: 42, Loss: 0.0019326817709952593\n",
      "Epoch: 135, Iteration: 43, Loss: 0.0021546806674450636\n",
      "Epoch: 135, Iteration: 44, Loss: 0.0016929019475355744\n",
      "Epoch: 135, Iteration: 45, Loss: 0.0019193292828276753\n",
      "Epoch: 135, Iteration: 46, Loss: 0.0017517765518277884\n",
      "Epoch: 135, Iteration: 47, Loss: 0.0017022315878421068\n",
      "Epoch: 135, Iteration: 48, Loss: 0.0016653116326779127\n",
      "Epoch: 135, Iteration: 49, Loss: 0.0019136506598442793\n",
      "Epoch: 135, Iteration: 50, Loss: 0.0018062248127534986\n",
      "Epoch: 135, Iteration: 51, Loss: 0.0019276592647656798\n",
      "Epoch: 135, Iteration: 52, Loss: 0.0018581505864858627\n",
      "Epoch: 135, Iteration: 53, Loss: 0.0018984624184668064\n",
      "Epoch: 135, Iteration: 54, Loss: 0.001858214265666902\n",
      "Epoch: 135, Iteration: 55, Loss: 0.0019957188051193953\n",
      "Epoch: 135, Iteration: 56, Loss: 0.0021536583080887794\n",
      "Epoch: 135, Iteration: 57, Loss: 0.001974553568288684\n",
      "Epoch: 135, Iteration: 58, Loss: 0.0020708548836410046\n",
      "Epoch: 135, Iteration: 59, Loss: 0.0020499820820987225\n",
      "Epoch: 135, Iteration: 60, Loss: 0.0019144231919199228\n",
      "Epoch: 135, Iteration: 61, Loss: 0.0020166959147900343\n",
      "Epoch: 135, Iteration: 62, Loss: 0.001773654017597437\n",
      "Epoch: 135, Iteration: 63, Loss: 0.0018005138263106346\n",
      "Epoch: 135, Iteration: 64, Loss: 0.0020323486533015966\n",
      "Epoch: 135, Iteration: 65, Loss: 0.0016911805141717196\n",
      "Epoch: 135, Iteration: 66, Loss: 0.0018961839377880096\n",
      "Epoch: 135, Iteration: 67, Loss: 0.001757079386152327\n",
      "Epoch: 135, Iteration: 68, Loss: 0.002015448408201337\n",
      "Epoch: 135, Iteration: 69, Loss: 0.0019518674816936255\n",
      "Epoch: 135, Iteration: 70, Loss: 0.002094890456646681\n",
      "Epoch: 135, Iteration: 71, Loss: 0.0018210597336292267\n",
      "Epoch: 135, Iteration: 72, Loss: 0.0019124587997794151\n",
      "Epoch: 135, Iteration: 73, Loss: 0.0021672232542186975\n",
      "Epoch: 135, Iteration: 74, Loss: 0.0021211367566138506\n",
      "Epoch: 135, Iteration: 75, Loss: 0.0019957926124334335\n",
      "Epoch: 135, Iteration: 76, Loss: 0.0017069748137146235\n",
      "Epoch: 135, Iteration: 77, Loss: 0.002046626526862383\n",
      "Epoch: 135, Iteration: 78, Loss: 0.0018581055337563157\n",
      "Epoch: 135, Iteration: 79, Loss: 0.0019160548690706491\n",
      "Epoch: 135, Iteration: 80, Loss: 0.0018034996464848518\n",
      "Epoch: 135, Iteration: 81, Loss: 0.0018307786667719483\n",
      "Epoch: 135, Iteration: 82, Loss: 0.002014243509620428\n",
      "Epoch: 135, Iteration: 83, Loss: 0.0018336200155317783\n",
      "Epoch: 135, Iteration: 84, Loss: 0.0017663538455963135\n",
      "Epoch: 135, Iteration: 85, Loss: 0.0017898166552186012\n",
      "Epoch: 135, Iteration: 86, Loss: 0.002012819517403841\n",
      "Epoch: 135, Iteration: 87, Loss: 0.0019501398783177137\n",
      "Epoch: 135, Iteration: 88, Loss: 0.0019175470806658268\n",
      "Epoch: 135, Iteration: 89, Loss: 0.0018125659553334117\n",
      "Epoch: 135, Iteration: 90, Loss: 0.0017451094463467598\n",
      "Epoch: 135, Iteration: 91, Loss: 0.002119640354067087\n",
      "Epoch: 135, Iteration: 92, Loss: 0.0017547248862683773\n",
      "Epoch: 135, Iteration: 93, Loss: 0.001778059871867299\n",
      "Epoch: 135, Iteration: 94, Loss: 0.001996703678742051\n",
      "Epoch: 135, Iteration: 95, Loss: 0.0016390294767916203\n",
      "Epoch: 135, Iteration: 96, Loss: 0.0021337326616048813\n",
      "Epoch: 135, Iteration: 97, Loss: 0.0019755992107093334\n",
      "Epoch: 135, Iteration: 98, Loss: 0.001990159507840872\n",
      "Epoch: 135, Iteration: 99, Loss: 0.0016930530546233058\n",
      "Epoch: 135, Iteration: 100, Loss: 0.0020291325636208057\n",
      "Epoch: 135, Iteration: 101, Loss: 0.0016887399833649397\n",
      "Epoch: 135, Iteration: 102, Loss: 0.0018637939356267452\n",
      "Epoch: 135, Iteration: 103, Loss: 0.0018253730377182364\n",
      "Epoch: 135, Iteration: 104, Loss: 0.0017649726942181587\n",
      "Epoch: 135, Iteration: 105, Loss: 0.0019111649598926306\n",
      "Epoch: 135, Iteration: 106, Loss: 0.0018826317973434925\n",
      "Epoch: 135, Iteration: 107, Loss: 0.001981424866244197\n",
      "Epoch: 135, Iteration: 108, Loss: 0.0020544088911265135\n",
      "Epoch: 135, Iteration: 109, Loss: 0.0019381775055080652\n",
      "Epoch: 135, Iteration: 110, Loss: 0.002200603950768709\n",
      "Epoch: 135, Iteration: 111, Loss: 0.0018106476636603475\n",
      "Epoch: 135, Iteration: 112, Loss: 0.001667014672420919\n",
      "Epoch: 135, Iteration: 113, Loss: 0.0018191912677139044\n",
      "Epoch: 135, Iteration: 114, Loss: 0.0017249132506549358\n",
      "Epoch: 135, Iteration: 115, Loss: 0.0020799036137759686\n",
      "Epoch: 135, Iteration: 116, Loss: 0.002041335916146636\n",
      "Epoch: 135, Iteration: 117, Loss: 0.0018700052751228213\n",
      "Epoch: 135, Iteration: 118, Loss: 0.0022378615103662014\n",
      "Epoch: 135, Iteration: 119, Loss: 0.0018080630106851459\n",
      "Epoch: 135, Iteration: 120, Loss: 0.0018359131645411253\n",
      "Epoch: 135, Iteration: 121, Loss: 0.00183896254748106\n",
      "Epoch: 135, Iteration: 122, Loss: 0.001928598154336214\n",
      "Epoch: 135, Iteration: 123, Loss: 0.001856609364040196\n",
      "Epoch: 135, Iteration: 124, Loss: 0.0019956391770392656\n",
      "Epoch: 135, Iteration: 125, Loss: 0.0019051306881010532\n",
      "Epoch: 135, Iteration: 126, Loss: 0.0018930035876110196\n",
      "Epoch: 135, Iteration: 127, Loss: 0.002034455770626664\n",
      "Epoch: 135, Iteration: 128, Loss: 0.002045894041657448\n",
      "Epoch: 135, Iteration: 129, Loss: 0.0018817723030224442\n",
      "Epoch: 135, Iteration: 130, Loss: 0.0019462757045403123\n",
      "Epoch: 135, Iteration: 131, Loss: 0.0020210817456245422\n",
      "Epoch: 135, Iteration: 132, Loss: 0.0020511741749942303\n",
      "Epoch: 135, Iteration: 133, Loss: 0.002068864880129695\n",
      "Epoch: 135, Iteration: 134, Loss: 0.001977583160623908\n",
      "Epoch: 135, Iteration: 135, Loss: 0.0020514619536697865\n",
      "Epoch: 135, Iteration: 136, Loss: 0.0018173909047618508\n",
      "Epoch: 135, Iteration: 137, Loss: 0.0019260217668488622\n",
      "Epoch: 135, Iteration: 138, Loss: 0.002124377293512225\n",
      "Epoch: 135, Iteration: 139, Loss: 0.0018887098412960768\n",
      "Epoch: 135, Iteration: 140, Loss: 0.0018564328784123063\n",
      "Epoch: 135, Iteration: 141, Loss: 0.001867505139671266\n",
      "Epoch: 135, Iteration: 142, Loss: 0.00200037844479084\n",
      "Epoch: 135, Iteration: 143, Loss: 0.0018851294880732894\n",
      "Epoch: 135, Iteration: 144, Loss: 0.0018105931812897325\n",
      "Epoch: 135, Iteration: 145, Loss: 0.0018643871881067753\n",
      "Epoch: 135, Iteration: 146, Loss: 0.002076560165733099\n",
      "Epoch: 135, Iteration: 147, Loss: 0.0018933360697701573\n",
      "Epoch: 135, Iteration: 148, Loss: 0.0019100845092907548\n",
      "Epoch: 135, Iteration: 149, Loss: 0.0017960318364202976\n",
      "Epoch: 135, Iteration: 150, Loss: 0.0018276837654411793\n",
      "Epoch: 135, Iteration: 151, Loss: 0.001931430771946907\n",
      "Epoch: 135, Iteration: 152, Loss: 0.001722055021673441\n",
      "Epoch: 135, Iteration: 153, Loss: 0.0020010354928672314\n",
      "Epoch: 135, Iteration: 154, Loss: 0.0017719604074954987\n",
      "Epoch: 135, Iteration: 155, Loss: 0.001751844072714448\n",
      "Epoch: 135, Iteration: 156, Loss: 0.0018052167724817991\n",
      "Epoch: 135, Iteration: 157, Loss: 0.001843747915700078\n",
      "Epoch: 135, Iteration: 158, Loss: 0.0017275898717343807\n",
      "Epoch: 135, Iteration: 159, Loss: 0.0019440428586676717\n",
      "Epoch: 135, Iteration: 160, Loss: 0.0019837142899632454\n",
      "Epoch: 135, Iteration: 161, Loss: 0.001763892825692892\n",
      "Epoch: 135, Iteration: 162, Loss: 0.002041697269305587\n",
      "Epoch: 135, Iteration: 163, Loss: 0.0017525089206174016\n",
      "Epoch: 135, Iteration: 164, Loss: 0.0017907737055793405\n",
      "Epoch: 135, Iteration: 165, Loss: 0.0019106960389763117\n",
      "Epoch: 135, Iteration: 166, Loss: 0.0018090663943439722\n",
      "Epoch: 135, Iteration: 167, Loss: 0.0018304049735888839\n",
      "Epoch: 135, Iteration: 168, Loss: 0.0018196674063801765\n",
      "Epoch: 135, Iteration: 169, Loss: 0.0017943151760846376\n",
      "Epoch: 135, Iteration: 170, Loss: 0.0017796650063246489\n",
      "Epoch: 135, Iteration: 171, Loss: 0.0017515297513455153\n",
      "Epoch: 135, Iteration: 172, Loss: 0.0019313758239150047\n",
      "Epoch: 135, Iteration: 173, Loss: 0.001903268159367144\n",
      "Epoch: 135, Iteration: 174, Loss: 0.0017923509003594518\n",
      "Epoch: 135, Iteration: 175, Loss: 0.0015995618887245655\n",
      "Epoch: 135, Iteration: 176, Loss: 0.0018893844680860639\n",
      "Epoch: 135, Iteration: 177, Loss: 0.001893511158414185\n",
      "Epoch: 135, Iteration: 178, Loss: 0.002007822971791029\n",
      "Epoch: 135, Iteration: 179, Loss: 0.0017809343989938498\n",
      "Epoch: 135, Iteration: 180, Loss: 0.0020484155975282192\n",
      "Epoch: 135, Iteration: 181, Loss: 0.0019449216779321432\n",
      "Epoch: 135, Iteration: 182, Loss: 0.002013628836721182\n",
      "Epoch: 135, Iteration: 183, Loss: 0.001676836283877492\n",
      "Epoch: 135, Iteration: 184, Loss: 0.0020836652256548405\n",
      "Epoch: 135, Iteration: 185, Loss: 0.001903065014630556\n",
      "Epoch: 135, Iteration: 186, Loss: 0.0018544831546023488\n",
      "Epoch: 135, Iteration: 187, Loss: 0.0017412560991942883\n",
      "Epoch: 135, Iteration: 188, Loss: 0.0018721329979598522\n",
      "Epoch: 135, Iteration: 189, Loss: 0.0019310091156512499\n",
      "Epoch: 135, Iteration: 190, Loss: 0.001835727714933455\n",
      "Epoch: 135, Iteration: 191, Loss: 0.0018051278311759233\n",
      "Epoch: 135, Iteration: 192, Loss: 0.0019404420163482428\n",
      "Epoch: 135, Iteration: 193, Loss: 0.001818572636693716\n",
      "Epoch: 135, Iteration: 194, Loss: 0.002007844392210245\n",
      "Epoch: 135, Iteration: 195, Loss: 0.0017917713848873973\n",
      "Epoch: 135, Iteration: 196, Loss: 0.002035680692642927\n",
      "Epoch: 135, Iteration: 197, Loss: 0.0019526213873177767\n",
      "Epoch: 135, Iteration: 198, Loss: 0.0020651142112910748\n",
      "Epoch: 135, Iteration: 199, Loss: 0.0020138926338404417\n",
      "Epoch: 135, Iteration: 200, Loss: 0.0019697395619004965\n",
      "Epoch: 135, Iteration: 201, Loss: 0.0018171719275414944\n",
      "Epoch: 135, Iteration: 202, Loss: 0.0018792510963976383\n",
      "Epoch: 135, Iteration: 203, Loss: 0.0020118076354265213\n",
      "Epoch: 135, Iteration: 204, Loss: 0.0016699403058737516\n",
      "Epoch: 135, Iteration: 205, Loss: 0.0018802399281412363\n",
      "Epoch: 135, Iteration: 206, Loss: 0.0018512228270992637\n",
      "Epoch: 135, Iteration: 207, Loss: 0.0017773660365492105\n",
      "Epoch: 135, Iteration: 208, Loss: 0.0019908121321350336\n",
      "Epoch: 135, Iteration: 209, Loss: 0.001797714619897306\n",
      "Epoch: 135, Iteration: 210, Loss: 0.001737644081003964\n",
      "Epoch: 135, Iteration: 211, Loss: 0.00184222764801234\n",
      "Epoch: 135, Iteration: 212, Loss: 0.0019203722476959229\n",
      "Epoch: 135, Iteration: 213, Loss: 0.0016843632329255342\n",
      "Epoch: 135, Iteration: 214, Loss: 0.0016642359551042318\n",
      "Epoch: 135, Iteration: 215, Loss: 0.0020591896027326584\n",
      "Epoch: 135, Iteration: 216, Loss: 0.0019000672036781907\n",
      "Epoch: 135, Iteration: 217, Loss: 0.0017382632941007614\n",
      "Epoch: 135, Iteration: 218, Loss: 0.0017956593073904514\n",
      "Epoch: 135, Iteration: 219, Loss: 0.0019008312374353409\n",
      "Epoch: 135, Iteration: 220, Loss: 0.001714636804535985\n",
      "Epoch: 135, Iteration: 221, Loss: 0.0018784431740641594\n",
      "Epoch: 135, Iteration: 222, Loss: 0.001823420519940555\n",
      "Epoch: 135, Iteration: 223, Loss: 0.0017296408768743277\n",
      "Epoch: 135, Iteration: 224, Loss: 0.0018648637924343348\n",
      "Epoch: 135, Iteration: 225, Loss: 0.0017766115488484502\n",
      "Epoch: 135, Iteration: 226, Loss: 0.0019124677637591958\n",
      "Epoch: 135, Iteration: 227, Loss: 0.001893298584036529\n",
      "Epoch: 135, Iteration: 228, Loss: 0.0017380162607878447\n",
      "Epoch: 135, Iteration: 229, Loss: 0.001812224742025137\n",
      "Epoch: 135, Iteration: 230, Loss: 0.0018145912326872349\n",
      "Epoch: 135, Iteration: 231, Loss: 0.0018423674628138542\n",
      "Epoch: 135, Iteration: 232, Loss: 0.001962529495358467\n",
      "Epoch: 135, Iteration: 233, Loss: 0.0018326026620343328\n",
      "Epoch: 135, Iteration: 234, Loss: 0.001885739155113697\n",
      "Epoch: 135, Iteration: 235, Loss: 0.002052977913990617\n",
      "Epoch: 135, Iteration: 236, Loss: 0.0018026058096438646\n",
      "Epoch: 135, Iteration: 237, Loss: 0.001784952124580741\n",
      "Epoch: 135, Iteration: 238, Loss: 0.0019740154966712\n",
      "Epoch: 135, Iteration: 239, Loss: 0.0020218021236360073\n",
      "Epoch: 135, Iteration: 240, Loss: 0.0018198102479800582\n",
      "Epoch: 135, Iteration: 241, Loss: 0.0018982789479196072\n",
      "Epoch: 135, Iteration: 242, Loss: 0.0019483636133372784\n",
      "Epoch: 135, Iteration: 243, Loss: 0.0019676368683576584\n",
      "Epoch: 135, Iteration: 244, Loss: 0.0019945744425058365\n",
      "Epoch: 135, Iteration: 245, Loss: 0.0018360507674515247\n",
      "Epoch: 135, Iteration: 246, Loss: 0.001948007382452488\n",
      "Epoch: 135, Iteration: 247, Loss: 0.0018797770608216524\n",
      "Epoch: 135, Iteration: 248, Loss: 0.0018106943462044\n",
      "Epoch: 135, Iteration: 249, Loss: 0.001555710332468152\n",
      "Epoch: 135, Iteration: 250, Loss: 0.0019383803009986877\n",
      "Epoch: 135, Iteration: 251, Loss: 0.002092699520289898\n",
      "Epoch: 135, Iteration: 252, Loss: 0.0017491055186837912\n",
      "Epoch: 135, Iteration: 253, Loss: 0.0018585971556603909\n",
      "Epoch: 135, Iteration: 254, Loss: 0.0018017601687461138\n",
      "Epoch: 135, Iteration: 255, Loss: 0.0016342632006853819\n",
      "Epoch: 135, Iteration: 256, Loss: 0.001722637447528541\n",
      "Epoch: 135, Iteration: 257, Loss: 0.00208010571077466\n",
      "Epoch: 135, Iteration: 258, Loss: 0.0017872520256787539\n",
      "Epoch: 135, Iteration: 259, Loss: 0.0017591596115380526\n",
      "Epoch: 135, Iteration: 260, Loss: 0.001715426566079259\n",
      "Epoch: 135, Iteration: 261, Loss: 0.001609574188478291\n",
      "Epoch: 135, Iteration: 262, Loss: 0.001641552895307541\n",
      "Epoch: 135, Iteration: 263, Loss: 0.0018317033536732197\n",
      "Epoch: 135, Iteration: 264, Loss: 0.0019526469986885786\n",
      "Epoch: 135, Iteration: 265, Loss: 0.0018988054944202304\n",
      "Epoch: 135, Iteration: 266, Loss: 0.001746077323332429\n",
      "Epoch: 135, Iteration: 267, Loss: 0.0017426065169274807\n",
      "Epoch: 135, Iteration: 268, Loss: 0.001885176869109273\n",
      "Epoch: 135, Iteration: 269, Loss: 0.001550707733258605\n",
      "Epoch: 135, Iteration: 270, Loss: 0.0019448102684691548\n",
      "Epoch: 135, Iteration: 271, Loss: 0.002162632532417774\n",
      "Epoch: 135, Iteration: 272, Loss: 0.0018203394720330834\n",
      "Epoch: 135, Iteration: 273, Loss: 0.0017451968742534518\n",
      "Epoch: 135, Iteration: 274, Loss: 0.0019409472588449717\n",
      "Epoch: 135 Loss: 0.0018836324644308311\n",
      "Epoch: 136, Iteration: 0, Loss: 0.0019850386306643486\n",
      "Epoch: 136, Iteration: 1, Loss: 0.0019482204224914312\n",
      "Epoch: 136, Iteration: 2, Loss: 0.0019338004058226943\n",
      "Epoch: 136, Iteration: 3, Loss: 0.0018968279473483562\n",
      "Epoch: 136, Iteration: 4, Loss: 0.001786650624126196\n",
      "Epoch: 136, Iteration: 5, Loss: 0.001842206809669733\n",
      "Epoch: 136, Iteration: 6, Loss: 0.001778721110895276\n",
      "Epoch: 136, Iteration: 7, Loss: 0.001884049386717379\n",
      "Epoch: 136, Iteration: 8, Loss: 0.0017602585721760988\n",
      "Epoch: 136, Iteration: 9, Loss: 0.001761867431923747\n",
      "Epoch: 136, Iteration: 10, Loss: 0.0017493423074483871\n",
      "Epoch: 136, Iteration: 11, Loss: 0.0019091748399659991\n",
      "Epoch: 136, Iteration: 12, Loss: 0.001989549957215786\n",
      "Epoch: 136, Iteration: 13, Loss: 0.0019025499932467937\n",
      "Epoch: 136, Iteration: 14, Loss: 0.0018450047355145216\n",
      "Epoch: 136, Iteration: 15, Loss: 0.001813693786971271\n",
      "Epoch: 136, Iteration: 16, Loss: 0.0017585483146831393\n",
      "Epoch: 136, Iteration: 17, Loss: 0.0016661625122651458\n",
      "Epoch: 136, Iteration: 18, Loss: 0.0019517301116138697\n",
      "Epoch: 136, Iteration: 19, Loss: 0.001960945315659046\n",
      "Epoch: 136, Iteration: 20, Loss: 0.002098162192851305\n",
      "Epoch: 136, Iteration: 21, Loss: 0.001884040073491633\n",
      "Epoch: 136, Iteration: 22, Loss: 0.001874794834293425\n",
      "Epoch: 136, Iteration: 23, Loss: 0.0019393858965486288\n",
      "Epoch: 136, Iteration: 24, Loss: 0.0017089341999962926\n",
      "Epoch: 136, Iteration: 25, Loss: 0.001619685092009604\n",
      "Epoch: 136, Iteration: 26, Loss: 0.0020619966089725494\n",
      "Epoch: 136, Iteration: 27, Loss: 0.002128873486071825\n",
      "Epoch: 136, Iteration: 28, Loss: 0.0018053140956908464\n",
      "Epoch: 136, Iteration: 29, Loss: 0.0017416583141312003\n",
      "Epoch: 136, Iteration: 30, Loss: 0.0018605223158374429\n",
      "Epoch: 136, Iteration: 31, Loss: 0.002078396501019597\n",
      "Epoch: 136, Iteration: 32, Loss: 0.0016945598181337118\n",
      "Epoch: 136, Iteration: 33, Loss: 0.0017753709107637405\n",
      "Epoch: 136, Iteration: 34, Loss: 0.0020981926936656237\n",
      "Epoch: 136, Iteration: 35, Loss: 0.0019508620025590062\n",
      "Epoch: 136, Iteration: 36, Loss: 0.0019361143931746483\n",
      "Epoch: 136, Iteration: 37, Loss: 0.0017144749872386456\n",
      "Epoch: 136, Iteration: 38, Loss: 0.0018840683624148369\n",
      "Epoch: 136, Iteration: 39, Loss: 0.0019248910248279572\n",
      "Epoch: 136, Iteration: 40, Loss: 0.0019487662939354777\n",
      "Epoch: 136, Iteration: 41, Loss: 0.001739506609737873\n",
      "Epoch: 136, Iteration: 42, Loss: 0.0018840412376448512\n",
      "Epoch: 136, Iteration: 43, Loss: 0.0022562018129974604\n",
      "Epoch: 136, Iteration: 44, Loss: 0.0017915737116709352\n",
      "Epoch: 136, Iteration: 45, Loss: 0.0021795521024614573\n",
      "Epoch: 136, Iteration: 46, Loss: 0.0017287733498960733\n",
      "Epoch: 136, Iteration: 47, Loss: 0.001844153506681323\n",
      "Epoch: 136, Iteration: 48, Loss: 0.002069668611511588\n",
      "Epoch: 136, Iteration: 49, Loss: 0.002194995991885662\n",
      "Epoch: 136, Iteration: 50, Loss: 0.001790334819816053\n",
      "Epoch: 136, Iteration: 51, Loss: 0.0018477130215615034\n",
      "Epoch: 136, Iteration: 52, Loss: 0.002214079722762108\n",
      "Epoch: 136, Iteration: 53, Loss: 0.001997158396989107\n",
      "Epoch: 136, Iteration: 54, Loss: 0.0018943601753562689\n",
      "Epoch: 136, Iteration: 55, Loss: 0.0018262278754264116\n",
      "Epoch: 136, Iteration: 56, Loss: 0.001772425719536841\n",
      "Epoch: 136, Iteration: 57, Loss: 0.0017882221145555377\n",
      "Epoch: 136, Iteration: 58, Loss: 0.0017676997231319547\n",
      "Epoch: 136, Iteration: 59, Loss: 0.00179435801692307\n",
      "Epoch: 136, Iteration: 60, Loss: 0.0022319708950817585\n",
      "Epoch: 136, Iteration: 61, Loss: 0.001900192117318511\n",
      "Epoch: 136, Iteration: 62, Loss: 0.0019626179710030556\n",
      "Epoch: 136, Iteration: 63, Loss: 0.0021011503413319588\n",
      "Epoch: 136, Iteration: 64, Loss: 0.0016893683932721615\n",
      "Epoch: 136, Iteration: 65, Loss: 0.001860767020843923\n",
      "Epoch: 136, Iteration: 66, Loss: 0.0017753661377355456\n",
      "Epoch: 136, Iteration: 67, Loss: 0.001961321569979191\n",
      "Epoch: 136, Iteration: 68, Loss: 0.0018756540957838297\n",
      "Epoch: 136, Iteration: 69, Loss: 0.001860974240116775\n",
      "Epoch: 136, Iteration: 70, Loss: 0.00186817382927984\n",
      "Epoch: 136, Iteration: 71, Loss: 0.00171191047411412\n",
      "Epoch: 136, Iteration: 72, Loss: 0.0017829289427027106\n",
      "Epoch: 136, Iteration: 73, Loss: 0.0020334278233349323\n",
      "Epoch: 136, Iteration: 74, Loss: 0.0019190245075151324\n",
      "Epoch: 136, Iteration: 75, Loss: 0.0017239185981452465\n",
      "Epoch: 136, Iteration: 76, Loss: 0.0019075690070167184\n",
      "Epoch: 136, Iteration: 77, Loss: 0.0017713448032736778\n",
      "Epoch: 136, Iteration: 78, Loss: 0.002058416372165084\n",
      "Epoch: 136, Iteration: 79, Loss: 0.0020012001041322947\n",
      "Epoch: 136, Iteration: 80, Loss: 0.0018615935696288943\n",
      "Epoch: 136, Iteration: 81, Loss: 0.0019384965999051929\n",
      "Epoch: 136, Iteration: 82, Loss: 0.0020286813378334045\n",
      "Epoch: 136, Iteration: 83, Loss: 0.0017453415784984827\n",
      "Epoch: 136, Iteration: 84, Loss: 0.0017800349742174149\n",
      "Epoch: 136, Iteration: 85, Loss: 0.0017830205615609884\n",
      "Epoch: 136, Iteration: 86, Loss: 0.002000123029574752\n",
      "Epoch: 136, Iteration: 87, Loss: 0.002024461980909109\n",
      "Epoch: 136, Iteration: 88, Loss: 0.001865142141468823\n",
      "Epoch: 136, Iteration: 89, Loss: 0.0020100106485188007\n",
      "Epoch: 136, Iteration: 90, Loss: 0.0018966541392728686\n",
      "Epoch: 136, Iteration: 91, Loss: 0.0020984166767448187\n",
      "Epoch: 136, Iteration: 92, Loss: 0.0020378504414111376\n",
      "Epoch: 136, Iteration: 93, Loss: 0.0020291085820645094\n",
      "Epoch: 136, Iteration: 94, Loss: 0.0020361733622848988\n",
      "Epoch: 136, Iteration: 95, Loss: 0.0019194942433387041\n",
      "Epoch: 136, Iteration: 96, Loss: 0.0017709422390908003\n",
      "Epoch: 136, Iteration: 97, Loss: 0.0020137308165431023\n",
      "Epoch: 136, Iteration: 98, Loss: 0.0019098439952358603\n",
      "Epoch: 136, Iteration: 99, Loss: 0.0017292576376348734\n",
      "Epoch: 136, Iteration: 100, Loss: 0.0018517778953537345\n",
      "Epoch: 136, Iteration: 101, Loss: 0.0020350245758891106\n",
      "Epoch: 136, Iteration: 102, Loss: 0.0018881859723478556\n",
      "Epoch: 136, Iteration: 103, Loss: 0.0017008354188874364\n",
      "Epoch: 136, Iteration: 104, Loss: 0.002108275191858411\n",
      "Epoch: 136, Iteration: 105, Loss: 0.0021430333144962788\n",
      "Epoch: 136, Iteration: 106, Loss: 0.0018406739691272378\n",
      "Epoch: 136, Iteration: 107, Loss: 0.002007657429203391\n",
      "Epoch: 136, Iteration: 108, Loss: 0.001833850983530283\n",
      "Epoch: 136, Iteration: 109, Loss: 0.0018846712773665786\n",
      "Epoch: 136, Iteration: 110, Loss: 0.0017225401243194938\n",
      "Epoch: 136, Iteration: 111, Loss: 0.0019327012123540044\n",
      "Epoch: 136, Iteration: 112, Loss: 0.001810834975913167\n",
      "Epoch: 136, Iteration: 113, Loss: 0.0017047509318217635\n",
      "Epoch: 136, Iteration: 114, Loss: 0.0019177880603820086\n",
      "Epoch: 136, Iteration: 115, Loss: 0.001944481860846281\n",
      "Epoch: 136, Iteration: 116, Loss: 0.0018864464946091175\n",
      "Epoch: 136, Iteration: 117, Loss: 0.0021476198453456163\n",
      "Epoch: 136, Iteration: 118, Loss: 0.0016039859037846327\n",
      "Epoch: 136, Iteration: 119, Loss: 0.0018597835442051291\n",
      "Epoch: 136, Iteration: 120, Loss: 0.0023644433822482824\n",
      "Epoch: 136, Iteration: 121, Loss: 0.0023566195741295815\n",
      "Epoch: 136, Iteration: 122, Loss: 0.0019261817215010524\n",
      "Epoch: 136, Iteration: 123, Loss: 0.0016650002216920257\n",
      "Epoch: 136, Iteration: 124, Loss: 0.0017286096699535847\n",
      "Epoch: 136, Iteration: 125, Loss: 0.0019681951962411404\n",
      "Epoch: 136, Iteration: 126, Loss: 0.0018915594555437565\n",
      "Epoch: 136, Iteration: 127, Loss: 0.001762236701324582\n",
      "Epoch: 136, Iteration: 128, Loss: 0.001719063613563776\n",
      "Epoch: 136, Iteration: 129, Loss: 0.0018165974179282784\n",
      "Epoch: 136, Iteration: 130, Loss: 0.0019327605841681361\n",
      "Epoch: 136, Iteration: 131, Loss: 0.0015478582354262471\n",
      "Epoch: 136, Iteration: 132, Loss: 0.0018827901221811771\n",
      "Epoch: 136, Iteration: 133, Loss: 0.001882623415440321\n",
      "Epoch: 136, Iteration: 134, Loss: 0.0019062506034970284\n",
      "Epoch: 136, Iteration: 135, Loss: 0.0018645089585334063\n",
      "Epoch: 136, Iteration: 136, Loss: 0.0018435297533869743\n",
      "Epoch: 136, Iteration: 137, Loss: 0.002084689447656274\n",
      "Epoch: 136, Iteration: 138, Loss: 0.0019467114470899105\n",
      "Epoch: 136, Iteration: 139, Loss: 0.001837269403040409\n",
      "Epoch: 136, Iteration: 140, Loss: 0.0018327392172068357\n",
      "Epoch: 136, Iteration: 141, Loss: 0.0020100956317037344\n",
      "Epoch: 136, Iteration: 142, Loss: 0.002182130003347993\n",
      "Epoch: 136, Iteration: 143, Loss: 0.001699721673503518\n",
      "Epoch: 136, Iteration: 144, Loss: 0.002195457462221384\n",
      "Epoch: 136, Iteration: 145, Loss: 0.0018189246766269207\n",
      "Epoch: 136, Iteration: 146, Loss: 0.0017964186845347285\n",
      "Epoch: 136, Iteration: 147, Loss: 0.0018423188012093306\n",
      "Epoch: 136, Iteration: 148, Loss: 0.0018403786234557629\n",
      "Epoch: 136, Iteration: 149, Loss: 0.0018843425204977393\n",
      "Epoch: 136, Iteration: 150, Loss: 0.00165572389960289\n",
      "Epoch: 136, Iteration: 151, Loss: 0.0018595521105453372\n",
      "Epoch: 136, Iteration: 152, Loss: 0.001615528017282486\n",
      "Epoch: 136, Iteration: 153, Loss: 0.001645783893764019\n",
      "Epoch: 136, Iteration: 154, Loss: 0.0019330274080857635\n",
      "Epoch: 136, Iteration: 155, Loss: 0.0018075653351843357\n",
      "Epoch: 136, Iteration: 156, Loss: 0.001812484348192811\n",
      "Epoch: 136, Iteration: 157, Loss: 0.0019590517040342093\n",
      "Epoch: 136, Iteration: 158, Loss: 0.0019015239086002111\n",
      "Epoch: 136, Iteration: 159, Loss: 0.0018246255349367857\n",
      "Epoch: 136, Iteration: 160, Loss: 0.0019860544707626104\n",
      "Epoch: 136, Iteration: 161, Loss: 0.0018062782473862171\n",
      "Epoch: 136, Iteration: 162, Loss: 0.0018565491773188114\n",
      "Epoch: 136, Iteration: 163, Loss: 0.0018786757718771696\n",
      "Epoch: 136, Iteration: 164, Loss: 0.0021958649158477783\n",
      "Epoch: 136, Iteration: 165, Loss: 0.0019384387414902449\n",
      "Epoch: 136, Iteration: 166, Loss: 0.001682181260548532\n",
      "Epoch: 136, Iteration: 167, Loss: 0.0018091517267748713\n",
      "Epoch: 136, Iteration: 168, Loss: 0.0016813082620501518\n",
      "Epoch: 136, Iteration: 169, Loss: 0.0018386656884104013\n",
      "Epoch: 136, Iteration: 170, Loss: 0.001866980455815792\n",
      "Epoch: 136, Iteration: 171, Loss: 0.001930256374180317\n",
      "Epoch: 136, Iteration: 172, Loss: 0.001983573194593191\n",
      "Epoch: 136, Iteration: 173, Loss: 0.002089659683406353\n",
      "Epoch: 136, Iteration: 174, Loss: 0.0017143861623480916\n",
      "Epoch: 136, Iteration: 175, Loss: 0.0020651835948228836\n",
      "Epoch: 136, Iteration: 176, Loss: 0.0019597052596509457\n",
      "Epoch: 136, Iteration: 177, Loss: 0.0017601691652089357\n",
      "Epoch: 136, Iteration: 178, Loss: 0.001955590443685651\n",
      "Epoch: 136, Iteration: 179, Loss: 0.001898736460134387\n",
      "Epoch: 136, Iteration: 180, Loss: 0.0018377848900854588\n",
      "Epoch: 136, Iteration: 181, Loss: 0.0016883404459804296\n",
      "Epoch: 136, Iteration: 182, Loss: 0.001973887672647834\n",
      "Epoch: 136, Iteration: 183, Loss: 0.001922661205753684\n",
      "Epoch: 136, Iteration: 184, Loss: 0.001651195459999144\n",
      "Epoch: 136, Iteration: 185, Loss: 0.0018364700954407454\n",
      "Epoch: 136, Iteration: 186, Loss: 0.0018498592544347048\n",
      "Epoch: 136, Iteration: 187, Loss: 0.001808449625968933\n",
      "Epoch: 136, Iteration: 188, Loss: 0.001954927807673812\n",
      "Epoch: 136, Iteration: 189, Loss: 0.002151443623006344\n",
      "Epoch: 136, Iteration: 190, Loss: 0.001790229813195765\n",
      "Epoch: 136, Iteration: 191, Loss: 0.0017925926949828863\n",
      "Epoch: 136, Iteration: 192, Loss: 0.0019875571597367525\n",
      "Epoch: 136, Iteration: 193, Loss: 0.0019668303430080414\n",
      "Epoch: 136, Iteration: 194, Loss: 0.0017483750125393271\n",
      "Epoch: 136, Iteration: 195, Loss: 0.0018283696845173836\n",
      "Epoch: 136, Iteration: 196, Loss: 0.0019931369461119175\n",
      "Epoch: 136, Iteration: 197, Loss: 0.0018586096120998263\n",
      "Epoch: 136, Iteration: 198, Loss: 0.001658329856581986\n",
      "Epoch: 136, Iteration: 199, Loss: 0.0017911698669195175\n",
      "Epoch: 136, Iteration: 200, Loss: 0.0019395853159949183\n",
      "Epoch: 136, Iteration: 201, Loss: 0.001854093512520194\n",
      "Epoch: 136, Iteration: 202, Loss: 0.0020172037184238434\n",
      "Epoch: 136, Iteration: 203, Loss: 0.0015646922402083874\n",
      "Epoch: 136, Iteration: 204, Loss: 0.0018179004546254873\n",
      "Epoch: 136, Iteration: 205, Loss: 0.0019991304725408554\n",
      "Epoch: 136, Iteration: 206, Loss: 0.001681649824604392\n",
      "Epoch: 136, Iteration: 207, Loss: 0.001980866538360715\n",
      "Epoch: 136, Iteration: 208, Loss: 0.0018957310821861029\n",
      "Epoch: 136, Iteration: 209, Loss: 0.0020754863508045673\n",
      "Epoch: 136, Iteration: 210, Loss: 0.0018273352179676294\n",
      "Epoch: 136, Iteration: 211, Loss: 0.0019878577440977097\n",
      "Epoch: 136, Iteration: 212, Loss: 0.001628623460419476\n",
      "Epoch: 136, Iteration: 213, Loss: 0.0017520328983664513\n",
      "Epoch: 136, Iteration: 214, Loss: 0.0016530302818864584\n",
      "Epoch: 136, Iteration: 215, Loss: 0.001970290904864669\n",
      "Epoch: 136, Iteration: 216, Loss: 0.0020226987544447184\n",
      "Epoch: 136, Iteration: 217, Loss: 0.0017944567371159792\n",
      "Epoch: 136, Iteration: 218, Loss: 0.0020955195650458336\n",
      "Epoch: 136, Iteration: 219, Loss: 0.0018913253443315625\n",
      "Epoch: 136, Iteration: 220, Loss: 0.0016439585015177727\n",
      "Epoch: 136, Iteration: 221, Loss: 0.0015982023905962706\n",
      "Epoch: 136, Iteration: 222, Loss: 0.0020685731433331966\n",
      "Epoch: 136, Iteration: 223, Loss: 0.0019727125763893127\n",
      "Epoch: 136, Iteration: 224, Loss: 0.002073778538033366\n",
      "Epoch: 136, Iteration: 225, Loss: 0.0019236997468397021\n",
      "Epoch: 136, Iteration: 226, Loss: 0.0020072124898433685\n",
      "Epoch: 136, Iteration: 227, Loss: 0.0016526104882359505\n",
      "Epoch: 136, Iteration: 228, Loss: 0.0020355558954179287\n",
      "Epoch: 136, Iteration: 229, Loss: 0.0019193151965737343\n",
      "Epoch: 136, Iteration: 230, Loss: 0.0018354554194957018\n",
      "Epoch: 136, Iteration: 231, Loss: 0.001985021401196718\n",
      "Epoch: 136, Iteration: 232, Loss: 0.0017435478512197733\n",
      "Epoch: 136, Iteration: 233, Loss: 0.001820215256884694\n",
      "Epoch: 136, Iteration: 234, Loss: 0.0019063735380768776\n",
      "Epoch: 136, Iteration: 235, Loss: 0.001895975787192583\n",
      "Epoch: 136, Iteration: 236, Loss: 0.002035797806456685\n",
      "Epoch: 136, Iteration: 237, Loss: 0.0016722972504794598\n",
      "Epoch: 136, Iteration: 238, Loss: 0.002027985407039523\n",
      "Epoch: 136, Iteration: 239, Loss: 0.0019300940912216902\n",
      "Epoch: 136, Iteration: 240, Loss: 0.0019588060677051544\n",
      "Epoch: 136, Iteration: 241, Loss: 0.0018037583213299513\n",
      "Epoch: 136, Iteration: 242, Loss: 0.001820578589104116\n",
      "Epoch: 136, Iteration: 243, Loss: 0.001970860408619046\n",
      "Epoch: 136, Iteration: 244, Loss: 0.0018792185001075268\n",
      "Epoch: 136, Iteration: 245, Loss: 0.0020078150555491447\n",
      "Epoch: 136, Iteration: 246, Loss: 0.0018952761311084032\n",
      "Epoch: 136, Iteration: 247, Loss: 0.0018345901044085622\n",
      "Epoch: 136, Iteration: 248, Loss: 0.001995739061385393\n",
      "Epoch: 136, Iteration: 249, Loss: 0.0018615273293107748\n",
      "Epoch: 136, Iteration: 250, Loss: 0.002038950566202402\n",
      "Epoch: 136, Iteration: 251, Loss: 0.0017594030359759927\n",
      "Epoch: 136, Iteration: 252, Loss: 0.0018295306945219636\n",
      "Epoch: 136, Iteration: 253, Loss: 0.0019424825441092253\n",
      "Epoch: 136, Iteration: 254, Loss: 0.0016667320160195231\n",
      "Epoch: 136, Iteration: 255, Loss: 0.0015210786368697882\n",
      "Epoch: 136, Iteration: 256, Loss: 0.0018103100592270494\n",
      "Epoch: 136, Iteration: 257, Loss: 0.002082118531689048\n",
      "Epoch: 136, Iteration: 258, Loss: 0.0018438232364133\n",
      "Epoch: 136, Iteration: 259, Loss: 0.0018008616752922535\n",
      "Epoch: 136, Iteration: 260, Loss: 0.001929107471369207\n",
      "Epoch: 136, Iteration: 261, Loss: 0.0018611564300954342\n",
      "Epoch: 136, Iteration: 262, Loss: 0.002237358596175909\n",
      "Epoch: 136, Iteration: 263, Loss: 0.0018546921201050282\n",
      "Epoch: 136, Iteration: 264, Loss: 0.00172456877771765\n",
      "Epoch: 136, Iteration: 265, Loss: 0.0019476909656077623\n",
      "Epoch: 136, Iteration: 266, Loss: 0.001889069564640522\n",
      "Epoch: 136, Iteration: 267, Loss: 0.0016957080224528909\n",
      "Epoch: 136, Iteration: 268, Loss: 0.0019994471222162247\n",
      "Epoch: 136, Iteration: 269, Loss: 0.001656633336097002\n",
      "Epoch: 136, Iteration: 270, Loss: 0.0018384393770247698\n",
      "Epoch: 136, Iteration: 271, Loss: 0.001951000653207302\n",
      "Epoch: 136, Iteration: 272, Loss: 0.0018818280659615993\n",
      "Epoch: 136, Iteration: 273, Loss: 0.0018550126114860177\n",
      "Epoch: 136, Iteration: 274, Loss: 0.001634709071367979\n",
      "Epoch: 136 Loss: 0.001884765062379726\n",
      "Epoch: 137, Iteration: 0, Loss: 0.0020410188008099794\n",
      "Epoch: 137, Iteration: 1, Loss: 0.0018887564074248075\n",
      "Epoch: 137, Iteration: 2, Loss: 0.0017410411965101957\n",
      "Epoch: 137, Iteration: 3, Loss: 0.0018460788996890187\n",
      "Epoch: 137, Iteration: 4, Loss: 0.0018131043761968613\n",
      "Epoch: 137, Iteration: 5, Loss: 0.0017758016474545002\n",
      "Epoch: 137, Iteration: 6, Loss: 0.001643090508878231\n",
      "Epoch: 137, Iteration: 7, Loss: 0.0019459160976111889\n",
      "Epoch: 137, Iteration: 8, Loss: 0.002191714011132717\n",
      "Epoch: 137, Iteration: 9, Loss: 0.0020054983906447887\n",
      "Epoch: 137, Iteration: 10, Loss: 0.0019134895410388708\n",
      "Epoch: 137, Iteration: 11, Loss: 0.0019293036311864853\n",
      "Epoch: 137, Iteration: 12, Loss: 0.001825591898523271\n",
      "Epoch: 137, Iteration: 13, Loss: 0.0016266265884041786\n",
      "Epoch: 137, Iteration: 14, Loss: 0.0018258173950016499\n",
      "Epoch: 137, Iteration: 15, Loss: 0.0019879406318068504\n",
      "Epoch: 137, Iteration: 16, Loss: 0.0019921816419810057\n",
      "Epoch: 137, Iteration: 17, Loss: 0.0017157441470772028\n",
      "Epoch: 137, Iteration: 18, Loss: 0.00205923686735332\n",
      "Epoch: 137, Iteration: 19, Loss: 0.0019851536490023136\n",
      "Epoch: 137, Iteration: 20, Loss: 0.0020136351231485605\n",
      "Epoch: 137, Iteration: 21, Loss: 0.0017330498667433858\n",
      "Epoch: 137, Iteration: 22, Loss: 0.0020110830664634705\n",
      "Epoch: 137, Iteration: 23, Loss: 0.0018848143517971039\n",
      "Epoch: 137, Iteration: 24, Loss: 0.0018323840340599418\n",
      "Epoch: 137, Iteration: 25, Loss: 0.0020601372234523296\n",
      "Epoch: 137, Iteration: 26, Loss: 0.0017707906663417816\n",
      "Epoch: 137, Iteration: 27, Loss: 0.0016225671861320734\n",
      "Epoch: 137, Iteration: 28, Loss: 0.001662418944761157\n",
      "Epoch: 137, Iteration: 29, Loss: 0.0017885619308799505\n",
      "Epoch: 137, Iteration: 30, Loss: 0.001831929897889495\n",
      "Epoch: 137, Iteration: 31, Loss: 0.0018768110312521458\n",
      "Epoch: 137, Iteration: 32, Loss: 0.001898959744721651\n",
      "Epoch: 137, Iteration: 33, Loss: 0.0019072743598371744\n",
      "Epoch: 137, Iteration: 34, Loss: 0.0019414599519222975\n",
      "Epoch: 137, Iteration: 35, Loss: 0.0019799801521003246\n",
      "Epoch: 137, Iteration: 36, Loss: 0.00176205241587013\n",
      "Epoch: 137, Iteration: 37, Loss: 0.001956466818228364\n",
      "Epoch: 137, Iteration: 38, Loss: 0.001927209203131497\n",
      "Epoch: 137, Iteration: 39, Loss: 0.0016071273712441325\n",
      "Epoch: 137, Iteration: 40, Loss: 0.001951403683051467\n",
      "Epoch: 137, Iteration: 41, Loss: 0.0017292645061388612\n",
      "Epoch: 137, Iteration: 42, Loss: 0.0018998590530827641\n",
      "Epoch: 137, Iteration: 43, Loss: 0.0016536815091967583\n",
      "Epoch: 137, Iteration: 44, Loss: 0.0018263814272359014\n",
      "Epoch: 137, Iteration: 45, Loss: 0.001773382886312902\n",
      "Epoch: 137, Iteration: 46, Loss: 0.0020253066904842854\n",
      "Epoch: 137, Iteration: 47, Loss: 0.0018724458059296012\n",
      "Epoch: 137, Iteration: 48, Loss: 0.0015869145281612873\n",
      "Epoch: 137, Iteration: 49, Loss: 0.0019673448987305164\n",
      "Epoch: 137, Iteration: 50, Loss: 0.001808037399314344\n",
      "Epoch: 137, Iteration: 51, Loss: 0.002068726811558008\n",
      "Epoch: 137, Iteration: 52, Loss: 0.0018083485774695873\n",
      "Epoch: 137, Iteration: 53, Loss: 0.0018568853847682476\n",
      "Epoch: 137, Iteration: 54, Loss: 0.002016047714278102\n",
      "Epoch: 137, Iteration: 55, Loss: 0.0016397085273638368\n",
      "Epoch: 137, Iteration: 56, Loss: 0.001993466867133975\n",
      "Epoch: 137, Iteration: 57, Loss: 0.0018423744477331638\n",
      "Epoch: 137, Iteration: 58, Loss: 0.0019497686298564076\n",
      "Epoch: 137, Iteration: 59, Loss: 0.0018648060504347086\n",
      "Epoch: 137, Iteration: 60, Loss: 0.0020308704115450382\n",
      "Epoch: 137, Iteration: 61, Loss: 0.0018853159854188561\n",
      "Epoch: 137, Iteration: 62, Loss: 0.0016899066977202892\n",
      "Epoch: 137, Iteration: 63, Loss: 0.0020133797079324722\n",
      "Epoch: 137, Iteration: 64, Loss: 0.001907623140141368\n",
      "Epoch: 137, Iteration: 65, Loss: 0.001738624181598425\n",
      "Epoch: 137, Iteration: 66, Loss: 0.001814173418097198\n",
      "Epoch: 137, Iteration: 67, Loss: 0.001895550056360662\n",
      "Epoch: 137, Iteration: 68, Loss: 0.001756457844749093\n",
      "Epoch: 137, Iteration: 69, Loss: 0.001741843530908227\n",
      "Epoch: 137, Iteration: 70, Loss: 0.00218206737190485\n",
      "Epoch: 137, Iteration: 71, Loss: 0.0020649584475904703\n",
      "Epoch: 137, Iteration: 72, Loss: 0.002032782416790724\n",
      "Epoch: 137, Iteration: 73, Loss: 0.001980692381039262\n",
      "Epoch: 137, Iteration: 74, Loss: 0.0017801024951040745\n",
      "Epoch: 137, Iteration: 75, Loss: 0.0019661053083837032\n",
      "Epoch: 137, Iteration: 76, Loss: 0.0016135512851178646\n",
      "Epoch: 137, Iteration: 77, Loss: 0.0017459244700148702\n",
      "Epoch: 137, Iteration: 78, Loss: 0.0019403415499255061\n",
      "Epoch: 137, Iteration: 79, Loss: 0.0019902968779206276\n",
      "Epoch: 137, Iteration: 80, Loss: 0.001977540785446763\n",
      "Epoch: 137, Iteration: 81, Loss: 0.0019816977437585592\n",
      "Epoch: 137, Iteration: 82, Loss: 0.0016217282973229885\n",
      "Epoch: 137, Iteration: 83, Loss: 0.0017576938262209296\n",
      "Epoch: 137, Iteration: 84, Loss: 0.0019351111259311438\n",
      "Epoch: 137, Iteration: 85, Loss: 0.0018747642170637846\n",
      "Epoch: 137, Iteration: 86, Loss: 0.0020626173354685307\n",
      "Epoch: 137, Iteration: 87, Loss: 0.001810257090255618\n",
      "Epoch: 137, Iteration: 88, Loss: 0.0018391249468550086\n",
      "Epoch: 137, Iteration: 89, Loss: 0.0018781444523483515\n",
      "Epoch: 137, Iteration: 90, Loss: 0.0017542502610012889\n",
      "Epoch: 137, Iteration: 91, Loss: 0.0016616008942946792\n",
      "Epoch: 137, Iteration: 92, Loss: 0.0017640471924096346\n",
      "Epoch: 137, Iteration: 93, Loss: 0.0017599635757505894\n",
      "Epoch: 137, Iteration: 94, Loss: 0.0016941134817898273\n",
      "Epoch: 137, Iteration: 95, Loss: 0.001878911629319191\n",
      "Epoch: 137, Iteration: 96, Loss: 0.001856667222455144\n",
      "Epoch: 137, Iteration: 97, Loss: 0.001799193094484508\n",
      "Epoch: 137, Iteration: 98, Loss: 0.001819934812374413\n",
      "Epoch: 137, Iteration: 99, Loss: 0.0018754020566120744\n",
      "Epoch: 137, Iteration: 100, Loss: 0.00195258145686239\n",
      "Epoch: 137, Iteration: 101, Loss: 0.00226774625480175\n",
      "Epoch: 137, Iteration: 102, Loss: 0.0017631682567298412\n",
      "Epoch: 137, Iteration: 103, Loss: 0.0018502892926335335\n",
      "Epoch: 137, Iteration: 104, Loss: 0.0019839638844132423\n",
      "Epoch: 137, Iteration: 105, Loss: 0.0018596575828269124\n",
      "Epoch: 137, Iteration: 106, Loss: 0.001862900098785758\n",
      "Epoch: 137, Iteration: 107, Loss: 0.0018503336468711495\n",
      "Epoch: 137, Iteration: 108, Loss: 0.001899974886327982\n",
      "Epoch: 137, Iteration: 109, Loss: 0.001926247263327241\n",
      "Epoch: 137, Iteration: 110, Loss: 0.0019309873459860682\n",
      "Epoch: 137, Iteration: 111, Loss: 0.0020290864631533623\n",
      "Epoch: 137, Iteration: 112, Loss: 0.0017921477556228638\n",
      "Epoch: 137, Iteration: 113, Loss: 0.001912836218252778\n",
      "Epoch: 137, Iteration: 114, Loss: 0.001968276919797063\n",
      "Epoch: 137, Iteration: 115, Loss: 0.0017590108327567577\n",
      "Epoch: 137, Iteration: 116, Loss: 0.002127590123564005\n",
      "Epoch: 137, Iteration: 117, Loss: 0.002363959327340126\n",
      "Epoch: 137, Iteration: 118, Loss: 0.0017938828095793724\n",
      "Epoch: 137, Iteration: 119, Loss: 0.0019133980385959148\n",
      "Epoch: 137, Iteration: 120, Loss: 0.0018312132451683283\n",
      "Epoch: 137, Iteration: 121, Loss: 0.0017687559593468904\n",
      "Epoch: 137, Iteration: 122, Loss: 0.002043898683041334\n",
      "Epoch: 137, Iteration: 123, Loss: 0.0018075949046760798\n",
      "Epoch: 137, Iteration: 124, Loss: 0.002038586651906371\n",
      "Epoch: 137, Iteration: 125, Loss: 0.0017814910970628262\n",
      "Epoch: 137, Iteration: 126, Loss: 0.0017657516291365027\n",
      "Epoch: 137, Iteration: 127, Loss: 0.0019107505213469267\n",
      "Epoch: 137, Iteration: 128, Loss: 0.0019732837099581957\n",
      "Epoch: 137, Iteration: 129, Loss: 0.0017972192727029324\n",
      "Epoch: 137, Iteration: 130, Loss: 0.0017531800549477339\n",
      "Epoch: 137, Iteration: 131, Loss: 0.001887040096335113\n",
      "Epoch: 137, Iteration: 132, Loss: 0.00181185407564044\n",
      "Epoch: 137, Iteration: 133, Loss: 0.001972589176148176\n",
      "Epoch: 137, Iteration: 134, Loss: 0.001998994732275605\n",
      "Epoch: 137, Iteration: 135, Loss: 0.0017596425022929907\n",
      "Epoch: 137, Iteration: 136, Loss: 0.001808931934647262\n",
      "Epoch: 137, Iteration: 137, Loss: 0.002019373932853341\n",
      "Epoch: 137, Iteration: 138, Loss: 0.0017656987765803933\n",
      "Epoch: 137, Iteration: 139, Loss: 0.001532047986984253\n",
      "Epoch: 137, Iteration: 140, Loss: 0.0016767007764428854\n",
      "Epoch: 137, Iteration: 141, Loss: 0.0018493486568331718\n",
      "Epoch: 137, Iteration: 142, Loss: 0.0020596145186573267\n",
      "Epoch: 137, Iteration: 143, Loss: 0.0021017207764089108\n",
      "Epoch: 137, Iteration: 144, Loss: 0.0019409918459132314\n",
      "Epoch: 137, Iteration: 145, Loss: 0.002095223870128393\n",
      "Epoch: 137, Iteration: 146, Loss: 0.0018536419374868274\n",
      "Epoch: 137, Iteration: 147, Loss: 0.0018955410923808813\n",
      "Epoch: 137, Iteration: 148, Loss: 0.002059641294181347\n",
      "Epoch: 137, Iteration: 149, Loss: 0.0017350881826132536\n",
      "Epoch: 137, Iteration: 150, Loss: 0.001941919676028192\n",
      "Epoch: 137, Iteration: 151, Loss: 0.0019083884544670582\n",
      "Epoch: 137, Iteration: 152, Loss: 0.0016559124924242496\n",
      "Epoch: 137, Iteration: 153, Loss: 0.0018317144131287932\n",
      "Epoch: 137, Iteration: 154, Loss: 0.0018750722520053387\n",
      "Epoch: 137, Iteration: 155, Loss: 0.0019411438843235373\n",
      "Epoch: 137, Iteration: 156, Loss: 0.0016350277001038194\n",
      "Epoch: 137, Iteration: 157, Loss: 0.0020188549533486366\n",
      "Epoch: 137, Iteration: 158, Loss: 0.0019515976309776306\n",
      "Epoch: 137, Iteration: 159, Loss: 0.0017856326885521412\n",
      "Epoch: 137, Iteration: 160, Loss: 0.0018324488773941994\n",
      "Epoch: 137, Iteration: 161, Loss: 0.002098390832543373\n",
      "Epoch: 137, Iteration: 162, Loss: 0.0019220317481085658\n",
      "Epoch: 137, Iteration: 163, Loss: 0.002220442984253168\n",
      "Epoch: 137, Iteration: 164, Loss: 0.0019511609571054578\n",
      "Epoch: 137, Iteration: 165, Loss: 0.0020638168789446354\n",
      "Epoch: 137, Iteration: 166, Loss: 0.0019469993421807885\n",
      "Epoch: 137, Iteration: 167, Loss: 0.0020388676784932613\n",
      "Epoch: 137, Iteration: 168, Loss: 0.002011591801419854\n",
      "Epoch: 137, Iteration: 169, Loss: 0.0016755096148699522\n",
      "Epoch: 137, Iteration: 170, Loss: 0.001953825354576111\n",
      "Epoch: 137, Iteration: 171, Loss: 0.0018262339290231466\n",
      "Epoch: 137, Iteration: 172, Loss: 0.0018725352128967643\n",
      "Epoch: 137, Iteration: 173, Loss: 0.0020115901716053486\n",
      "Epoch: 137, Iteration: 174, Loss: 0.0021145539358258247\n",
      "Epoch: 137, Iteration: 175, Loss: 0.002011730335652828\n",
      "Epoch: 137, Iteration: 176, Loss: 0.0021009896881878376\n",
      "Epoch: 137, Iteration: 177, Loss: 0.0018734713084995747\n",
      "Epoch: 137, Iteration: 178, Loss: 0.0017613633535802364\n",
      "Epoch: 137, Iteration: 179, Loss: 0.001877175527624786\n",
      "Epoch: 137, Iteration: 180, Loss: 0.0018177683232352138\n",
      "Epoch: 137, Iteration: 181, Loss: 0.0018755735363811255\n",
      "Epoch: 137, Iteration: 182, Loss: 0.0018097070278599858\n",
      "Epoch: 137, Iteration: 183, Loss: 0.0018498667050153017\n",
      "Epoch: 137, Iteration: 184, Loss: 0.001825063955038786\n",
      "Epoch: 137, Iteration: 185, Loss: 0.0015817785169929266\n",
      "Epoch: 137, Iteration: 186, Loss: 0.001909824088215828\n",
      "Epoch: 137, Iteration: 187, Loss: 0.002131273504346609\n",
      "Epoch: 137, Iteration: 188, Loss: 0.0016306722536683083\n",
      "Epoch: 137, Iteration: 189, Loss: 0.001575377071276307\n",
      "Epoch: 137, Iteration: 190, Loss: 0.0019852593541145325\n",
      "Epoch: 137, Iteration: 191, Loss: 0.002044811611995101\n",
      "Epoch: 137, Iteration: 192, Loss: 0.0019501812057569623\n",
      "Epoch: 137, Iteration: 193, Loss: 0.0019589578732848167\n",
      "Epoch: 137, Iteration: 194, Loss: 0.0017476421780884266\n",
      "Epoch: 137, Iteration: 195, Loss: 0.0016868155216798186\n",
      "Epoch: 137, Iteration: 196, Loss: 0.0018154856516048312\n",
      "Epoch: 137, Iteration: 197, Loss: 0.002052288269624114\n",
      "Epoch: 137, Iteration: 198, Loss: 0.0019718450494110584\n",
      "Epoch: 137, Iteration: 199, Loss: 0.0018263347446918488\n",
      "Epoch: 137, Iteration: 200, Loss: 0.0018356848740950227\n",
      "Epoch: 137, Iteration: 201, Loss: 0.0016294998349621892\n",
      "Epoch: 137, Iteration: 202, Loss: 0.0018667837139219046\n",
      "Epoch: 137, Iteration: 203, Loss: 0.0019108271226286888\n",
      "Epoch: 137, Iteration: 204, Loss: 0.0019479217007756233\n",
      "Epoch: 137, Iteration: 205, Loss: 0.002072918927296996\n",
      "Epoch: 137, Iteration: 206, Loss: 0.0021506967023015022\n",
      "Epoch: 137, Iteration: 207, Loss: 0.0017904526321217418\n",
      "Epoch: 137, Iteration: 208, Loss: 0.001913689193315804\n",
      "Epoch: 137, Iteration: 209, Loss: 0.0018374129431322217\n",
      "Epoch: 137, Iteration: 210, Loss: 0.0020391736179590225\n",
      "Epoch: 137, Iteration: 211, Loss: 0.0019176448695361614\n",
      "Epoch: 137, Iteration: 212, Loss: 0.0019230443285778165\n",
      "Epoch: 137, Iteration: 213, Loss: 0.001942797563970089\n",
      "Epoch: 137, Iteration: 214, Loss: 0.0019037240417674184\n",
      "Epoch: 137, Iteration: 215, Loss: 0.001875248970463872\n",
      "Epoch: 137, Iteration: 216, Loss: 0.0018825086299329996\n",
      "Epoch: 137, Iteration: 217, Loss: 0.0017785474192351103\n",
      "Epoch: 137, Iteration: 218, Loss: 0.0018454684177413583\n",
      "Epoch: 137, Iteration: 219, Loss: 0.002046342706307769\n",
      "Epoch: 137, Iteration: 220, Loss: 0.0019103125669062138\n",
      "Epoch: 137, Iteration: 221, Loss: 0.001938174944370985\n",
      "Epoch: 137, Iteration: 222, Loss: 0.0019154729088768363\n",
      "Epoch: 137, Iteration: 223, Loss: 0.0018653747392818332\n",
      "Epoch: 137, Iteration: 224, Loss: 0.002049601636826992\n",
      "Epoch: 137, Iteration: 225, Loss: 0.001965937437489629\n",
      "Epoch: 137, Iteration: 226, Loss: 0.0019057601457461715\n",
      "Epoch: 137, Iteration: 227, Loss: 0.0018652286380529404\n",
      "Epoch: 137, Iteration: 228, Loss: 0.0017627198249101639\n",
      "Epoch: 137, Iteration: 229, Loss: 0.0018663755618035793\n",
      "Epoch: 137, Iteration: 230, Loss: 0.002184704877436161\n",
      "Epoch: 137, Iteration: 231, Loss: 0.0018806152511388063\n",
      "Epoch: 137, Iteration: 232, Loss: 0.0019569729920476675\n",
      "Epoch: 137, Iteration: 233, Loss: 0.0018657429609447718\n",
      "Epoch: 137, Iteration: 234, Loss: 0.0019747852347791195\n",
      "Epoch: 137, Iteration: 235, Loss: 0.00206328765489161\n",
      "Epoch: 137, Iteration: 236, Loss: 0.002251257188618183\n",
      "Epoch: 137, Iteration: 237, Loss: 0.0016474155709147453\n",
      "Epoch: 137, Iteration: 238, Loss: 0.0019016001606360078\n",
      "Epoch: 137, Iteration: 239, Loss: 0.0020371433347463608\n",
      "Epoch: 137, Iteration: 240, Loss: 0.0019776595290750265\n",
      "Epoch: 137, Iteration: 241, Loss: 0.002079978119581938\n",
      "Epoch: 137, Iteration: 242, Loss: 0.0017749296966940165\n",
      "Epoch: 137, Iteration: 243, Loss: 0.0019142022356390953\n",
      "Epoch: 137, Iteration: 244, Loss: 0.0020609144121408463\n",
      "Epoch: 137, Iteration: 245, Loss: 0.001836828887462616\n",
      "Epoch: 137, Iteration: 246, Loss: 0.0018049149075523019\n",
      "Epoch: 137, Iteration: 247, Loss: 0.0018551810644567013\n",
      "Epoch: 137, Iteration: 248, Loss: 0.0017370979767292738\n",
      "Epoch: 137, Iteration: 249, Loss: 0.0017970751505345106\n",
      "Epoch: 137, Iteration: 250, Loss: 0.0019157981732860208\n",
      "Epoch: 137, Iteration: 251, Loss: 0.0019031786359846592\n",
      "Epoch: 137, Iteration: 252, Loss: 0.0017531703924760222\n",
      "Epoch: 137, Iteration: 253, Loss: 0.0016058884793892503\n",
      "Epoch: 137, Iteration: 254, Loss: 0.002096341224387288\n",
      "Epoch: 137, Iteration: 255, Loss: 0.0017026669811457396\n",
      "Epoch: 137, Iteration: 256, Loss: 0.0016126062255352736\n",
      "Epoch: 137, Iteration: 257, Loss: 0.0017462646355852485\n",
      "Epoch: 137, Iteration: 258, Loss: 0.0017718513263389468\n",
      "Epoch: 137, Iteration: 259, Loss: 0.0019002893241122365\n",
      "Epoch: 137, Iteration: 260, Loss: 0.001900293747894466\n",
      "Epoch: 137, Iteration: 261, Loss: 0.0019492149585857987\n",
      "Epoch: 137, Iteration: 262, Loss: 0.0020151075441390276\n",
      "Epoch: 137, Iteration: 263, Loss: 0.0019880777690559626\n",
      "Epoch: 137, Iteration: 264, Loss: 0.001810683635994792\n",
      "Epoch: 137, Iteration: 265, Loss: 0.001961821224540472\n",
      "Epoch: 137, Iteration: 266, Loss: 0.0019045567605644464\n",
      "Epoch: 137, Iteration: 267, Loss: 0.0014990434283390641\n",
      "Epoch: 137, Iteration: 268, Loss: 0.0018621867056936026\n",
      "Epoch: 137, Iteration: 269, Loss: 0.001852958113886416\n",
      "Epoch: 137, Iteration: 270, Loss: 0.0017995107918977737\n",
      "Epoch: 137, Iteration: 271, Loss: 0.0018681541550904512\n",
      "Epoch: 137, Iteration: 272, Loss: 0.0018622481729835272\n",
      "Epoch: 137, Iteration: 273, Loss: 0.0019388908986002207\n",
      "Epoch: 137, Iteration: 274, Loss: 0.0017859245417639613\n",
      "Epoch: 137 Loss: 0.0018844314800094335\n",
      "Epoch: 138, Iteration: 0, Loss: 0.0018815177027136087\n",
      "Epoch: 138, Iteration: 1, Loss: 0.0022151009179651737\n",
      "Epoch: 138, Iteration: 2, Loss: 0.0018104072660207748\n",
      "Epoch: 138, Iteration: 3, Loss: 0.001915374887175858\n",
      "Epoch: 138, Iteration: 4, Loss: 0.001673057908192277\n",
      "Epoch: 138, Iteration: 5, Loss: 0.0019125253893435001\n",
      "Epoch: 138, Iteration: 6, Loss: 0.0019291229546070099\n",
      "Epoch: 138, Iteration: 7, Loss: 0.0017187644261866808\n",
      "Epoch: 138, Iteration: 8, Loss: 0.0016778877470642328\n",
      "Epoch: 138, Iteration: 9, Loss: 0.001857057330198586\n",
      "Epoch: 138, Iteration: 10, Loss: 0.0017974856309592724\n",
      "Epoch: 138, Iteration: 11, Loss: 0.0018864176236093044\n",
      "Epoch: 138, Iteration: 12, Loss: 0.0019062483916059136\n",
      "Epoch: 138, Iteration: 13, Loss: 0.001608151476830244\n",
      "Epoch: 138, Iteration: 14, Loss: 0.0021475113462656736\n",
      "Epoch: 138, Iteration: 15, Loss: 0.0019419151358306408\n",
      "Epoch: 138, Iteration: 16, Loss: 0.0019704224541783333\n",
      "Epoch: 138, Iteration: 17, Loss: 0.0018898192793130875\n",
      "Epoch: 138, Iteration: 18, Loss: 0.0017008990980684757\n",
      "Epoch: 138, Iteration: 19, Loss: 0.0018171996343880892\n",
      "Epoch: 138, Iteration: 20, Loss: 0.0017785521922633052\n",
      "Epoch: 138, Iteration: 21, Loss: 0.0018766752909868956\n",
      "Epoch: 138, Iteration: 22, Loss: 0.002019156701862812\n",
      "Epoch: 138, Iteration: 23, Loss: 0.0015906698536127806\n",
      "Epoch: 138, Iteration: 24, Loss: 0.0016888887621462345\n",
      "Epoch: 138, Iteration: 25, Loss: 0.0017987024039030075\n",
      "Epoch: 138, Iteration: 26, Loss: 0.002037651604041457\n",
      "Epoch: 138, Iteration: 27, Loss: 0.0017849408322945237\n",
      "Epoch: 138, Iteration: 28, Loss: 0.0016161585226655006\n",
      "Epoch: 138, Iteration: 29, Loss: 0.0018118713051080704\n",
      "Epoch: 138, Iteration: 30, Loss: 0.0020004338584840298\n",
      "Epoch: 138, Iteration: 31, Loss: 0.0018396256491541862\n",
      "Epoch: 138, Iteration: 32, Loss: 0.0019479061011224985\n",
      "Epoch: 138, Iteration: 33, Loss: 0.0016886547673493624\n",
      "Epoch: 138, Iteration: 34, Loss: 0.0017019696533679962\n",
      "Epoch: 138, Iteration: 35, Loss: 0.0016839071176946163\n",
      "Epoch: 138, Iteration: 36, Loss: 0.0018745430279523134\n",
      "Epoch: 138, Iteration: 37, Loss: 0.0022092750295996666\n",
      "Epoch: 138, Iteration: 38, Loss: 0.0018984670750796795\n",
      "Epoch: 138, Iteration: 39, Loss: 0.0016790934605523944\n",
      "Epoch: 138, Iteration: 40, Loss: 0.0017113536596298218\n",
      "Epoch: 138, Iteration: 41, Loss: 0.0016646707663312554\n",
      "Epoch: 138, Iteration: 42, Loss: 0.001740049454383552\n",
      "Epoch: 138, Iteration: 43, Loss: 0.001990274526178837\n",
      "Epoch: 138, Iteration: 44, Loss: 0.0019128724234178662\n",
      "Epoch: 138, Iteration: 45, Loss: 0.0018600225448608398\n",
      "Epoch: 138, Iteration: 46, Loss: 0.0018932162784039974\n",
      "Epoch: 138, Iteration: 47, Loss: 0.001956525258719921\n",
      "Epoch: 138, Iteration: 48, Loss: 0.0021563146729022264\n",
      "Epoch: 138, Iteration: 49, Loss: 0.0018351402832195163\n",
      "Epoch: 138, Iteration: 50, Loss: 0.0020145659800618887\n",
      "Epoch: 138, Iteration: 51, Loss: 0.0018878922564908862\n",
      "Epoch: 138, Iteration: 52, Loss: 0.0019284510053694248\n",
      "Epoch: 138, Iteration: 53, Loss: 0.0018514376133680344\n",
      "Epoch: 138, Iteration: 54, Loss: 0.0021241852082312107\n",
      "Epoch: 138, Iteration: 55, Loss: 0.0020151652861386538\n",
      "Epoch: 138, Iteration: 56, Loss: 0.0018131936667487025\n",
      "Epoch: 138, Iteration: 57, Loss: 0.0018833242356777191\n",
      "Epoch: 138, Iteration: 58, Loss: 0.0019012740813195705\n",
      "Epoch: 138, Iteration: 59, Loss: 0.0016842351760715246\n",
      "Epoch: 138, Iteration: 60, Loss: 0.001896803267300129\n",
      "Epoch: 138, Iteration: 61, Loss: 0.0018733157776296139\n",
      "Epoch: 138, Iteration: 62, Loss: 0.0018316574860364199\n",
      "Epoch: 138, Iteration: 63, Loss: 0.0017416365444660187\n",
      "Epoch: 138, Iteration: 64, Loss: 0.0018467812333256006\n",
      "Epoch: 138, Iteration: 65, Loss: 0.0016825690399855375\n",
      "Epoch: 138, Iteration: 66, Loss: 0.0018780454993247986\n",
      "Epoch: 138, Iteration: 67, Loss: 0.0020522871054708958\n",
      "Epoch: 138, Iteration: 68, Loss: 0.0018151457188650966\n",
      "Epoch: 138, Iteration: 69, Loss: 0.001642786432057619\n",
      "Epoch: 138, Iteration: 70, Loss: 0.0021590585820376873\n",
      "Epoch: 138, Iteration: 71, Loss: 0.001898548100143671\n",
      "Epoch: 138, Iteration: 72, Loss: 0.0018833852373063564\n",
      "Epoch: 138, Iteration: 73, Loss: 0.0017801475478336215\n",
      "Epoch: 138, Iteration: 74, Loss: 0.0020145222079008818\n",
      "Epoch: 138, Iteration: 75, Loss: 0.0017756555462256074\n",
      "Epoch: 138, Iteration: 76, Loss: 0.0017417799681425095\n",
      "Epoch: 138, Iteration: 77, Loss: 0.001718518091365695\n",
      "Epoch: 138, Iteration: 78, Loss: 0.0018158499151468277\n",
      "Epoch: 138, Iteration: 79, Loss: 0.0019857133738696575\n",
      "Epoch: 138, Iteration: 80, Loss: 0.001970086246728897\n",
      "Epoch: 138, Iteration: 81, Loss: 0.0020619104616343975\n",
      "Epoch: 138, Iteration: 82, Loss: 0.0018295301124453545\n",
      "Epoch: 138, Iteration: 83, Loss: 0.0019607720896601677\n",
      "Epoch: 138, Iteration: 84, Loss: 0.0019851760007441044\n",
      "Epoch: 138, Iteration: 85, Loss: 0.0019746702164411545\n",
      "Epoch: 138, Iteration: 86, Loss: 0.0018024947494268417\n",
      "Epoch: 138, Iteration: 87, Loss: 0.0018603333737701178\n",
      "Epoch: 138, Iteration: 88, Loss: 0.0017744936048984528\n",
      "Epoch: 138, Iteration: 89, Loss: 0.0017330513801425695\n",
      "Epoch: 138, Iteration: 90, Loss: 0.0019408523803576827\n",
      "Epoch: 138, Iteration: 91, Loss: 0.0017793136648833752\n",
      "Epoch: 138, Iteration: 92, Loss: 0.0020294738933444023\n",
      "Epoch: 138, Iteration: 93, Loss: 0.0018566888757050037\n",
      "Epoch: 138, Iteration: 94, Loss: 0.002013070974498987\n",
      "Epoch: 138, Iteration: 95, Loss: 0.002065539127215743\n",
      "Epoch: 138, Iteration: 96, Loss: 0.0019006187794730067\n",
      "Epoch: 138, Iteration: 97, Loss: 0.0018375723157078028\n",
      "Epoch: 138, Iteration: 98, Loss: 0.0016802636673673987\n",
      "Epoch: 138, Iteration: 99, Loss: 0.0018925684271380305\n",
      "Epoch: 138, Iteration: 100, Loss: 0.0020643500611186028\n",
      "Epoch: 138, Iteration: 101, Loss: 0.002100512618198991\n",
      "Epoch: 138, Iteration: 102, Loss: 0.0018096272833645344\n",
      "Epoch: 138, Iteration: 103, Loss: 0.0017603184096515179\n",
      "Epoch: 138, Iteration: 104, Loss: 0.001919729053042829\n",
      "Epoch: 138, Iteration: 105, Loss: 0.0018344649579375982\n",
      "Epoch: 138, Iteration: 106, Loss: 0.0020022676326334476\n",
      "Epoch: 138, Iteration: 107, Loss: 0.0018844704609364271\n",
      "Epoch: 138, Iteration: 108, Loss: 0.0017458670772612095\n",
      "Epoch: 138, Iteration: 109, Loss: 0.0020425147376954556\n",
      "Epoch: 138, Iteration: 110, Loss: 0.002141475211828947\n",
      "Epoch: 138, Iteration: 111, Loss: 0.0018717031925916672\n",
      "Epoch: 138, Iteration: 112, Loss: 0.0017597776604816318\n",
      "Epoch: 138, Iteration: 113, Loss: 0.0018696782644838095\n",
      "Epoch: 138, Iteration: 114, Loss: 0.001743143773637712\n",
      "Epoch: 138, Iteration: 115, Loss: 0.0019669518806040287\n",
      "Epoch: 138, Iteration: 116, Loss: 0.001988745294511318\n",
      "Epoch: 138, Iteration: 117, Loss: 0.001956106163561344\n",
      "Epoch: 138, Iteration: 118, Loss: 0.001962783047929406\n",
      "Epoch: 138, Iteration: 119, Loss: 0.00203405087813735\n",
      "Epoch: 138, Iteration: 120, Loss: 0.0017055666539818048\n",
      "Epoch: 138, Iteration: 121, Loss: 0.001722810324281454\n",
      "Epoch: 138, Iteration: 122, Loss: 0.0017639521975070238\n",
      "Epoch: 138, Iteration: 123, Loss: 0.0020566387102007866\n",
      "Epoch: 138, Iteration: 124, Loss: 0.0019503749208524823\n",
      "Epoch: 138, Iteration: 125, Loss: 0.0016333414241671562\n",
      "Epoch: 138, Iteration: 126, Loss: 0.001840795623138547\n",
      "Epoch: 138, Iteration: 127, Loss: 0.001762661151587963\n",
      "Epoch: 138, Iteration: 128, Loss: 0.0020681670866906643\n",
      "Epoch: 138, Iteration: 129, Loss: 0.001868421328254044\n",
      "Epoch: 138, Iteration: 130, Loss: 0.0019200654933229089\n",
      "Epoch: 138, Iteration: 131, Loss: 0.0016347875352948904\n",
      "Epoch: 138, Iteration: 132, Loss: 0.002022188389673829\n",
      "Epoch: 138, Iteration: 133, Loss: 0.0018533333204686642\n",
      "Epoch: 138, Iteration: 134, Loss: 0.00186833250336349\n",
      "Epoch: 138, Iteration: 135, Loss: 0.0018422494176775217\n",
      "Epoch: 138, Iteration: 136, Loss: 0.001739124534651637\n",
      "Epoch: 138, Iteration: 137, Loss: 0.001902664196677506\n",
      "Epoch: 138, Iteration: 138, Loss: 0.001991434022784233\n",
      "Epoch: 138, Iteration: 139, Loss: 0.002085748128592968\n",
      "Epoch: 138, Iteration: 140, Loss: 0.0017248999793082476\n",
      "Epoch: 138, Iteration: 141, Loss: 0.0019927325192838907\n",
      "Epoch: 138, Iteration: 142, Loss: 0.001815958064980805\n",
      "Epoch: 138, Iteration: 143, Loss: 0.0017398730851709843\n",
      "Epoch: 138, Iteration: 144, Loss: 0.001965874107554555\n",
      "Epoch: 138, Iteration: 145, Loss: 0.0016665318980813026\n",
      "Epoch: 138, Iteration: 146, Loss: 0.001578836003318429\n",
      "Epoch: 138, Iteration: 147, Loss: 0.0017624960746616125\n",
      "Epoch: 138, Iteration: 148, Loss: 0.0020678630098700523\n",
      "Epoch: 138, Iteration: 149, Loss: 0.0017287768423557281\n",
      "Epoch: 138, Iteration: 150, Loss: 0.00199984316714108\n",
      "Epoch: 138, Iteration: 151, Loss: 0.002000665757805109\n",
      "Epoch: 138, Iteration: 152, Loss: 0.0017823505913838744\n",
      "Epoch: 138, Iteration: 153, Loss: 0.0018268811982125044\n",
      "Epoch: 138, Iteration: 154, Loss: 0.0017461195820942521\n",
      "Epoch: 138, Iteration: 155, Loss: 0.001821290934458375\n",
      "Epoch: 138, Iteration: 156, Loss: 0.0019193935440853238\n",
      "Epoch: 138, Iteration: 157, Loss: 0.0016316460678353906\n",
      "Epoch: 138, Iteration: 158, Loss: 0.0018928064964711666\n",
      "Epoch: 138, Iteration: 159, Loss: 0.0021709632128477097\n",
      "Epoch: 138, Iteration: 160, Loss: 0.0019042830681428313\n",
      "Epoch: 138, Iteration: 161, Loss: 0.0020341116469353437\n",
      "Epoch: 138, Iteration: 162, Loss: 0.001628880389034748\n",
      "Epoch: 138, Iteration: 163, Loss: 0.002045832574367523\n",
      "Epoch: 138, Iteration: 164, Loss: 0.0018733157776296139\n",
      "Epoch: 138, Iteration: 165, Loss: 0.0017549796029925346\n",
      "Epoch: 138, Iteration: 166, Loss: 0.0019370538648217916\n",
      "Epoch: 138, Iteration: 167, Loss: 0.0020490307360887527\n",
      "Epoch: 138, Iteration: 168, Loss: 0.0017677054274827242\n",
      "Epoch: 138, Iteration: 169, Loss: 0.0018916795961558819\n",
      "Epoch: 138, Iteration: 170, Loss: 0.0019483906216919422\n",
      "Epoch: 138, Iteration: 171, Loss: 0.00209436914883554\n",
      "Epoch: 138, Iteration: 172, Loss: 0.0018616266315802932\n",
      "Epoch: 138, Iteration: 173, Loss: 0.0018557190196588635\n",
      "Epoch: 138, Iteration: 174, Loss: 0.001796496333554387\n",
      "Epoch: 138, Iteration: 175, Loss: 0.0019818253349512815\n",
      "Epoch: 138, Iteration: 176, Loss: 0.0019447768572717905\n",
      "Epoch: 138, Iteration: 177, Loss: 0.0017365484964102507\n",
      "Epoch: 138, Iteration: 178, Loss: 0.00183389731682837\n",
      "Epoch: 138, Iteration: 179, Loss: 0.00189026421867311\n",
      "Epoch: 138, Iteration: 180, Loss: 0.0019843592308461666\n",
      "Epoch: 138, Iteration: 181, Loss: 0.0019369613146409392\n",
      "Epoch: 138, Iteration: 182, Loss: 0.0018335056956857443\n",
      "Epoch: 138, Iteration: 183, Loss: 0.001796802505850792\n",
      "Epoch: 138, Iteration: 184, Loss: 0.00221952679567039\n",
      "Epoch: 138, Iteration: 185, Loss: 0.0019016835140064359\n",
      "Epoch: 138, Iteration: 186, Loss: 0.0019381435122340918\n",
      "Epoch: 138, Iteration: 187, Loss: 0.0018468048656359315\n",
      "Epoch: 138, Iteration: 188, Loss: 0.0018813537899404764\n",
      "Epoch: 138, Iteration: 189, Loss: 0.001631952472962439\n",
      "Epoch: 138, Iteration: 190, Loss: 0.0018712785094976425\n",
      "Epoch: 138, Iteration: 191, Loss: 0.001810772460885346\n",
      "Epoch: 138, Iteration: 192, Loss: 0.0016278764232993126\n",
      "Epoch: 138, Iteration: 193, Loss: 0.0017805173993110657\n",
      "Epoch: 138, Iteration: 194, Loss: 0.0017179693095386028\n",
      "Epoch: 138, Iteration: 195, Loss: 0.001856047660112381\n",
      "Epoch: 138, Iteration: 196, Loss: 0.00204326375387609\n",
      "Epoch: 138, Iteration: 197, Loss: 0.0018998973537236452\n",
      "Epoch: 138, Iteration: 198, Loss: 0.001855890266597271\n",
      "Epoch: 138, Iteration: 199, Loss: 0.001983718480914831\n",
      "Epoch: 138, Iteration: 200, Loss: 0.0018651410937309265\n",
      "Epoch: 138, Iteration: 201, Loss: 0.0018474560929462314\n",
      "Epoch: 138, Iteration: 202, Loss: 0.0017957317177206278\n",
      "Epoch: 138, Iteration: 203, Loss: 0.0017840046202763915\n",
      "Epoch: 138, Iteration: 204, Loss: 0.002121033612638712\n",
      "Epoch: 138, Iteration: 205, Loss: 0.0018684889655560255\n",
      "Epoch: 138, Iteration: 206, Loss: 0.0019363805186003447\n",
      "Epoch: 138, Iteration: 207, Loss: 0.0020394569728523493\n",
      "Epoch: 138, Iteration: 208, Loss: 0.001943341107107699\n",
      "Epoch: 138, Iteration: 209, Loss: 0.0018978286534547806\n",
      "Epoch: 138, Iteration: 210, Loss: 0.0017794131999835372\n",
      "Epoch: 138, Iteration: 211, Loss: 0.0018868811894208193\n",
      "Epoch: 138, Iteration: 212, Loss: 0.0020058031659573317\n",
      "Epoch: 138, Iteration: 213, Loss: 0.0018527080537751317\n",
      "Epoch: 138, Iteration: 214, Loss: 0.0017533167265355587\n",
      "Epoch: 138, Iteration: 215, Loss: 0.0017778183100745082\n",
      "Epoch: 138, Iteration: 216, Loss: 0.0018095115665346384\n",
      "Epoch: 138, Iteration: 217, Loss: 0.0018431392963975668\n",
      "Epoch: 138, Iteration: 218, Loss: 0.002123689278960228\n",
      "Epoch: 138, Iteration: 219, Loss: 0.0018138883169740438\n",
      "Epoch: 138, Iteration: 220, Loss: 0.0018633012659847736\n",
      "Epoch: 138, Iteration: 221, Loss: 0.001834171824157238\n",
      "Epoch: 138, Iteration: 222, Loss: 0.0019324738532304764\n",
      "Epoch: 138, Iteration: 223, Loss: 0.0019353257957845926\n",
      "Epoch: 138, Iteration: 224, Loss: 0.0018015169771388173\n",
      "Epoch: 138, Iteration: 225, Loss: 0.0019350305665284395\n",
      "Epoch: 138, Iteration: 226, Loss: 0.0019759424030780792\n",
      "Epoch: 138, Iteration: 227, Loss: 0.0019263424910604954\n",
      "Epoch: 138, Iteration: 228, Loss: 0.0019223582930862904\n",
      "Epoch: 138, Iteration: 229, Loss: 0.0017712670378386974\n",
      "Epoch: 138, Iteration: 230, Loss: 0.0018752060132101178\n",
      "Epoch: 138, Iteration: 231, Loss: 0.0019749626517295837\n",
      "Epoch: 138, Iteration: 232, Loss: 0.0017808617558330297\n",
      "Epoch: 138, Iteration: 233, Loss: 0.0020667673088610172\n",
      "Epoch: 138, Iteration: 234, Loss: 0.002062649466097355\n",
      "Epoch: 138, Iteration: 235, Loss: 0.0016712056240066886\n",
      "Epoch: 138, Iteration: 236, Loss: 0.0019397271098569036\n",
      "Epoch: 138, Iteration: 237, Loss: 0.0017649184446781874\n",
      "Epoch: 138, Iteration: 238, Loss: 0.001922865048982203\n",
      "Epoch: 138, Iteration: 239, Loss: 0.00194423773791641\n",
      "Epoch: 138, Iteration: 240, Loss: 0.0017759944312274456\n",
      "Epoch: 138, Iteration: 241, Loss: 0.0017391328001394868\n",
      "Epoch: 138, Iteration: 242, Loss: 0.002087224740535021\n",
      "Epoch: 138, Iteration: 243, Loss: 0.001871629268862307\n",
      "Epoch: 138, Iteration: 244, Loss: 0.0020045971032232046\n",
      "Epoch: 138, Iteration: 245, Loss: 0.0021705557592213154\n",
      "Epoch: 138, Iteration: 246, Loss: 0.00184356898535043\n",
      "Epoch: 138, Iteration: 247, Loss: 0.0019088932313024998\n",
      "Epoch: 138, Iteration: 248, Loss: 0.001850159838795662\n",
      "Epoch: 138, Iteration: 249, Loss: 0.0020622964948415756\n",
      "Epoch: 138, Iteration: 250, Loss: 0.0017343973740935326\n",
      "Epoch: 138, Iteration: 251, Loss: 0.0018643115181475878\n",
      "Epoch: 138, Iteration: 252, Loss: 0.0020041954703629017\n",
      "Epoch: 138, Iteration: 253, Loss: 0.001907859230414033\n",
      "Epoch: 138, Iteration: 254, Loss: 0.0017471894389018416\n",
      "Epoch: 138, Iteration: 255, Loss: 0.001972739351913333\n",
      "Epoch: 138, Iteration: 256, Loss: 0.0018614724976941943\n",
      "Epoch: 138, Iteration: 257, Loss: 0.0020043125841766596\n",
      "Epoch: 138, Iteration: 258, Loss: 0.001936374930664897\n",
      "Epoch: 138, Iteration: 259, Loss: 0.0018208206165581942\n",
      "Epoch: 138, Iteration: 260, Loss: 0.002363006817176938\n",
      "Epoch: 138, Iteration: 261, Loss: 0.0017833738820627332\n",
      "Epoch: 138, Iteration: 262, Loss: 0.0018564878264442086\n",
      "Epoch: 138, Iteration: 263, Loss: 0.001974026672542095\n",
      "Epoch: 138, Iteration: 264, Loss: 0.0019136406481266022\n",
      "Epoch: 138, Iteration: 265, Loss: 0.0021686565596610308\n",
      "Epoch: 138, Iteration: 266, Loss: 0.002003735164180398\n",
      "Epoch: 138, Iteration: 267, Loss: 0.0018601699266582727\n",
      "Epoch: 138, Iteration: 268, Loss: 0.001679296838119626\n",
      "Epoch: 138, Iteration: 269, Loss: 0.0017900969833135605\n",
      "Epoch: 138, Iteration: 270, Loss: 0.0019044389482587576\n",
      "Epoch: 138, Iteration: 271, Loss: 0.0019420660100877285\n",
      "Epoch: 138, Iteration: 272, Loss: 0.001957247033715248\n",
      "Epoch: 138, Iteration: 273, Loss: 0.001966883661225438\n",
      "Epoch: 138, Iteration: 274, Loss: 0.001696078572422266\n",
      "Epoch: 138 Loss: 0.0018811076947268547\n",
      "Epoch: 139, Iteration: 0, Loss: 0.0022069262340664864\n",
      "Epoch: 139, Iteration: 1, Loss: 0.0018916355911642313\n",
      "Epoch: 139, Iteration: 2, Loss: 0.0018213177099823952\n",
      "Epoch: 139, Iteration: 3, Loss: 0.0021289652213454247\n",
      "Epoch: 139, Iteration: 4, Loss: 0.0017942908452823758\n",
      "Epoch: 139, Iteration: 5, Loss: 0.001803910592570901\n",
      "Epoch: 139, Iteration: 6, Loss: 0.001979940105229616\n",
      "Epoch: 139, Iteration: 7, Loss: 0.0018233871087431908\n",
      "Epoch: 139, Iteration: 8, Loss: 0.0019970627035945654\n",
      "Epoch: 139, Iteration: 9, Loss: 0.00224032299593091\n",
      "Epoch: 139, Iteration: 10, Loss: 0.002056649886071682\n",
      "Epoch: 139, Iteration: 11, Loss: 0.0018757462967187166\n",
      "Epoch: 139, Iteration: 12, Loss: 0.0017285863868892193\n",
      "Epoch: 139, Iteration: 13, Loss: 0.001990499207749963\n",
      "Epoch: 139, Iteration: 14, Loss: 0.001997754443436861\n",
      "Epoch: 139, Iteration: 15, Loss: 0.001824769889935851\n",
      "Epoch: 139, Iteration: 16, Loss: 0.001813986455090344\n",
      "Epoch: 139, Iteration: 17, Loss: 0.0017885762499645352\n",
      "Epoch: 139, Iteration: 18, Loss: 0.0017445898847654462\n",
      "Epoch: 139, Iteration: 19, Loss: 0.0018394894432276487\n",
      "Epoch: 139, Iteration: 20, Loss: 0.0021024392917752266\n",
      "Epoch: 139, Iteration: 21, Loss: 0.0019177328795194626\n",
      "Epoch: 139, Iteration: 22, Loss: 0.0016503476072102785\n",
      "Epoch: 139, Iteration: 23, Loss: 0.0018791714683175087\n",
      "Epoch: 139, Iteration: 24, Loss: 0.001881488598883152\n",
      "Epoch: 139, Iteration: 25, Loss: 0.00216667796485126\n",
      "Epoch: 139, Iteration: 26, Loss: 0.0017274925485253334\n",
      "Epoch: 139, Iteration: 27, Loss: 0.001743882428854704\n",
      "Epoch: 139, Iteration: 28, Loss: 0.0018504811450839043\n",
      "Epoch: 139, Iteration: 29, Loss: 0.0017536988016217947\n",
      "Epoch: 139, Iteration: 30, Loss: 0.002109525026753545\n",
      "Epoch: 139, Iteration: 31, Loss: 0.001982922200113535\n",
      "Epoch: 139, Iteration: 32, Loss: 0.0015095308190211654\n",
      "Epoch: 139, Iteration: 33, Loss: 0.0020143676083534956\n",
      "Epoch: 139, Iteration: 34, Loss: 0.00174422946292907\n",
      "Epoch: 139, Iteration: 35, Loss: 0.0017510547768324614\n",
      "Epoch: 139, Iteration: 36, Loss: 0.002053910866379738\n",
      "Epoch: 139, Iteration: 37, Loss: 0.0020291893742978573\n",
      "Epoch: 139, Iteration: 38, Loss: 0.001917518675327301\n",
      "Epoch: 139, Iteration: 39, Loss: 0.0017809614073485136\n",
      "Epoch: 139, Iteration: 40, Loss: 0.0017532737692818046\n",
      "Epoch: 139, Iteration: 41, Loss: 0.0018547531217336655\n",
      "Epoch: 139, Iteration: 42, Loss: 0.0017375226598232985\n",
      "Epoch: 139, Iteration: 43, Loss: 0.0017996570095419884\n",
      "Epoch: 139, Iteration: 44, Loss: 0.0018299961229786277\n",
      "Epoch: 139, Iteration: 45, Loss: 0.0019234755309298635\n",
      "Epoch: 139, Iteration: 46, Loss: 0.001927482313476503\n",
      "Epoch: 139, Iteration: 47, Loss: 0.0018026572652161121\n",
      "Epoch: 139, Iteration: 48, Loss: 0.0019331839866936207\n",
      "Epoch: 139, Iteration: 49, Loss: 0.0019129454158246517\n",
      "Epoch: 139, Iteration: 50, Loss: 0.0021281875669956207\n",
      "Epoch: 139, Iteration: 51, Loss: 0.0018864247249439359\n",
      "Epoch: 139, Iteration: 52, Loss: 0.0021109802182763815\n",
      "Epoch: 139, Iteration: 53, Loss: 0.0021515521220862865\n",
      "Epoch: 139, Iteration: 54, Loss: 0.0018011544598266482\n",
      "Epoch: 139, Iteration: 55, Loss: 0.0016009787796065211\n",
      "Epoch: 139, Iteration: 56, Loss: 0.0016552549786865711\n",
      "Epoch: 139, Iteration: 57, Loss: 0.0018779755337163806\n",
      "Epoch: 139, Iteration: 58, Loss: 0.0021497332490980625\n",
      "Epoch: 139, Iteration: 59, Loss: 0.0019252973143011332\n",
      "Epoch: 139, Iteration: 60, Loss: 0.0020879660733044147\n",
      "Epoch: 139, Iteration: 61, Loss: 0.001808537752367556\n",
      "Epoch: 139, Iteration: 62, Loss: 0.0018628020770847797\n",
      "Epoch: 139, Iteration: 63, Loss: 0.0017426011618226767\n",
      "Epoch: 139, Iteration: 64, Loss: 0.001970393816009164\n",
      "Epoch: 139, Iteration: 65, Loss: 0.0017953604692593217\n",
      "Epoch: 139, Iteration: 66, Loss: 0.0019823089241981506\n",
      "Epoch: 139, Iteration: 67, Loss: 0.0019259301479905844\n",
      "Epoch: 139, Iteration: 68, Loss: 0.0020503271371126175\n",
      "Epoch: 139, Iteration: 69, Loss: 0.0017952949274331331\n",
      "Epoch: 139, Iteration: 70, Loss: 0.001790153095498681\n",
      "Epoch: 139, Iteration: 71, Loss: 0.0018294950714334846\n",
      "Epoch: 139, Iteration: 72, Loss: 0.001991848461329937\n",
      "Epoch: 139, Iteration: 73, Loss: 0.001981366891413927\n",
      "Epoch: 139, Iteration: 74, Loss: 0.001525643514469266\n",
      "Epoch: 139, Iteration: 75, Loss: 0.001856890507042408\n",
      "Epoch: 139, Iteration: 76, Loss: 0.001716669648885727\n",
      "Epoch: 139, Iteration: 77, Loss: 0.0019352135714143515\n",
      "Epoch: 139, Iteration: 78, Loss: 0.0017190652433782816\n",
      "Epoch: 139, Iteration: 79, Loss: 0.0020185313187539577\n",
      "Epoch: 139, Iteration: 80, Loss: 0.0019592337775975466\n",
      "Epoch: 139, Iteration: 81, Loss: 0.0017786024836823344\n",
      "Epoch: 139, Iteration: 82, Loss: 0.001749177579768002\n",
      "Epoch: 139, Iteration: 83, Loss: 0.001985006034374237\n",
      "Epoch: 139, Iteration: 84, Loss: 0.002072045113891363\n",
      "Epoch: 139, Iteration: 85, Loss: 0.0016919525805860758\n",
      "Epoch: 139, Iteration: 86, Loss: 0.0019764811731874943\n",
      "Epoch: 139, Iteration: 87, Loss: 0.0019359620055183768\n",
      "Epoch: 139, Iteration: 88, Loss: 0.0018786013824865222\n",
      "Epoch: 139, Iteration: 89, Loss: 0.0017116580856963992\n",
      "Epoch: 139, Iteration: 90, Loss: 0.0017740530893206596\n",
      "Epoch: 139, Iteration: 91, Loss: 0.0019770634826272726\n",
      "Epoch: 139, Iteration: 92, Loss: 0.0017533578211441636\n",
      "Epoch: 139, Iteration: 93, Loss: 0.002010580152273178\n",
      "Epoch: 139, Iteration: 94, Loss: 0.0016778617864474654\n",
      "Epoch: 139, Iteration: 95, Loss: 0.002019047737121582\n",
      "Epoch: 139, Iteration: 96, Loss: 0.0019730529747903347\n",
      "Epoch: 139, Iteration: 97, Loss: 0.0021249977871775627\n",
      "Epoch: 139, Iteration: 98, Loss: 0.001991742756217718\n",
      "Epoch: 139, Iteration: 99, Loss: 0.0018658884800970554\n",
      "Epoch: 139, Iteration: 100, Loss: 0.0017921286635100842\n",
      "Epoch: 139, Iteration: 101, Loss: 0.002015454228967428\n",
      "Epoch: 139, Iteration: 102, Loss: 0.0017244727350771427\n",
      "Epoch: 139, Iteration: 103, Loss: 0.0018604957731440663\n",
      "Epoch: 139, Iteration: 104, Loss: 0.001871734275482595\n",
      "Epoch: 139, Iteration: 105, Loss: 0.001847445615567267\n",
      "Epoch: 139, Iteration: 106, Loss: 0.0019261117558926344\n",
      "Epoch: 139, Iteration: 107, Loss: 0.0020098702516406775\n",
      "Epoch: 139, Iteration: 108, Loss: 0.0016649978933855891\n",
      "Epoch: 139, Iteration: 109, Loss: 0.002169417217373848\n",
      "Epoch: 139, Iteration: 110, Loss: 0.0018756981007754803\n",
      "Epoch: 139, Iteration: 111, Loss: 0.001943220035172999\n",
      "Epoch: 139, Iteration: 112, Loss: 0.0016765599139034748\n",
      "Epoch: 139, Iteration: 113, Loss: 0.0018675338942557573\n",
      "Epoch: 139, Iteration: 114, Loss: 0.0016744490712881088\n",
      "Epoch: 139, Iteration: 115, Loss: 0.001727469265460968\n",
      "Epoch: 139, Iteration: 116, Loss: 0.0018608113750815392\n",
      "Epoch: 139, Iteration: 117, Loss: 0.001863255980424583\n",
      "Epoch: 139, Iteration: 118, Loss: 0.0020296857692301273\n",
      "Epoch: 139, Iteration: 119, Loss: 0.0018388675525784492\n",
      "Epoch: 139, Iteration: 120, Loss: 0.0018446851754561067\n",
      "Epoch: 139, Iteration: 121, Loss: 0.0020093582570552826\n",
      "Epoch: 139, Iteration: 122, Loss: 0.0019015460275113583\n",
      "Epoch: 139, Iteration: 123, Loss: 0.001859700889326632\n",
      "Epoch: 139, Iteration: 124, Loss: 0.002152501605451107\n",
      "Epoch: 139, Iteration: 125, Loss: 0.0018644490046426654\n",
      "Epoch: 139, Iteration: 126, Loss: 0.001932795625180006\n",
      "Epoch: 139, Iteration: 127, Loss: 0.001717285835184157\n",
      "Epoch: 139, Iteration: 128, Loss: 0.0017853816971182823\n",
      "Epoch: 139, Iteration: 129, Loss: 0.0017986319726333022\n",
      "Epoch: 139, Iteration: 130, Loss: 0.0019080080091953278\n",
      "Epoch: 139, Iteration: 131, Loss: 0.0018871895736083388\n",
      "Epoch: 139, Iteration: 132, Loss: 0.0021781395189464092\n",
      "Epoch: 139, Iteration: 133, Loss: 0.0018046367913484573\n",
      "Epoch: 139, Iteration: 134, Loss: 0.0017831812147051096\n",
      "Epoch: 139, Iteration: 135, Loss: 0.0020103049464523792\n",
      "Epoch: 139, Iteration: 136, Loss: 0.0021599687170237303\n",
      "Epoch: 139, Iteration: 137, Loss: 0.002074490301311016\n",
      "Epoch: 139, Iteration: 138, Loss: 0.0018286749254912138\n",
      "Epoch: 139, Iteration: 139, Loss: 0.0019574787002056837\n",
      "Epoch: 139, Iteration: 140, Loss: 0.001864735153503716\n",
      "Epoch: 139, Iteration: 141, Loss: 0.001790334703400731\n",
      "Epoch: 139, Iteration: 142, Loss: 0.0019739612471312284\n",
      "Epoch: 139, Iteration: 143, Loss: 0.0018738041399046779\n",
      "Epoch: 139, Iteration: 144, Loss: 0.001687056152150035\n",
      "Epoch: 139, Iteration: 145, Loss: 0.001670844154432416\n",
      "Epoch: 139, Iteration: 146, Loss: 0.002094713971018791\n",
      "Epoch: 139, Iteration: 147, Loss: 0.0017602690495550632\n",
      "Epoch: 139, Iteration: 148, Loss: 0.0016175841446965933\n",
      "Epoch: 139, Iteration: 149, Loss: 0.0018800734542310238\n",
      "Epoch: 139, Iteration: 150, Loss: 0.001899044495075941\n",
      "Epoch: 139, Iteration: 151, Loss: 0.0019313677912577987\n",
      "Epoch: 139, Iteration: 152, Loss: 0.0017822098452597857\n",
      "Epoch: 139, Iteration: 153, Loss: 0.0016803680919110775\n",
      "Epoch: 139, Iteration: 154, Loss: 0.0019281465793028474\n",
      "Epoch: 139, Iteration: 155, Loss: 0.0017123667057603598\n",
      "Epoch: 139, Iteration: 156, Loss: 0.0016017407178878784\n",
      "Epoch: 139, Iteration: 157, Loss: 0.0021956206765025854\n",
      "Epoch: 139, Iteration: 158, Loss: 0.0019353802781552076\n",
      "Epoch: 139, Iteration: 159, Loss: 0.0018980958266183734\n",
      "Epoch: 139, Iteration: 160, Loss: 0.0018546052742749453\n",
      "Epoch: 139, Iteration: 161, Loss: 0.0018308258149772882\n",
      "Epoch: 139, Iteration: 162, Loss: 0.002108111046254635\n",
      "Epoch: 139, Iteration: 163, Loss: 0.001970932586118579\n",
      "Epoch: 139, Iteration: 164, Loss: 0.0017342527862638235\n",
      "Epoch: 139, Iteration: 165, Loss: 0.002003447152674198\n",
      "Epoch: 139, Iteration: 166, Loss: 0.0019804013427346945\n",
      "Epoch: 139, Iteration: 167, Loss: 0.001811364432796836\n",
      "Epoch: 139, Iteration: 168, Loss: 0.0017334104049950838\n",
      "Epoch: 139, Iteration: 169, Loss: 0.0017636592965573072\n",
      "Epoch: 139, Iteration: 170, Loss: 0.0017506431322544813\n",
      "Epoch: 139, Iteration: 171, Loss: 0.0017990204505622387\n",
      "Epoch: 139, Iteration: 172, Loss: 0.0020131932105869055\n",
      "Epoch: 139, Iteration: 173, Loss: 0.0018199363257735968\n",
      "Epoch: 139, Iteration: 174, Loss: 0.0019184187985956669\n",
      "Epoch: 139, Iteration: 175, Loss: 0.0019683814607560635\n",
      "Epoch: 139, Iteration: 176, Loss: 0.0017961766570806503\n",
      "Epoch: 139, Iteration: 177, Loss: 0.0019389300141483545\n",
      "Epoch: 139, Iteration: 178, Loss: 0.0021176114678382874\n",
      "Epoch: 139, Iteration: 179, Loss: 0.001776093733496964\n",
      "Epoch: 139, Iteration: 180, Loss: 0.001620799652300775\n",
      "Epoch: 139, Iteration: 181, Loss: 0.0019372539827600121\n",
      "Epoch: 139, Iteration: 182, Loss: 0.0019831582903862\n",
      "Epoch: 139, Iteration: 183, Loss: 0.0019890782423317432\n",
      "Epoch: 139, Iteration: 184, Loss: 0.001909844926558435\n",
      "Epoch: 139, Iteration: 185, Loss: 0.001818740158341825\n",
      "Epoch: 139, Iteration: 186, Loss: 0.0019829256925731897\n",
      "Epoch: 139, Iteration: 187, Loss: 0.0021496221888810396\n",
      "Epoch: 139, Iteration: 188, Loss: 0.0020965354051440954\n",
      "Epoch: 139, Iteration: 189, Loss: 0.001753766555339098\n",
      "Epoch: 139, Iteration: 190, Loss: 0.0015766338910907507\n",
      "Epoch: 139, Iteration: 191, Loss: 0.00173641974106431\n",
      "Epoch: 139, Iteration: 192, Loss: 0.0019134830217808485\n",
      "Epoch: 139, Iteration: 193, Loss: 0.002053310628980398\n",
      "Epoch: 139, Iteration: 194, Loss: 0.0017802051734179258\n",
      "Epoch: 139, Iteration: 195, Loss: 0.001733326818794012\n",
      "Epoch: 139, Iteration: 196, Loss: 0.0018395425286144018\n",
      "Epoch: 139, Iteration: 197, Loss: 0.001988612348213792\n",
      "Epoch: 139, Iteration: 198, Loss: 0.001815770287066698\n",
      "Epoch: 139, Iteration: 199, Loss: 0.001602083444595337\n",
      "Epoch: 139, Iteration: 200, Loss: 0.0017342452192679048\n",
      "Epoch: 139, Iteration: 201, Loss: 0.0020048946607857943\n",
      "Epoch: 139, Iteration: 202, Loss: 0.0018588773673400283\n",
      "Epoch: 139, Iteration: 203, Loss: 0.0020924252457916737\n",
      "Epoch: 139, Iteration: 204, Loss: 0.0019377138232812285\n",
      "Epoch: 139, Iteration: 205, Loss: 0.0017208545468747616\n",
      "Epoch: 139, Iteration: 206, Loss: 0.001992509001865983\n",
      "Epoch: 139, Iteration: 207, Loss: 0.0019285163143649697\n",
      "Epoch: 139, Iteration: 208, Loss: 0.0018436090322211385\n",
      "Epoch: 139, Iteration: 209, Loss: 0.0018097187858074903\n",
      "Epoch: 139, Iteration: 210, Loss: 0.0019870023243129253\n",
      "Epoch: 139, Iteration: 211, Loss: 0.0019112619338557124\n",
      "Epoch: 139, Iteration: 212, Loss: 0.0018562570912763476\n",
      "Epoch: 139, Iteration: 213, Loss: 0.0021376446820795536\n",
      "Epoch: 139, Iteration: 214, Loss: 0.002029766794294119\n",
      "Epoch: 139, Iteration: 215, Loss: 0.0017818014603108168\n",
      "Epoch: 139, Iteration: 216, Loss: 0.0017332970164716244\n",
      "Epoch: 139, Iteration: 217, Loss: 0.0019193135667592287\n",
      "Epoch: 139, Iteration: 218, Loss: 0.0016265606973320246\n",
      "Epoch: 139, Iteration: 219, Loss: 0.0017737853340804577\n",
      "Epoch: 139, Iteration: 220, Loss: 0.0018812274793162942\n",
      "Epoch: 139, Iteration: 221, Loss: 0.0017483269330114126\n",
      "Epoch: 139, Iteration: 222, Loss: 0.001952957478351891\n",
      "Epoch: 139, Iteration: 223, Loss: 0.0019738604314625263\n",
      "Epoch: 139, Iteration: 224, Loss: 0.0020089566241949797\n",
      "Epoch: 139, Iteration: 225, Loss: 0.0016484056832268834\n",
      "Epoch: 139, Iteration: 226, Loss: 0.0017361994832754135\n",
      "Epoch: 139, Iteration: 227, Loss: 0.0020102919079363346\n",
      "Epoch: 139, Iteration: 228, Loss: 0.0019817601423710585\n",
      "Epoch: 139, Iteration: 229, Loss: 0.0018882020376622677\n",
      "Epoch: 139, Iteration: 230, Loss: 0.0019064064836129546\n",
      "Epoch: 139, Iteration: 231, Loss: 0.0020206940826028585\n",
      "Epoch: 139, Iteration: 232, Loss: 0.001904417178593576\n",
      "Epoch: 139, Iteration: 233, Loss: 0.002089761197566986\n",
      "Epoch: 139, Iteration: 234, Loss: 0.0020866466220468283\n",
      "Epoch: 139, Iteration: 235, Loss: 0.0014765101950615644\n",
      "Epoch: 139, Iteration: 236, Loss: 0.001970705110579729\n",
      "Epoch: 139, Iteration: 237, Loss: 0.0018373386701568961\n",
      "Epoch: 139, Iteration: 238, Loss: 0.0018406269373372197\n",
      "Epoch: 139, Iteration: 239, Loss: 0.002050252864137292\n",
      "Epoch: 139, Iteration: 240, Loss: 0.00182059477083385\n",
      "Epoch: 139, Iteration: 241, Loss: 0.0017323024803772569\n",
      "Epoch: 139, Iteration: 242, Loss: 0.0018425362650305033\n",
      "Epoch: 139, Iteration: 243, Loss: 0.001686988864094019\n",
      "Epoch: 139, Iteration: 244, Loss: 0.001973559847101569\n",
      "Epoch: 139, Iteration: 245, Loss: 0.0019055778393521905\n",
      "Epoch: 139, Iteration: 246, Loss: 0.0019016952719539404\n",
      "Epoch: 139, Iteration: 247, Loss: 0.0019672680646181107\n",
      "Epoch: 139, Iteration: 248, Loss: 0.001913556014187634\n",
      "Epoch: 139, Iteration: 249, Loss: 0.0017236457206308842\n",
      "Epoch: 139, Iteration: 250, Loss: 0.0019128394778817892\n",
      "Epoch: 139, Iteration: 251, Loss: 0.0018846652237698436\n",
      "Epoch: 139, Iteration: 252, Loss: 0.0017530119512230158\n",
      "Epoch: 139, Iteration: 253, Loss: 0.001803455175831914\n",
      "Epoch: 139, Iteration: 254, Loss: 0.002047313377261162\n",
      "Epoch: 139, Iteration: 255, Loss: 0.001838796422816813\n",
      "Epoch: 139, Iteration: 256, Loss: 0.0018329635495319963\n",
      "Epoch: 139, Iteration: 257, Loss: 0.0018807820742949843\n",
      "Epoch: 139, Iteration: 258, Loss: 0.0017773100407794118\n",
      "Epoch: 139, Iteration: 259, Loss: 0.0017743406351655722\n",
      "Epoch: 139, Iteration: 260, Loss: 0.0021172768902033567\n",
      "Epoch: 139, Iteration: 261, Loss: 0.001846061903052032\n",
      "Epoch: 139, Iteration: 262, Loss: 0.0018342039547860622\n",
      "Epoch: 139, Iteration: 263, Loss: 0.002111374866217375\n",
      "Epoch: 139, Iteration: 264, Loss: 0.0022096936590969563\n",
      "Epoch: 139, Iteration: 265, Loss: 0.002034763339906931\n",
      "Epoch: 139, Iteration: 266, Loss: 0.0017023520777001977\n",
      "Epoch: 139, Iteration: 267, Loss: 0.0016360664740204811\n",
      "Epoch: 139, Iteration: 268, Loss: 0.0018589863320812583\n",
      "Epoch: 139, Iteration: 269, Loss: 0.0018760430393740535\n",
      "Epoch: 139, Iteration: 270, Loss: 0.0019374366383999586\n",
      "Epoch: 139, Iteration: 271, Loss: 0.0018011387437582016\n",
      "Epoch: 139, Iteration: 272, Loss: 0.0018595244036987424\n",
      "Epoch: 139, Iteration: 273, Loss: 0.0017315892037004232\n",
      "Epoch: 139, Iteration: 274, Loss: 0.002004398964345455\n",
      "Epoch: 139 Loss: 0.0018833060802082985\n",
      "Epoch: 140, Iteration: 0, Loss: 0.002083495957776904\n",
      "Epoch: 140, Iteration: 1, Loss: 0.0017248692456632853\n",
      "Epoch: 140, Iteration: 2, Loss: 0.001724216272123158\n",
      "Epoch: 140, Iteration: 3, Loss: 0.0019454349530860782\n",
      "Epoch: 140, Iteration: 4, Loss: 0.0016572464955970645\n",
      "Epoch: 140, Iteration: 5, Loss: 0.0021576639264822006\n",
      "Epoch: 140, Iteration: 6, Loss: 0.002016790211200714\n",
      "Epoch: 140, Iteration: 7, Loss: 0.0018145854119211435\n",
      "Epoch: 140, Iteration: 8, Loss: 0.0020493557676672935\n",
      "Epoch: 140, Iteration: 9, Loss: 0.0020850393921136856\n",
      "Epoch: 140, Iteration: 10, Loss: 0.0017046690918505192\n",
      "Epoch: 140, Iteration: 11, Loss: 0.0019184802658855915\n",
      "Epoch: 140, Iteration: 12, Loss: 0.0016453162534162402\n",
      "Epoch: 140, Iteration: 13, Loss: 0.0019219358218833804\n",
      "Epoch: 140, Iteration: 14, Loss: 0.0018878914415836334\n",
      "Epoch: 140, Iteration: 15, Loss: 0.0019820360466837883\n",
      "Epoch: 140, Iteration: 16, Loss: 0.001745865447446704\n",
      "Epoch: 140, Iteration: 17, Loss: 0.0017661753809079528\n",
      "Epoch: 140, Iteration: 18, Loss: 0.0020329831168055534\n",
      "Epoch: 140, Iteration: 19, Loss: 0.0017461107345297933\n",
      "Epoch: 140, Iteration: 20, Loss: 0.0020265276543796062\n",
      "Epoch: 140, Iteration: 21, Loss: 0.001682985806837678\n",
      "Epoch: 140, Iteration: 22, Loss: 0.0017418405041098595\n",
      "Epoch: 140, Iteration: 23, Loss: 0.0019674734212458134\n",
      "Epoch: 140, Iteration: 24, Loss: 0.001916110864840448\n",
      "Epoch: 140, Iteration: 25, Loss: 0.001700912369415164\n",
      "Epoch: 140, Iteration: 26, Loss: 0.0018004439771175385\n",
      "Epoch: 140, Iteration: 27, Loss: 0.0018733724718913436\n",
      "Epoch: 140, Iteration: 28, Loss: 0.0016628948505967855\n",
      "Epoch: 140, Iteration: 29, Loss: 0.001700693042948842\n",
      "Epoch: 140, Iteration: 30, Loss: 0.0019071698188781738\n",
      "Epoch: 140, Iteration: 31, Loss: 0.0023119058459997177\n",
      "Epoch: 140, Iteration: 32, Loss: 0.0019279401749372482\n",
      "Epoch: 140, Iteration: 33, Loss: 0.001695068902336061\n",
      "Epoch: 140, Iteration: 34, Loss: 0.001800851197913289\n",
      "Epoch: 140, Iteration: 35, Loss: 0.0019627693109214306\n",
      "Epoch: 140, Iteration: 36, Loss: 0.0018805726431310177\n",
      "Epoch: 140, Iteration: 37, Loss: 0.002038378268480301\n",
      "Epoch: 140, Iteration: 38, Loss: 0.0017167829209938645\n",
      "Epoch: 140, Iteration: 39, Loss: 0.0022068419493734837\n",
      "Epoch: 140, Iteration: 40, Loss: 0.0017354744486510754\n",
      "Epoch: 140, Iteration: 41, Loss: 0.001774178585037589\n",
      "Epoch: 140, Iteration: 42, Loss: 0.0016133870230987668\n",
      "Epoch: 140, Iteration: 43, Loss: 0.0018983930349349976\n",
      "Epoch: 140, Iteration: 44, Loss: 0.0017459100345149636\n",
      "Epoch: 140, Iteration: 45, Loss: 0.0017417995259165764\n",
      "Epoch: 140, Iteration: 46, Loss: 0.0018067269120365381\n",
      "Epoch: 140, Iteration: 47, Loss: 0.0017524347640573978\n",
      "Epoch: 140, Iteration: 48, Loss: 0.0017708714585751295\n",
      "Epoch: 140, Iteration: 49, Loss: 0.00179668131750077\n",
      "Epoch: 140, Iteration: 50, Loss: 0.0020585618913173676\n",
      "Epoch: 140, Iteration: 51, Loss: 0.0018931400263682008\n",
      "Epoch: 140, Iteration: 52, Loss: 0.0018198424950242043\n",
      "Epoch: 140, Iteration: 53, Loss: 0.001757290679961443\n",
      "Epoch: 140, Iteration: 54, Loss: 0.001868378953076899\n",
      "Epoch: 140, Iteration: 55, Loss: 0.0017020807135850191\n",
      "Epoch: 140, Iteration: 56, Loss: 0.0018625641241669655\n",
      "Epoch: 140, Iteration: 57, Loss: 0.001951986225321889\n",
      "Epoch: 140, Iteration: 58, Loss: 0.0020256442949175835\n",
      "Epoch: 140, Iteration: 59, Loss: 0.0018680314533412457\n",
      "Epoch: 140, Iteration: 60, Loss: 0.0019345777109265327\n",
      "Epoch: 140, Iteration: 61, Loss: 0.0017592558870092034\n",
      "Epoch: 140, Iteration: 62, Loss: 0.001804392784833908\n",
      "Epoch: 140, Iteration: 63, Loss: 0.001956884516403079\n",
      "Epoch: 140, Iteration: 64, Loss: 0.0020424420945346355\n",
      "Epoch: 140, Iteration: 65, Loss: 0.001974809681996703\n",
      "Epoch: 140, Iteration: 66, Loss: 0.001988643780350685\n",
      "Epoch: 140, Iteration: 67, Loss: 0.001979762688279152\n",
      "Epoch: 140, Iteration: 68, Loss: 0.0017588441260159016\n",
      "Epoch: 140, Iteration: 69, Loss: 0.001739160856232047\n",
      "Epoch: 140, Iteration: 70, Loss: 0.0017160625429823995\n",
      "Epoch: 140, Iteration: 71, Loss: 0.0017922716215252876\n",
      "Epoch: 140, Iteration: 72, Loss: 0.0018373739439994097\n",
      "Epoch: 140, Iteration: 73, Loss: 0.0018670718418434262\n",
      "Epoch: 140, Iteration: 74, Loss: 0.0019738911651074886\n",
      "Epoch: 140, Iteration: 75, Loss: 0.0018065564800053835\n",
      "Epoch: 140, Iteration: 76, Loss: 0.0017765755765140057\n",
      "Epoch: 140, Iteration: 77, Loss: 0.0017721762415021658\n",
      "Epoch: 140, Iteration: 78, Loss: 0.0017633205279707909\n",
      "Epoch: 140, Iteration: 79, Loss: 0.001833951799198985\n",
      "Epoch: 140, Iteration: 80, Loss: 0.0021740272641181946\n",
      "Epoch: 140, Iteration: 81, Loss: 0.0017524783033877611\n",
      "Epoch: 140, Iteration: 82, Loss: 0.0019736408721655607\n",
      "Epoch: 140, Iteration: 83, Loss: 0.0020263642072677612\n",
      "Epoch: 140, Iteration: 84, Loss: 0.001921510323882103\n",
      "Epoch: 140, Iteration: 85, Loss: 0.0018593179993331432\n",
      "Epoch: 140, Iteration: 86, Loss: 0.0020945724099874496\n",
      "Epoch: 140, Iteration: 87, Loss: 0.0018780326936393976\n",
      "Epoch: 140, Iteration: 88, Loss: 0.001685051596723497\n",
      "Epoch: 140, Iteration: 89, Loss: 0.0016717612743377686\n",
      "Epoch: 140, Iteration: 90, Loss: 0.0017407035920768976\n",
      "Epoch: 140, Iteration: 91, Loss: 0.00209999387152493\n",
      "Epoch: 140, Iteration: 92, Loss: 0.0018623827490955591\n",
      "Epoch: 140, Iteration: 93, Loss: 0.0019599522929638624\n",
      "Epoch: 140, Iteration: 94, Loss: 0.001721539767459035\n",
      "Epoch: 140, Iteration: 95, Loss: 0.001832308596931398\n",
      "Epoch: 140, Iteration: 96, Loss: 0.001866003731265664\n",
      "Epoch: 140, Iteration: 97, Loss: 0.0019185994751751423\n",
      "Epoch: 140, Iteration: 98, Loss: 0.0019443832570686936\n",
      "Epoch: 140, Iteration: 99, Loss: 0.0019277388928458095\n",
      "Epoch: 140, Iteration: 100, Loss: 0.0018383804708719254\n",
      "Epoch: 140, Iteration: 101, Loss: 0.0016873604618012905\n",
      "Epoch: 140, Iteration: 102, Loss: 0.0018044104799628258\n",
      "Epoch: 140, Iteration: 103, Loss: 0.0021583340130746365\n",
      "Epoch: 140, Iteration: 104, Loss: 0.0022471377160400152\n",
      "Epoch: 140, Iteration: 105, Loss: 0.001874410081654787\n",
      "Epoch: 140, Iteration: 106, Loss: 0.002096224809065461\n",
      "Epoch: 140, Iteration: 107, Loss: 0.0019026421941816807\n",
      "Epoch: 140, Iteration: 108, Loss: 0.0020451033487915993\n",
      "Epoch: 140, Iteration: 109, Loss: 0.0018654938321560621\n",
      "Epoch: 140, Iteration: 110, Loss: 0.0018211418064311147\n",
      "Epoch: 140, Iteration: 111, Loss: 0.0018887551268562675\n",
      "Epoch: 140, Iteration: 112, Loss: 0.0017824394162744284\n",
      "Epoch: 140, Iteration: 113, Loss: 0.0019690915942192078\n",
      "Epoch: 140, Iteration: 114, Loss: 0.001807166263461113\n",
      "Epoch: 140, Iteration: 115, Loss: 0.0021625072695314884\n",
      "Epoch: 140, Iteration: 116, Loss: 0.0015976293943822384\n",
      "Epoch: 140, Iteration: 117, Loss: 0.0018942374736070633\n",
      "Epoch: 140, Iteration: 118, Loss: 0.0017927326261997223\n",
      "Epoch: 140, Iteration: 119, Loss: 0.0016502831131219864\n",
      "Epoch: 140, Iteration: 120, Loss: 0.0018927592318505049\n",
      "Epoch: 140, Iteration: 121, Loss: 0.002345528919249773\n",
      "Epoch: 140, Iteration: 122, Loss: 0.0017975311493501067\n",
      "Epoch: 140, Iteration: 123, Loss: 0.0017698199953883886\n",
      "Epoch: 140, Iteration: 124, Loss: 0.0016685740556567907\n",
      "Epoch: 140, Iteration: 125, Loss: 0.0017649366054683924\n",
      "Epoch: 140, Iteration: 126, Loss: 0.0017442191019654274\n",
      "Epoch: 140, Iteration: 127, Loss: 0.001585853286087513\n",
      "Epoch: 140, Iteration: 128, Loss: 0.0019848118536174297\n",
      "Epoch: 140, Iteration: 129, Loss: 0.0018341235117986798\n",
      "Epoch: 140, Iteration: 130, Loss: 0.001939184032380581\n",
      "Epoch: 140, Iteration: 131, Loss: 0.0020683263428509235\n",
      "Epoch: 140, Iteration: 132, Loss: 0.002113910159096122\n",
      "Epoch: 140, Iteration: 133, Loss: 0.0020102323032915592\n",
      "Epoch: 140, Iteration: 134, Loss: 0.0021457679104059935\n",
      "Epoch: 140, Iteration: 135, Loss: 0.0016771568916738033\n",
      "Epoch: 140, Iteration: 136, Loss: 0.0020215397235006094\n",
      "Epoch: 140, Iteration: 137, Loss: 0.0018202868523076177\n",
      "Epoch: 140, Iteration: 138, Loss: 0.0018541046883910894\n",
      "Epoch: 140, Iteration: 139, Loss: 0.0020851248409599066\n",
      "Epoch: 140, Iteration: 140, Loss: 0.0017835419857874513\n",
      "Epoch: 140, Iteration: 141, Loss: 0.0018245091196149588\n",
      "Epoch: 140, Iteration: 142, Loss: 0.0017357076285406947\n",
      "Epoch: 140, Iteration: 143, Loss: 0.001917453482747078\n",
      "Epoch: 140, Iteration: 144, Loss: 0.0020725256763398647\n",
      "Epoch: 140, Iteration: 145, Loss: 0.0017856685444712639\n",
      "Epoch: 140, Iteration: 146, Loss: 0.0018289688741788268\n",
      "Epoch: 140, Iteration: 147, Loss: 0.0018599041504785419\n",
      "Epoch: 140, Iteration: 148, Loss: 0.002014780417084694\n",
      "Epoch: 140, Iteration: 149, Loss: 0.00159474927932024\n",
      "Epoch: 140, Iteration: 150, Loss: 0.0017856780905276537\n",
      "Epoch: 140, Iteration: 151, Loss: 0.0017799792112782598\n",
      "Epoch: 140, Iteration: 152, Loss: 0.001821481972001493\n",
      "Epoch: 140, Iteration: 153, Loss: 0.001989961601793766\n",
      "Epoch: 140, Iteration: 154, Loss: 0.0020296317525207996\n",
      "Epoch: 140, Iteration: 155, Loss: 0.001901409588754177\n",
      "Epoch: 140, Iteration: 156, Loss: 0.0020995126105844975\n",
      "Epoch: 140, Iteration: 157, Loss: 0.002214346546679735\n",
      "Epoch: 140, Iteration: 158, Loss: 0.0018891360377892852\n",
      "Epoch: 140, Iteration: 159, Loss: 0.002094142371788621\n",
      "Epoch: 140, Iteration: 160, Loss: 0.0019151153974235058\n",
      "Epoch: 140, Iteration: 161, Loss: 0.0017780773341655731\n",
      "Epoch: 140, Iteration: 162, Loss: 0.0018004684243351221\n",
      "Epoch: 140, Iteration: 163, Loss: 0.0019731326028704643\n",
      "Epoch: 140, Iteration: 164, Loss: 0.001883358578197658\n",
      "Epoch: 140, Iteration: 165, Loss: 0.001997683197259903\n",
      "Epoch: 140, Iteration: 166, Loss: 0.0017343543004244566\n",
      "Epoch: 140, Iteration: 167, Loss: 0.001726772403344512\n",
      "Epoch: 140, Iteration: 168, Loss: 0.0021316735073924065\n",
      "Epoch: 140, Iteration: 169, Loss: 0.0018985213246196508\n",
      "Epoch: 140, Iteration: 170, Loss: 0.001925644581206143\n",
      "Epoch: 140, Iteration: 171, Loss: 0.0017074223142117262\n",
      "Epoch: 140, Iteration: 172, Loss: 0.0020738630555570126\n",
      "Epoch: 140, Iteration: 173, Loss: 0.002062336541712284\n",
      "Epoch: 140, Iteration: 174, Loss: 0.001849425840191543\n",
      "Epoch: 140, Iteration: 175, Loss: 0.0019185780547559261\n",
      "Epoch: 140, Iteration: 176, Loss: 0.0019647981971502304\n",
      "Epoch: 140, Iteration: 177, Loss: 0.0018855526577681303\n",
      "Epoch: 140, Iteration: 178, Loss: 0.0019757961854338646\n",
      "Epoch: 140, Iteration: 179, Loss: 0.001942149014212191\n",
      "Epoch: 140, Iteration: 180, Loss: 0.0017508044838905334\n",
      "Epoch: 140, Iteration: 181, Loss: 0.0020199157297611237\n",
      "Epoch: 140, Iteration: 182, Loss: 0.0018011648207902908\n",
      "Epoch: 140, Iteration: 183, Loss: 0.0017740443581715226\n",
      "Epoch: 140, Iteration: 184, Loss: 0.0020136910025030375\n",
      "Epoch: 140, Iteration: 185, Loss: 0.0017467178404331207\n",
      "Epoch: 140, Iteration: 186, Loss: 0.0018403239082545042\n",
      "Epoch: 140, Iteration: 187, Loss: 0.0018910198705270886\n",
      "Epoch: 140, Iteration: 188, Loss: 0.002254311926662922\n",
      "Epoch: 140, Iteration: 189, Loss: 0.0021476522088050842\n",
      "Epoch: 140, Iteration: 190, Loss: 0.001656565349549055\n",
      "Epoch: 140, Iteration: 191, Loss: 0.001762415748089552\n",
      "Epoch: 140, Iteration: 192, Loss: 0.0018245396204292774\n",
      "Epoch: 140, Iteration: 193, Loss: 0.0018829526379704475\n",
      "Epoch: 140, Iteration: 194, Loss: 0.0017897612415254116\n",
      "Epoch: 140, Iteration: 195, Loss: 0.001805430045351386\n",
      "Epoch: 140, Iteration: 196, Loss: 0.0017366309184581041\n",
      "Epoch: 140, Iteration: 197, Loss: 0.0021893498487770557\n",
      "Epoch: 140, Iteration: 198, Loss: 0.0018847659230232239\n",
      "Epoch: 140, Iteration: 199, Loss: 0.002084852894768119\n",
      "Epoch: 140, Iteration: 200, Loss: 0.001781898783519864\n",
      "Epoch: 140, Iteration: 201, Loss: 0.0018102610483765602\n",
      "Epoch: 140, Iteration: 202, Loss: 0.0017113577341660857\n",
      "Epoch: 140, Iteration: 203, Loss: 0.001955687068402767\n",
      "Epoch: 140, Iteration: 204, Loss: 0.0018260963261127472\n",
      "Epoch: 140, Iteration: 205, Loss: 0.0017171860672533512\n",
      "Epoch: 140, Iteration: 206, Loss: 0.002072499366477132\n",
      "Epoch: 140, Iteration: 207, Loss: 0.00196423870511353\n",
      "Epoch: 140, Iteration: 208, Loss: 0.001986444927752018\n",
      "Epoch: 140, Iteration: 209, Loss: 0.0019530488643795252\n",
      "Epoch: 140, Iteration: 210, Loss: 0.001932784216478467\n",
      "Epoch: 140, Iteration: 211, Loss: 0.0019043053034693003\n",
      "Epoch: 140, Iteration: 212, Loss: 0.0017173870000988245\n",
      "Epoch: 140, Iteration: 213, Loss: 0.0018960097804665565\n",
      "Epoch: 140, Iteration: 214, Loss: 0.0018207074608653784\n",
      "Epoch: 140, Iteration: 215, Loss: 0.0020113748032599688\n",
      "Epoch: 140, Iteration: 216, Loss: 0.0016710906056687236\n",
      "Epoch: 140, Iteration: 217, Loss: 0.0018211923306807876\n",
      "Epoch: 140, Iteration: 218, Loss: 0.0017731416737660766\n",
      "Epoch: 140, Iteration: 219, Loss: 0.0018078685970976949\n",
      "Epoch: 140, Iteration: 220, Loss: 0.001998672727495432\n",
      "Epoch: 140, Iteration: 221, Loss: 0.0018474706448614597\n",
      "Epoch: 140, Iteration: 222, Loss: 0.002058877144008875\n",
      "Epoch: 140, Iteration: 223, Loss: 0.0017034315969794989\n",
      "Epoch: 140, Iteration: 224, Loss: 0.0020694350823760033\n",
      "Epoch: 140, Iteration: 225, Loss: 0.001843057107180357\n",
      "Epoch: 140, Iteration: 226, Loss: 0.0020833034068346024\n",
      "Epoch: 140, Iteration: 227, Loss: 0.0019204888958483934\n",
      "Epoch: 140, Iteration: 228, Loss: 0.0019841929897665977\n",
      "Epoch: 140, Iteration: 229, Loss: 0.0018927511991932988\n",
      "Epoch: 140, Iteration: 230, Loss: 0.0020076879300177097\n",
      "Epoch: 140, Iteration: 231, Loss: 0.001963280141353607\n",
      "Epoch: 140, Iteration: 232, Loss: 0.0020318308379501104\n",
      "Epoch: 140, Iteration: 233, Loss: 0.0017332518473267555\n",
      "Epoch: 140, Iteration: 234, Loss: 0.001824510982260108\n",
      "Epoch: 140, Iteration: 235, Loss: 0.0020663284230977297\n",
      "Epoch: 140, Iteration: 236, Loss: 0.0017105957958847284\n",
      "Epoch: 140, Iteration: 237, Loss: 0.0017490708269178867\n",
      "Epoch: 140, Iteration: 238, Loss: 0.0018897944828495383\n",
      "Epoch: 140, Iteration: 239, Loss: 0.0020288480445742607\n",
      "Epoch: 140, Iteration: 240, Loss: 0.0017094271024689078\n",
      "Epoch: 140, Iteration: 241, Loss: 0.0019524677190929651\n",
      "Epoch: 140, Iteration: 242, Loss: 0.002026166534051299\n",
      "Epoch: 140, Iteration: 243, Loss: 0.0019642706029117107\n",
      "Epoch: 140, Iteration: 244, Loss: 0.001820794539526105\n",
      "Epoch: 140, Iteration: 245, Loss: 0.0017754514701664448\n",
      "Epoch: 140, Iteration: 246, Loss: 0.0019228223245590925\n",
      "Epoch: 140, Iteration: 247, Loss: 0.0019483426585793495\n",
      "Epoch: 140, Iteration: 248, Loss: 0.00188530539162457\n",
      "Epoch: 140, Iteration: 249, Loss: 0.001699748681858182\n",
      "Epoch: 140, Iteration: 250, Loss: 0.0017695819260552526\n",
      "Epoch: 140, Iteration: 251, Loss: 0.0017542719142511487\n",
      "Epoch: 140, Iteration: 252, Loss: 0.0021325538400560617\n",
      "Epoch: 140, Iteration: 253, Loss: 0.0019655954092741013\n",
      "Epoch: 140, Iteration: 254, Loss: 0.0017480142414569855\n",
      "Epoch: 140, Iteration: 255, Loss: 0.0016011063707992435\n",
      "Epoch: 140, Iteration: 256, Loss: 0.001876405207440257\n",
      "Epoch: 140, Iteration: 257, Loss: 0.0019096180330961943\n",
      "Epoch: 140, Iteration: 258, Loss: 0.0018852509092539549\n",
      "Epoch: 140, Iteration: 259, Loss: 0.001957385800778866\n",
      "Epoch: 140, Iteration: 260, Loss: 0.001760188490152359\n",
      "Epoch: 140, Iteration: 261, Loss: 0.0019111153669655323\n",
      "Epoch: 140, Iteration: 262, Loss: 0.0018677383195608854\n",
      "Epoch: 140, Iteration: 263, Loss: 0.0017688313964754343\n",
      "Epoch: 140, Iteration: 264, Loss: 0.002017383696511388\n",
      "Epoch: 140, Iteration: 265, Loss: 0.0016817590221762657\n",
      "Epoch: 140, Iteration: 266, Loss: 0.0018779054516926408\n",
      "Epoch: 140, Iteration: 267, Loss: 0.0017618363490328193\n",
      "Epoch: 140, Iteration: 268, Loss: 0.001732116099447012\n",
      "Epoch: 140, Iteration: 269, Loss: 0.0017626623157411814\n",
      "Epoch: 140, Iteration: 270, Loss: 0.0017818489577621222\n",
      "Epoch: 140, Iteration: 271, Loss: 0.0015498654684051871\n",
      "Epoch: 140, Iteration: 272, Loss: 0.001835278351791203\n",
      "Epoch: 140, Iteration: 273, Loss: 0.0018568518571555614\n",
      "Epoch: 140, Iteration: 274, Loss: 0.0018227117834612727\n",
      "Epoch: 140 Loss: 0.0018788938684878508\n",
      "Epoch: 141, Iteration: 0, Loss: 0.00182431866414845\n",
      "Epoch: 141, Iteration: 1, Loss: 0.0016215881332755089\n",
      "Epoch: 141, Iteration: 2, Loss: 0.0017377904150635004\n",
      "Epoch: 141, Iteration: 3, Loss: 0.001815731287933886\n",
      "Epoch: 141, Iteration: 4, Loss: 0.0017452045576646924\n",
      "Epoch: 141, Iteration: 5, Loss: 0.0018390171462669969\n",
      "Epoch: 141, Iteration: 6, Loss: 0.0018153964774683118\n",
      "Epoch: 141, Iteration: 7, Loss: 0.0017455002525821328\n",
      "Epoch: 141, Iteration: 8, Loss: 0.0017932377522811294\n",
      "Epoch: 141, Iteration: 9, Loss: 0.001879677758552134\n",
      "Epoch: 141, Iteration: 10, Loss: 0.001644724514335394\n",
      "Epoch: 141, Iteration: 11, Loss: 0.001926690572872758\n",
      "Epoch: 141, Iteration: 12, Loss: 0.0015834587393328547\n",
      "Epoch: 141, Iteration: 13, Loss: 0.002116604708135128\n",
      "Epoch: 141, Iteration: 14, Loss: 0.0019567208364605904\n",
      "Epoch: 141, Iteration: 15, Loss: 0.002037338213995099\n",
      "Epoch: 141, Iteration: 16, Loss: 0.0018372386693954468\n",
      "Epoch: 141, Iteration: 17, Loss: 0.0016682407585904002\n",
      "Epoch: 141, Iteration: 18, Loss: 0.0018670662539079785\n",
      "Epoch: 141, Iteration: 19, Loss: 0.001943062525242567\n",
      "Epoch: 141, Iteration: 20, Loss: 0.0018998832674697042\n",
      "Epoch: 141, Iteration: 21, Loss: 0.0017777420580387115\n",
      "Epoch: 141, Iteration: 22, Loss: 0.0018563640769571066\n",
      "Epoch: 141, Iteration: 23, Loss: 0.0017557719256728888\n",
      "Epoch: 141, Iteration: 24, Loss: 0.001901685376651585\n",
      "Epoch: 141, Iteration: 25, Loss: 0.002119617071002722\n",
      "Epoch: 141, Iteration: 26, Loss: 0.0022202027030289173\n",
      "Epoch: 141, Iteration: 27, Loss: 0.001974951708689332\n",
      "Epoch: 141, Iteration: 28, Loss: 0.0019750536885112524\n",
      "Epoch: 141, Iteration: 29, Loss: 0.0018472472438588738\n",
      "Epoch: 141, Iteration: 30, Loss: 0.0018737760838121176\n",
      "Epoch: 141, Iteration: 31, Loss: 0.001719817635603249\n",
      "Epoch: 141, Iteration: 32, Loss: 0.0017579130362719297\n",
      "Epoch: 141, Iteration: 33, Loss: 0.0016190747264772654\n",
      "Epoch: 141, Iteration: 34, Loss: 0.001995721599087119\n",
      "Epoch: 141, Iteration: 35, Loss: 0.002060519065707922\n",
      "Epoch: 141, Iteration: 36, Loss: 0.0016529674176126719\n",
      "Epoch: 141, Iteration: 37, Loss: 0.0019589951261878014\n",
      "Epoch: 141, Iteration: 38, Loss: 0.001896109082736075\n",
      "Epoch: 141, Iteration: 39, Loss: 0.0018930239602923393\n",
      "Epoch: 141, Iteration: 40, Loss: 0.0017989135812968016\n",
      "Epoch: 141, Iteration: 41, Loss: 0.0019354053074494004\n",
      "Epoch: 141, Iteration: 42, Loss: 0.001882772659882903\n",
      "Epoch: 141, Iteration: 43, Loss: 0.0020177706610411406\n",
      "Epoch: 141, Iteration: 44, Loss: 0.0018154209246858954\n",
      "Epoch: 141, Iteration: 45, Loss: 0.0018737902864813805\n",
      "Epoch: 141, Iteration: 46, Loss: 0.0018095022533088923\n",
      "Epoch: 141, Iteration: 47, Loss: 0.0019188780570402741\n",
      "Epoch: 141, Iteration: 48, Loss: 0.0017895270138978958\n",
      "Epoch: 141, Iteration: 49, Loss: 0.001773393014445901\n",
      "Epoch: 141, Iteration: 50, Loss: 0.0016268459148705006\n",
      "Epoch: 141, Iteration: 51, Loss: 0.0018598048482090235\n",
      "Epoch: 141, Iteration: 52, Loss: 0.0019578600767999887\n",
      "Epoch: 141, Iteration: 53, Loss: 0.001736401580274105\n",
      "Epoch: 141, Iteration: 54, Loss: 0.001837862771935761\n",
      "Epoch: 141, Iteration: 55, Loss: 0.001980225555598736\n",
      "Epoch: 141, Iteration: 56, Loss: 0.0017534445505589247\n",
      "Epoch: 141, Iteration: 57, Loss: 0.0016457759775221348\n",
      "Epoch: 141, Iteration: 58, Loss: 0.0017390742432326078\n",
      "Epoch: 141, Iteration: 59, Loss: 0.0018170607509091496\n",
      "Epoch: 141, Iteration: 60, Loss: 0.0016574442852288485\n",
      "Epoch: 141, Iteration: 61, Loss: 0.0017050881870090961\n",
      "Epoch: 141, Iteration: 62, Loss: 0.0018761989194899797\n",
      "Epoch: 141, Iteration: 63, Loss: 0.0017401131335645914\n",
      "Epoch: 141, Iteration: 64, Loss: 0.0016201104735955596\n",
      "Epoch: 141, Iteration: 65, Loss: 0.001561059383675456\n",
      "Epoch: 141, Iteration: 66, Loss: 0.0016674979124218225\n",
      "Epoch: 141, Iteration: 67, Loss: 0.001925010117702186\n",
      "Epoch: 141, Iteration: 68, Loss: 0.002198123140260577\n",
      "Epoch: 141, Iteration: 69, Loss: 0.002058144425973296\n",
      "Epoch: 141, Iteration: 70, Loss: 0.0020841900259256363\n",
      "Epoch: 141, Iteration: 71, Loss: 0.0018866484751924872\n",
      "Epoch: 141, Iteration: 72, Loss: 0.002232685685157776\n",
      "Epoch: 141, Iteration: 73, Loss: 0.0015712786698713899\n",
      "Epoch: 141, Iteration: 74, Loss: 0.001937281689606607\n",
      "Epoch: 141, Iteration: 75, Loss: 0.0019529203418642282\n",
      "Epoch: 141, Iteration: 76, Loss: 0.002304212423041463\n",
      "Epoch: 141, Iteration: 77, Loss: 0.0019530152203515172\n",
      "Epoch: 141, Iteration: 78, Loss: 0.001968949567526579\n",
      "Epoch: 141, Iteration: 79, Loss: 0.0018178379395976663\n",
      "Epoch: 141, Iteration: 80, Loss: 0.0021079969592392445\n",
      "Epoch: 141, Iteration: 81, Loss: 0.0017184513853862882\n",
      "Epoch: 141, Iteration: 82, Loss: 0.002004606183618307\n",
      "Epoch: 141, Iteration: 83, Loss: 0.0020134509541094303\n",
      "Epoch: 141, Iteration: 84, Loss: 0.0018062674207612872\n",
      "Epoch: 141, Iteration: 85, Loss: 0.0017807572148740292\n",
      "Epoch: 141, Iteration: 86, Loss: 0.00186276959720999\n",
      "Epoch: 141, Iteration: 87, Loss: 0.0019787733908742666\n",
      "Epoch: 141, Iteration: 88, Loss: 0.0018894766690209508\n",
      "Epoch: 141, Iteration: 89, Loss: 0.0018467623740434647\n",
      "Epoch: 141, Iteration: 90, Loss: 0.0018064928008243442\n",
      "Epoch: 141, Iteration: 91, Loss: 0.0018785560969263315\n",
      "Epoch: 141, Iteration: 92, Loss: 0.0017513495404273272\n",
      "Epoch: 141, Iteration: 93, Loss: 0.0016434622230008245\n",
      "Epoch: 141, Iteration: 94, Loss: 0.0023656480479985476\n",
      "Epoch: 141, Iteration: 95, Loss: 0.0018502187449485064\n",
      "Epoch: 141, Iteration: 96, Loss: 0.0018924153409898281\n",
      "Epoch: 141, Iteration: 97, Loss: 0.0018478711135685444\n",
      "Epoch: 141, Iteration: 98, Loss: 0.001885624136775732\n",
      "Epoch: 141, Iteration: 99, Loss: 0.002041898900642991\n",
      "Epoch: 141, Iteration: 100, Loss: 0.0017884760163724422\n",
      "Epoch: 141, Iteration: 101, Loss: 0.001732535078190267\n",
      "Epoch: 141, Iteration: 102, Loss: 0.001789748901501298\n",
      "Epoch: 141, Iteration: 103, Loss: 0.0017729067476466298\n",
      "Epoch: 141, Iteration: 104, Loss: 0.001757958671078086\n",
      "Epoch: 141, Iteration: 105, Loss: 0.001962953247129917\n",
      "Epoch: 141, Iteration: 106, Loss: 0.0018566491780802608\n",
      "Epoch: 141, Iteration: 107, Loss: 0.0018429744523018599\n",
      "Epoch: 141, Iteration: 108, Loss: 0.0018753567710518837\n",
      "Epoch: 141, Iteration: 109, Loss: 0.00201868312433362\n",
      "Epoch: 141, Iteration: 110, Loss: 0.0018155929865315557\n",
      "Epoch: 141, Iteration: 111, Loss: 0.001820220029912889\n",
      "Epoch: 141, Iteration: 112, Loss: 0.002130836481228471\n",
      "Epoch: 141, Iteration: 113, Loss: 0.0021556615829467773\n",
      "Epoch: 141, Iteration: 114, Loss: 0.0019207659643143415\n",
      "Epoch: 141, Iteration: 115, Loss: 0.001792311668395996\n",
      "Epoch: 141, Iteration: 116, Loss: 0.0018848206382244825\n",
      "Epoch: 141, Iteration: 117, Loss: 0.001854690257459879\n",
      "Epoch: 141, Iteration: 118, Loss: 0.0018349782330915332\n",
      "Epoch: 141, Iteration: 119, Loss: 0.0018990343669429421\n",
      "Epoch: 141, Iteration: 120, Loss: 0.0022965678945183754\n",
      "Epoch: 141, Iteration: 121, Loss: 0.002248534932732582\n",
      "Epoch: 141, Iteration: 122, Loss: 0.0021238215267658234\n",
      "Epoch: 141, Iteration: 123, Loss: 0.0017418127972632647\n",
      "Epoch: 141, Iteration: 124, Loss: 0.0018093379912897944\n",
      "Epoch: 141, Iteration: 125, Loss: 0.0017654686234891415\n",
      "Epoch: 141, Iteration: 126, Loss: 0.001834060181863606\n",
      "Epoch: 141, Iteration: 127, Loss: 0.001862448058091104\n",
      "Epoch: 141, Iteration: 128, Loss: 0.001790863461792469\n",
      "Epoch: 141, Iteration: 129, Loss: 0.0017411843873560429\n",
      "Epoch: 141, Iteration: 130, Loss: 0.0019118869677186012\n",
      "Epoch: 141, Iteration: 131, Loss: 0.001844170968979597\n",
      "Epoch: 141, Iteration: 132, Loss: 0.0017674150876700878\n",
      "Epoch: 141, Iteration: 133, Loss: 0.0018809024477377534\n",
      "Epoch: 141, Iteration: 134, Loss: 0.0019359476864337921\n",
      "Epoch: 141, Iteration: 135, Loss: 0.0017441724194213748\n",
      "Epoch: 141, Iteration: 136, Loss: 0.001990439835935831\n",
      "Epoch: 141, Iteration: 137, Loss: 0.0019563783425837755\n",
      "Epoch: 141, Iteration: 138, Loss: 0.0016796241980046034\n",
      "Epoch: 141, Iteration: 139, Loss: 0.001900735660456121\n",
      "Epoch: 141, Iteration: 140, Loss: 0.0016633793711662292\n",
      "Epoch: 141, Iteration: 141, Loss: 0.0019377206917852163\n",
      "Epoch: 141, Iteration: 142, Loss: 0.0020136539824306965\n",
      "Epoch: 141, Iteration: 143, Loss: 0.0018550948007032275\n",
      "Epoch: 141, Iteration: 144, Loss: 0.0017577295657247305\n",
      "Epoch: 141, Iteration: 145, Loss: 0.0019974582828581333\n",
      "Epoch: 141, Iteration: 146, Loss: 0.0018248569685965776\n",
      "Epoch: 141, Iteration: 147, Loss: 0.0018076479900628328\n",
      "Epoch: 141, Iteration: 148, Loss: 0.0018964651972055435\n",
      "Epoch: 141, Iteration: 149, Loss: 0.0019188886508345604\n",
      "Epoch: 141, Iteration: 150, Loss: 0.0019925707019865513\n",
      "Epoch: 141, Iteration: 151, Loss: 0.00172563293017447\n",
      "Epoch: 141, Iteration: 152, Loss: 0.0019721502903848886\n",
      "Epoch: 141, Iteration: 153, Loss: 0.0020458316430449486\n",
      "Epoch: 141, Iteration: 154, Loss: 0.0015821096021682024\n",
      "Epoch: 141, Iteration: 155, Loss: 0.001915163011290133\n",
      "Epoch: 141, Iteration: 156, Loss: 0.002073116134852171\n",
      "Epoch: 141, Iteration: 157, Loss: 0.001723968656733632\n",
      "Epoch: 141, Iteration: 158, Loss: 0.0020534449722617865\n",
      "Epoch: 141, Iteration: 159, Loss: 0.001819778117351234\n",
      "Epoch: 141, Iteration: 160, Loss: 0.002156173810362816\n",
      "Epoch: 141, Iteration: 161, Loss: 0.001955357613041997\n",
      "Epoch: 141, Iteration: 162, Loss: 0.001961649162694812\n",
      "Epoch: 141, Iteration: 163, Loss: 0.0017794780433177948\n",
      "Epoch: 141, Iteration: 164, Loss: 0.0020803571678698063\n",
      "Epoch: 141, Iteration: 165, Loss: 0.0017779289046302438\n",
      "Epoch: 141, Iteration: 166, Loss: 0.0019451889675110579\n",
      "Epoch: 141, Iteration: 167, Loss: 0.0017665732884779572\n",
      "Epoch: 141, Iteration: 168, Loss: 0.0018990518292412162\n",
      "Epoch: 141, Iteration: 169, Loss: 0.001784029882401228\n",
      "Epoch: 141, Iteration: 170, Loss: 0.002004563808441162\n",
      "Epoch: 141, Iteration: 171, Loss: 0.0021911077201366425\n",
      "Epoch: 141, Iteration: 172, Loss: 0.001926884986460209\n",
      "Epoch: 141, Iteration: 173, Loss: 0.001939007779583335\n",
      "Epoch: 141, Iteration: 174, Loss: 0.001720381318591535\n",
      "Epoch: 141, Iteration: 175, Loss: 0.001980466302484274\n",
      "Epoch: 141, Iteration: 176, Loss: 0.001953941537067294\n",
      "Epoch: 141, Iteration: 177, Loss: 0.0019739463459700346\n",
      "Epoch: 141, Iteration: 178, Loss: 0.001893234089948237\n",
      "Epoch: 141, Iteration: 179, Loss: 0.0018930388614535332\n",
      "Epoch: 141, Iteration: 180, Loss: 0.0018459034617990255\n",
      "Epoch: 141, Iteration: 181, Loss: 0.001676779007539153\n",
      "Epoch: 141, Iteration: 182, Loss: 0.0017946914304047823\n",
      "Epoch: 141, Iteration: 183, Loss: 0.00166747672483325\n",
      "Epoch: 141, Iteration: 184, Loss: 0.002052214927971363\n",
      "Epoch: 141, Iteration: 185, Loss: 0.0018516118871048093\n",
      "Epoch: 141, Iteration: 186, Loss: 0.0019934875890612602\n",
      "Epoch: 141, Iteration: 187, Loss: 0.0018727710703387856\n",
      "Epoch: 141, Iteration: 188, Loss: 0.0018862956203520298\n",
      "Epoch: 141, Iteration: 189, Loss: 0.0019120045471936464\n",
      "Epoch: 141, Iteration: 190, Loss: 0.0017695120768621564\n",
      "Epoch: 141, Iteration: 191, Loss: 0.0017981990240514278\n",
      "Epoch: 141, Iteration: 192, Loss: 0.001708991709165275\n",
      "Epoch: 141, Iteration: 193, Loss: 0.001957676839083433\n",
      "Epoch: 141, Iteration: 194, Loss: 0.001887065707705915\n",
      "Epoch: 141, Iteration: 195, Loss: 0.0019676757510751486\n",
      "Epoch: 141, Iteration: 196, Loss: 0.0020637211855500937\n",
      "Epoch: 141, Iteration: 197, Loss: 0.002002072986215353\n",
      "Epoch: 141, Iteration: 198, Loss: 0.0017448499565944076\n",
      "Epoch: 141, Iteration: 199, Loss: 0.0018442273139953613\n",
      "Epoch: 141, Iteration: 200, Loss: 0.0019801135640591383\n",
      "Epoch: 141, Iteration: 201, Loss: 0.002012526150792837\n",
      "Epoch: 141, Iteration: 202, Loss: 0.0018356379587203264\n",
      "Epoch: 141, Iteration: 203, Loss: 0.0016888976097106934\n",
      "Epoch: 141, Iteration: 204, Loss: 0.001846854342147708\n",
      "Epoch: 141, Iteration: 205, Loss: 0.0017966077430173755\n",
      "Epoch: 141, Iteration: 206, Loss: 0.0016562150558456779\n",
      "Epoch: 141, Iteration: 207, Loss: 0.001703412737697363\n",
      "Epoch: 141, Iteration: 208, Loss: 0.0016904233489185572\n",
      "Epoch: 141, Iteration: 209, Loss: 0.0017815493047237396\n",
      "Epoch: 141, Iteration: 210, Loss: 0.0019382585305720568\n",
      "Epoch: 141, Iteration: 211, Loss: 0.0019517703913152218\n",
      "Epoch: 141, Iteration: 212, Loss: 0.001975285355001688\n",
      "Epoch: 141, Iteration: 213, Loss: 0.0018162375781685114\n",
      "Epoch: 141, Iteration: 214, Loss: 0.0018826902378350496\n",
      "Epoch: 141, Iteration: 215, Loss: 0.001997013110667467\n",
      "Epoch: 141, Iteration: 216, Loss: 0.0017763080541044474\n",
      "Epoch: 141, Iteration: 217, Loss: 0.0017148405313491821\n",
      "Epoch: 141, Iteration: 218, Loss: 0.0019634284544736147\n",
      "Epoch: 141, Iteration: 219, Loss: 0.002020649379119277\n",
      "Epoch: 141, Iteration: 220, Loss: 0.0018488059286028147\n",
      "Epoch: 141, Iteration: 221, Loss: 0.0017494505736976862\n",
      "Epoch: 141, Iteration: 222, Loss: 0.0015831957571208477\n",
      "Epoch: 141, Iteration: 223, Loss: 0.0021091322414577007\n",
      "Epoch: 141, Iteration: 224, Loss: 0.002030898816883564\n",
      "Epoch: 141, Iteration: 225, Loss: 0.0018902892479673028\n",
      "Epoch: 141, Iteration: 226, Loss: 0.0020125224255025387\n",
      "Epoch: 141, Iteration: 227, Loss: 0.0017539464170113206\n",
      "Epoch: 141, Iteration: 228, Loss: 0.0016819268930703402\n",
      "Epoch: 141, Iteration: 229, Loss: 0.002101388294249773\n",
      "Epoch: 141, Iteration: 230, Loss: 0.002007453702390194\n",
      "Epoch: 141, Iteration: 231, Loss: 0.0018777736695483327\n",
      "Epoch: 141, Iteration: 232, Loss: 0.0018360245740041137\n",
      "Epoch: 141, Iteration: 233, Loss: 0.0018876768881455064\n",
      "Epoch: 141, Iteration: 234, Loss: 0.001752955256961286\n",
      "Epoch: 141, Iteration: 235, Loss: 0.0019849701784551144\n",
      "Epoch: 141, Iteration: 236, Loss: 0.0018837047973647714\n",
      "Epoch: 141, Iteration: 237, Loss: 0.0019066676031798124\n",
      "Epoch: 141, Iteration: 238, Loss: 0.0019901783671230078\n",
      "Epoch: 141, Iteration: 239, Loss: 0.0019065635278820992\n",
      "Epoch: 141, Iteration: 240, Loss: 0.0020925465505570173\n",
      "Epoch: 141, Iteration: 241, Loss: 0.0016541891964152455\n",
      "Epoch: 141, Iteration: 242, Loss: 0.0019352086819708347\n",
      "Epoch: 141, Iteration: 243, Loss: 0.00192171148955822\n",
      "Epoch: 141, Iteration: 244, Loss: 0.0020030871964991093\n",
      "Epoch: 141, Iteration: 245, Loss: 0.0020822901278734207\n",
      "Epoch: 141, Iteration: 246, Loss: 0.001765544293448329\n",
      "Epoch: 141, Iteration: 247, Loss: 0.0018190369009971619\n",
      "Epoch: 141, Iteration: 248, Loss: 0.0017992996145039797\n",
      "Epoch: 141, Iteration: 249, Loss: 0.0017301791813224554\n",
      "Epoch: 141, Iteration: 250, Loss: 0.001904764911159873\n",
      "Epoch: 141, Iteration: 251, Loss: 0.0016679734690114856\n",
      "Epoch: 141, Iteration: 252, Loss: 0.0023037397768348455\n",
      "Epoch: 141, Iteration: 253, Loss: 0.0017222372116521\n",
      "Epoch: 141, Iteration: 254, Loss: 0.0018791183829307556\n",
      "Epoch: 141, Iteration: 255, Loss: 0.0020473883487284184\n",
      "Epoch: 141, Iteration: 256, Loss: 0.0015975211281329393\n",
      "Epoch: 141, Iteration: 257, Loss: 0.0019441419281065464\n",
      "Epoch: 141, Iteration: 258, Loss: 0.0017429395811632276\n",
      "Epoch: 141, Iteration: 259, Loss: 0.0017437860369682312\n",
      "Epoch: 141, Iteration: 260, Loss: 0.001891319639980793\n",
      "Epoch: 141, Iteration: 261, Loss: 0.002098681405186653\n",
      "Epoch: 141, Iteration: 262, Loss: 0.0016626021824777126\n",
      "Epoch: 141, Iteration: 263, Loss: 0.0019870572723448277\n",
      "Epoch: 141, Iteration: 264, Loss: 0.001919203787110746\n",
      "Epoch: 141, Iteration: 265, Loss: 0.0019516455940902233\n",
      "Epoch: 141, Iteration: 266, Loss: 0.0019322256557643414\n",
      "Epoch: 141, Iteration: 267, Loss: 0.0019842865876853466\n",
      "Epoch: 141, Iteration: 268, Loss: 0.0019352368544787169\n",
      "Epoch: 141, Iteration: 269, Loss: 0.0018487254856154323\n",
      "Epoch: 141, Iteration: 270, Loss: 0.0017514578066766262\n",
      "Epoch: 141, Iteration: 271, Loss: 0.0020660064183175564\n",
      "Epoch: 141, Iteration: 272, Loss: 0.001916751149110496\n",
      "Epoch: 141, Iteration: 273, Loss: 0.0018236073665320873\n",
      "Epoch: 141, Iteration: 274, Loss: 0.001860425341874361\n",
      "Epoch: 141 Loss: 0.0018773774591166396\n",
      "Epoch: 142, Iteration: 0, Loss: 0.0015104295453056693\n",
      "Epoch: 142, Iteration: 1, Loss: 0.0018356452928856015\n",
      "Epoch: 142, Iteration: 2, Loss: 0.0016068692784756422\n",
      "Epoch: 142, Iteration: 3, Loss: 0.0017474435735493898\n",
      "Epoch: 142, Iteration: 4, Loss: 0.002070536371320486\n",
      "Epoch: 142, Iteration: 5, Loss: 0.0020450956653803587\n",
      "Epoch: 142, Iteration: 6, Loss: 0.0017409331630915403\n",
      "Epoch: 142, Iteration: 7, Loss: 0.001908985897898674\n",
      "Epoch: 142, Iteration: 8, Loss: 0.001995246857404709\n",
      "Epoch: 142, Iteration: 9, Loss: 0.001674364204518497\n",
      "Epoch: 142, Iteration: 10, Loss: 0.0018472406081855297\n",
      "Epoch: 142, Iteration: 11, Loss: 0.0018454059027135372\n",
      "Epoch: 142, Iteration: 12, Loss: 0.0018049010541290045\n",
      "Epoch: 142, Iteration: 13, Loss: 0.0018093808321282268\n",
      "Epoch: 142, Iteration: 14, Loss: 0.0016224447172135115\n",
      "Epoch: 142, Iteration: 15, Loss: 0.002032762859016657\n",
      "Epoch: 142, Iteration: 16, Loss: 0.0017875907942652702\n",
      "Epoch: 142, Iteration: 17, Loss: 0.0017538601532578468\n",
      "Epoch: 142, Iteration: 18, Loss: 0.00191027601249516\n",
      "Epoch: 142, Iteration: 19, Loss: 0.0019144872203469276\n",
      "Epoch: 142, Iteration: 20, Loss: 0.0017698201118037105\n",
      "Epoch: 142, Iteration: 21, Loss: 0.0020494156051427126\n",
      "Epoch: 142, Iteration: 22, Loss: 0.0020539790857583284\n",
      "Epoch: 142, Iteration: 23, Loss: 0.0018422192661091685\n",
      "Epoch: 142, Iteration: 24, Loss: 0.0018495720578357577\n",
      "Epoch: 142, Iteration: 25, Loss: 0.0017745144432410598\n",
      "Epoch: 142, Iteration: 26, Loss: 0.0019711379427462816\n",
      "Epoch: 142, Iteration: 27, Loss: 0.00213591568171978\n",
      "Epoch: 142, Iteration: 28, Loss: 0.0017887058202177286\n",
      "Epoch: 142, Iteration: 29, Loss: 0.0020734642166644335\n",
      "Epoch: 142, Iteration: 30, Loss: 0.001961928326636553\n",
      "Epoch: 142, Iteration: 31, Loss: 0.00195687310770154\n",
      "Epoch: 142, Iteration: 32, Loss: 0.0018612199928611517\n",
      "Epoch: 142, Iteration: 33, Loss: 0.0019516886677592993\n",
      "Epoch: 142, Iteration: 34, Loss: 0.001820530160330236\n",
      "Epoch: 142, Iteration: 35, Loss: 0.0017608877969905734\n",
      "Epoch: 142, Iteration: 36, Loss: 0.0017727805534377694\n",
      "Epoch: 142, Iteration: 37, Loss: 0.001953351777046919\n",
      "Epoch: 142, Iteration: 38, Loss: 0.0017616220284253359\n",
      "Epoch: 142, Iteration: 39, Loss: 0.0019493235740810633\n",
      "Epoch: 142, Iteration: 40, Loss: 0.0017690074164420366\n",
      "Epoch: 142, Iteration: 41, Loss: 0.0020476365461945534\n",
      "Epoch: 142, Iteration: 42, Loss: 0.0018277072813361883\n",
      "Epoch: 142, Iteration: 43, Loss: 0.0019224322168156505\n",
      "Epoch: 142, Iteration: 44, Loss: 0.001956288004294038\n",
      "Epoch: 142, Iteration: 45, Loss: 0.0018136468715965748\n",
      "Epoch: 142, Iteration: 46, Loss: 0.0020299237221479416\n",
      "Epoch: 142, Iteration: 47, Loss: 0.002154430840164423\n",
      "Epoch: 142, Iteration: 48, Loss: 0.0019562053494155407\n",
      "Epoch: 142, Iteration: 49, Loss: 0.0021489164792001247\n",
      "Epoch: 142, Iteration: 50, Loss: 0.001930812606588006\n",
      "Epoch: 142, Iteration: 51, Loss: 0.0020516570657491684\n",
      "Epoch: 142, Iteration: 52, Loss: 0.0018590008839964867\n",
      "Epoch: 142, Iteration: 53, Loss: 0.0019802358001470566\n",
      "Epoch: 142, Iteration: 54, Loss: 0.001977371284738183\n",
      "Epoch: 142, Iteration: 55, Loss: 0.0018581434851512313\n",
      "Epoch: 142, Iteration: 56, Loss: 0.0018524620682001114\n",
      "Epoch: 142, Iteration: 57, Loss: 0.0018020698335021734\n",
      "Epoch: 142, Iteration: 58, Loss: 0.0018413993529975414\n",
      "Epoch: 142, Iteration: 59, Loss: 0.0017526228912174702\n",
      "Epoch: 142, Iteration: 60, Loss: 0.0016000408213585615\n",
      "Epoch: 142, Iteration: 61, Loss: 0.0018054110696539283\n",
      "Epoch: 142, Iteration: 62, Loss: 0.0019111258443444967\n",
      "Epoch: 142, Iteration: 63, Loss: 0.0017944993451237679\n",
      "Epoch: 142, Iteration: 64, Loss: 0.001981819048523903\n",
      "Epoch: 142, Iteration: 65, Loss: 0.0017880047671496868\n",
      "Epoch: 142, Iteration: 66, Loss: 0.0021126961801201105\n",
      "Epoch: 142, Iteration: 67, Loss: 0.0021545300260186195\n",
      "Epoch: 142, Iteration: 68, Loss: 0.0018436615355312824\n",
      "Epoch: 142, Iteration: 69, Loss: 0.0018670863937586546\n",
      "Epoch: 142, Iteration: 70, Loss: 0.0019111143192276359\n",
      "Epoch: 142, Iteration: 71, Loss: 0.0016868249513208866\n",
      "Epoch: 142, Iteration: 72, Loss: 0.0017878798535093665\n",
      "Epoch: 142, Iteration: 73, Loss: 0.0018452040385454893\n",
      "Epoch: 142, Iteration: 74, Loss: 0.0020531248301267624\n",
      "Epoch: 142, Iteration: 75, Loss: 0.0020896869245916605\n",
      "Epoch: 142, Iteration: 76, Loss: 0.001939935376867652\n",
      "Epoch: 142, Iteration: 77, Loss: 0.0017090645851567388\n",
      "Epoch: 142, Iteration: 78, Loss: 0.001899426570162177\n",
      "Epoch: 142, Iteration: 79, Loss: 0.0018330062739551067\n",
      "Epoch: 142, Iteration: 80, Loss: 0.0016902529168874025\n",
      "Epoch: 142, Iteration: 81, Loss: 0.0020780113991349936\n",
      "Epoch: 142, Iteration: 82, Loss: 0.0015982515178620815\n",
      "Epoch: 142, Iteration: 83, Loss: 0.0017690947279334068\n",
      "Epoch: 142, Iteration: 84, Loss: 0.0015887131448835135\n",
      "Epoch: 142, Iteration: 85, Loss: 0.0018065932672470808\n",
      "Epoch: 142, Iteration: 86, Loss: 0.001733199693262577\n",
      "Epoch: 142, Iteration: 87, Loss: 0.0017896019853651524\n",
      "Epoch: 142, Iteration: 88, Loss: 0.001901215873658657\n",
      "Epoch: 142, Iteration: 89, Loss: 0.0017172947991639376\n",
      "Epoch: 142, Iteration: 90, Loss: 0.0018223072402179241\n",
      "Epoch: 142, Iteration: 91, Loss: 0.0018955793930217624\n",
      "Epoch: 142, Iteration: 92, Loss: 0.0021046078763902187\n",
      "Epoch: 142, Iteration: 93, Loss: 0.0019459265749901533\n",
      "Epoch: 142, Iteration: 94, Loss: 0.0017381629440933466\n",
      "Epoch: 142, Iteration: 95, Loss: 0.0018692859448492527\n",
      "Epoch: 142, Iteration: 96, Loss: 0.0017267020884901285\n",
      "Epoch: 142, Iteration: 97, Loss: 0.0019012411357834935\n",
      "Epoch: 142, Iteration: 98, Loss: 0.001990544144064188\n",
      "Epoch: 142, Iteration: 99, Loss: 0.0017811139114201069\n",
      "Epoch: 142, Iteration: 100, Loss: 0.0018273589666932821\n",
      "Epoch: 142, Iteration: 101, Loss: 0.0018171636620536447\n",
      "Epoch: 142, Iteration: 102, Loss: 0.0019406341016292572\n",
      "Epoch: 142, Iteration: 103, Loss: 0.0018529625376686454\n",
      "Epoch: 142, Iteration: 104, Loss: 0.0017354423180222511\n",
      "Epoch: 142, Iteration: 105, Loss: 0.0016242119017988443\n",
      "Epoch: 142, Iteration: 106, Loss: 0.001945033436641097\n",
      "Epoch: 142, Iteration: 107, Loss: 0.001697540981695056\n",
      "Epoch: 142, Iteration: 108, Loss: 0.0016980767250061035\n",
      "Epoch: 142, Iteration: 109, Loss: 0.0018111387034878135\n",
      "Epoch: 142, Iteration: 110, Loss: 0.002117041265591979\n",
      "Epoch: 142, Iteration: 111, Loss: 0.0018589687533676624\n",
      "Epoch: 142, Iteration: 112, Loss: 0.0018515171250328422\n",
      "Epoch: 142, Iteration: 113, Loss: 0.0016550759319216013\n",
      "Epoch: 142, Iteration: 114, Loss: 0.0018246239051222801\n",
      "Epoch: 142, Iteration: 115, Loss: 0.0019445134093984962\n",
      "Epoch: 142, Iteration: 116, Loss: 0.0018906183540821075\n",
      "Epoch: 142, Iteration: 117, Loss: 0.0018545169150456786\n",
      "Epoch: 142, Iteration: 118, Loss: 0.001805997220799327\n",
      "Epoch: 142, Iteration: 119, Loss: 0.0021414116490632296\n",
      "Epoch: 142, Iteration: 120, Loss: 0.001860771095380187\n",
      "Epoch: 142, Iteration: 121, Loss: 0.001998281804844737\n",
      "Epoch: 142, Iteration: 122, Loss: 0.0021665217354893684\n",
      "Epoch: 142, Iteration: 123, Loss: 0.0020309188403189182\n",
      "Epoch: 142, Iteration: 124, Loss: 0.0017370934365317225\n",
      "Epoch: 142, Iteration: 125, Loss: 0.002123979153111577\n",
      "Epoch: 142, Iteration: 126, Loss: 0.0017934022471308708\n",
      "Epoch: 142, Iteration: 127, Loss: 0.0017744370270520449\n",
      "Epoch: 142, Iteration: 128, Loss: 0.0020089815370738506\n",
      "Epoch: 142, Iteration: 129, Loss: 0.002047035377472639\n",
      "Epoch: 142, Iteration: 130, Loss: 0.0019378981087356806\n",
      "Epoch: 142, Iteration: 131, Loss: 0.0018596012378111482\n",
      "Epoch: 142, Iteration: 132, Loss: 0.0019852863624691963\n",
      "Epoch: 142, Iteration: 133, Loss: 0.0017669866792857647\n",
      "Epoch: 142, Iteration: 134, Loss: 0.0019557923078536987\n",
      "Epoch: 142, Iteration: 135, Loss: 0.0018286478007212281\n",
      "Epoch: 142, Iteration: 136, Loss: 0.0018501798622310162\n",
      "Epoch: 142, Iteration: 137, Loss: 0.0020314014982432127\n",
      "Epoch: 142, Iteration: 138, Loss: 0.0019050552509725094\n",
      "Epoch: 142, Iteration: 139, Loss: 0.001905215671285987\n",
      "Epoch: 142, Iteration: 140, Loss: 0.001669536461122334\n",
      "Epoch: 142, Iteration: 141, Loss: 0.0019509611884132028\n",
      "Epoch: 142, Iteration: 142, Loss: 0.0020014047622680664\n",
      "Epoch: 142, Iteration: 143, Loss: 0.0019119411008432508\n",
      "Epoch: 142, Iteration: 144, Loss: 0.0017888783477246761\n",
      "Epoch: 142, Iteration: 145, Loss: 0.0017890532035380602\n",
      "Epoch: 142, Iteration: 146, Loss: 0.0018744086846709251\n",
      "Epoch: 142, Iteration: 147, Loss: 0.0018746476853266358\n",
      "Epoch: 142, Iteration: 148, Loss: 0.001913398620672524\n",
      "Epoch: 142, Iteration: 149, Loss: 0.0020901071839034557\n",
      "Epoch: 142, Iteration: 150, Loss: 0.0019202757393941283\n",
      "Epoch: 142, Iteration: 151, Loss: 0.0017957709496840835\n",
      "Epoch: 142, Iteration: 152, Loss: 0.0016304118325933814\n",
      "Epoch: 142, Iteration: 153, Loss: 0.0020954690407961607\n",
      "Epoch: 142, Iteration: 154, Loss: 0.00183798314537853\n",
      "Epoch: 142, Iteration: 155, Loss: 0.0020107035525143147\n",
      "Epoch: 142, Iteration: 156, Loss: 0.0016841180622577667\n",
      "Epoch: 142, Iteration: 157, Loss: 0.0016494531882926822\n",
      "Epoch: 142, Iteration: 158, Loss: 0.0017565813614055514\n",
      "Epoch: 142, Iteration: 159, Loss: 0.0018267458071932197\n",
      "Epoch: 142, Iteration: 160, Loss: 0.0018248348496854305\n",
      "Epoch: 142, Iteration: 161, Loss: 0.0015964088961482048\n",
      "Epoch: 142, Iteration: 162, Loss: 0.001969326753169298\n",
      "Epoch: 142, Iteration: 163, Loss: 0.0019996999762952328\n",
      "Epoch: 142, Iteration: 164, Loss: 0.0018911195220425725\n",
      "Epoch: 142, Iteration: 165, Loss: 0.0018225697567686439\n",
      "Epoch: 142, Iteration: 166, Loss: 0.0018720435909926891\n",
      "Epoch: 142, Iteration: 167, Loss: 0.0018046103650704026\n",
      "Epoch: 142, Iteration: 168, Loss: 0.0018381926929578185\n",
      "Epoch: 142, Iteration: 169, Loss: 0.0017702130135148764\n",
      "Epoch: 142, Iteration: 170, Loss: 0.001849166234023869\n",
      "Epoch: 142, Iteration: 171, Loss: 0.0019231861224398017\n",
      "Epoch: 142, Iteration: 172, Loss: 0.0018060667207464576\n",
      "Epoch: 142, Iteration: 173, Loss: 0.0020218868739902973\n",
      "Epoch: 142, Iteration: 174, Loss: 0.0019342433661222458\n",
      "Epoch: 142, Iteration: 175, Loss: 0.0020546824671328068\n",
      "Epoch: 142, Iteration: 176, Loss: 0.0021084111649543047\n",
      "Epoch: 142, Iteration: 177, Loss: 0.0018474375829100609\n",
      "Epoch: 142, Iteration: 178, Loss: 0.001993214013054967\n",
      "Epoch: 142, Iteration: 179, Loss: 0.001603565877303481\n",
      "Epoch: 142, Iteration: 180, Loss: 0.0018162521300837398\n",
      "Epoch: 142, Iteration: 181, Loss: 0.001896431902423501\n",
      "Epoch: 142, Iteration: 182, Loss: 0.0019029377726837993\n",
      "Epoch: 142, Iteration: 183, Loss: 0.0017868408467620611\n",
      "Epoch: 142, Iteration: 184, Loss: 0.0018650866113603115\n",
      "Epoch: 142, Iteration: 185, Loss: 0.00168482621666044\n",
      "Epoch: 142, Iteration: 186, Loss: 0.001717681298032403\n",
      "Epoch: 142, Iteration: 187, Loss: 0.0020916853100061417\n",
      "Epoch: 142, Iteration: 188, Loss: 0.002002802910283208\n",
      "Epoch: 142, Iteration: 189, Loss: 0.0018305041594430804\n",
      "Epoch: 142, Iteration: 190, Loss: 0.0018135008867830038\n",
      "Epoch: 142, Iteration: 191, Loss: 0.0020282678306102753\n",
      "Epoch: 142, Iteration: 192, Loss: 0.0017941296100616455\n",
      "Epoch: 142, Iteration: 193, Loss: 0.0015901236329227686\n",
      "Epoch: 142, Iteration: 194, Loss: 0.0018845729064196348\n",
      "Epoch: 142, Iteration: 195, Loss: 0.001868998515419662\n",
      "Epoch: 142, Iteration: 196, Loss: 0.0019769417122006416\n",
      "Epoch: 142, Iteration: 197, Loss: 0.0019415536662563682\n",
      "Epoch: 142, Iteration: 198, Loss: 0.0021543398033827543\n",
      "Epoch: 142, Iteration: 199, Loss: 0.0017560259439051151\n",
      "Epoch: 142, Iteration: 200, Loss: 0.0018327408470213413\n",
      "Epoch: 142, Iteration: 201, Loss: 0.001851377310231328\n",
      "Epoch: 142, Iteration: 202, Loss: 0.0017462969990447164\n",
      "Epoch: 142, Iteration: 203, Loss: 0.001790399313904345\n",
      "Epoch: 142, Iteration: 204, Loss: 0.0021008923649787903\n",
      "Epoch: 142, Iteration: 205, Loss: 0.001763915061019361\n",
      "Epoch: 142, Iteration: 206, Loss: 0.0018756642239168286\n",
      "Epoch: 142, Iteration: 207, Loss: 0.0019183694384992123\n",
      "Epoch: 142, Iteration: 208, Loss: 0.0018689861753955483\n",
      "Epoch: 142, Iteration: 209, Loss: 0.0016586475539952517\n",
      "Epoch: 142, Iteration: 210, Loss: 0.001755536301061511\n",
      "Epoch: 142, Iteration: 211, Loss: 0.001748825190588832\n",
      "Epoch: 142, Iteration: 212, Loss: 0.0017515788786113262\n",
      "Epoch: 142, Iteration: 213, Loss: 0.00200269790366292\n",
      "Epoch: 142, Iteration: 214, Loss: 0.0017124814912676811\n",
      "Epoch: 142, Iteration: 215, Loss: 0.001680117566138506\n",
      "Epoch: 142, Iteration: 216, Loss: 0.002066702116280794\n",
      "Epoch: 142, Iteration: 217, Loss: 0.0017802079673856497\n",
      "Epoch: 142, Iteration: 218, Loss: 0.0018915091641247272\n",
      "Epoch: 142, Iteration: 219, Loss: 0.0018702310044318438\n",
      "Epoch: 142, Iteration: 220, Loss: 0.001959005603566766\n",
      "Epoch: 142, Iteration: 221, Loss: 0.001900594448670745\n",
      "Epoch: 142, Iteration: 222, Loss: 0.0018963192123919725\n",
      "Epoch: 142, Iteration: 223, Loss: 0.0017135764937847853\n",
      "Epoch: 142, Iteration: 224, Loss: 0.0018059962894767523\n",
      "Epoch: 142, Iteration: 225, Loss: 0.0019851764664053917\n",
      "Epoch: 142, Iteration: 226, Loss: 0.0017286876682192087\n",
      "Epoch: 142, Iteration: 227, Loss: 0.0016351905651390553\n",
      "Epoch: 142, Iteration: 228, Loss: 0.002099695149809122\n",
      "Epoch: 142, Iteration: 229, Loss: 0.0017495214706286788\n",
      "Epoch: 142, Iteration: 230, Loss: 0.0020074439235031605\n",
      "Epoch: 142, Iteration: 231, Loss: 0.0019082747166976333\n",
      "Epoch: 142, Iteration: 232, Loss: 0.0020716662984341383\n",
      "Epoch: 142, Iteration: 233, Loss: 0.0016613979823887348\n",
      "Epoch: 142, Iteration: 234, Loss: 0.0018108972581103444\n",
      "Epoch: 142, Iteration: 235, Loss: 0.00194821716286242\n",
      "Epoch: 142, Iteration: 236, Loss: 0.0017308281967416406\n",
      "Epoch: 142, Iteration: 237, Loss: 0.0019672042690217495\n",
      "Epoch: 142, Iteration: 238, Loss: 0.0018925342010334134\n",
      "Epoch: 142, Iteration: 239, Loss: 0.0016764558386057615\n",
      "Epoch: 142, Iteration: 240, Loss: 0.0018858007388189435\n",
      "Epoch: 142, Iteration: 241, Loss: 0.0018240147037431598\n",
      "Epoch: 142, Iteration: 242, Loss: 0.0018838050309568644\n",
      "Epoch: 142, Iteration: 243, Loss: 0.001997710671275854\n",
      "Epoch: 142, Iteration: 244, Loss: 0.001960062887519598\n",
      "Epoch: 142, Iteration: 245, Loss: 0.0017580159474164248\n",
      "Epoch: 142, Iteration: 246, Loss: 0.001879521063528955\n",
      "Epoch: 142, Iteration: 247, Loss: 0.0022038163151592016\n",
      "Epoch: 142, Iteration: 248, Loss: 0.0018115193815901875\n",
      "Epoch: 142, Iteration: 249, Loss: 0.0019320426508784294\n",
      "Epoch: 142, Iteration: 250, Loss: 0.002150915563106537\n",
      "Epoch: 142, Iteration: 251, Loss: 0.002051721792668104\n",
      "Epoch: 142, Iteration: 252, Loss: 0.002148107858374715\n",
      "Epoch: 142, Iteration: 253, Loss: 0.0017358302138745785\n",
      "Epoch: 142, Iteration: 254, Loss: 0.0019336979603394866\n",
      "Epoch: 142, Iteration: 255, Loss: 0.0017834589816629887\n",
      "Epoch: 142, Iteration: 256, Loss: 0.0018304521217942238\n",
      "Epoch: 142, Iteration: 257, Loss: 0.001901563722640276\n",
      "Epoch: 142, Iteration: 258, Loss: 0.0017599038546904922\n",
      "Epoch: 142, Iteration: 259, Loss: 0.0017848717980086803\n",
      "Epoch: 142, Iteration: 260, Loss: 0.0017255027778446674\n",
      "Epoch: 142, Iteration: 261, Loss: 0.0018728328868746758\n",
      "Epoch: 142, Iteration: 262, Loss: 0.0018104780465364456\n",
      "Epoch: 142, Iteration: 263, Loss: 0.0019420074531808496\n",
      "Epoch: 142, Iteration: 264, Loss: 0.0018475726246833801\n",
      "Epoch: 142, Iteration: 265, Loss: 0.0020307842642068863\n",
      "Epoch: 142, Iteration: 266, Loss: 0.0020341326016932726\n",
      "Epoch: 142, Iteration: 267, Loss: 0.0018344505224376917\n",
      "Epoch: 142, Iteration: 268, Loss: 0.0019179026130586863\n",
      "Epoch: 142, Iteration: 269, Loss: 0.0022896467708051205\n",
      "Epoch: 142, Iteration: 270, Loss: 0.0018041922012344003\n",
      "Epoch: 142, Iteration: 271, Loss: 0.0018978898879140615\n",
      "Epoch: 142, Iteration: 272, Loss: 0.0017300806939601898\n",
      "Epoch: 142, Iteration: 273, Loss: 0.0021052020601928234\n",
      "Epoch: 142, Iteration: 274, Loss: 0.001708535710349679\n",
      "Epoch: 142 Loss: 0.0018742971249751915\n",
      "Epoch: 143, Iteration: 0, Loss: 0.0018879713024944067\n",
      "Epoch: 143, Iteration: 1, Loss: 0.0017689556116238236\n",
      "Epoch: 143, Iteration: 2, Loss: 0.0019325832836329937\n",
      "Epoch: 143, Iteration: 3, Loss: 0.0018829884938895702\n",
      "Epoch: 143, Iteration: 4, Loss: 0.0019511883147060871\n",
      "Epoch: 143, Iteration: 5, Loss: 0.0018520433222875\n",
      "Epoch: 143, Iteration: 6, Loss: 0.0019338615238666534\n",
      "Epoch: 143, Iteration: 7, Loss: 0.0017709203530102968\n",
      "Epoch: 143, Iteration: 8, Loss: 0.0018360333051532507\n",
      "Epoch: 143, Iteration: 9, Loss: 0.001967890188097954\n",
      "Epoch: 143, Iteration: 10, Loss: 0.001872289809398353\n",
      "Epoch: 143, Iteration: 11, Loss: 0.00170991662889719\n",
      "Epoch: 143, Iteration: 12, Loss: 0.0017752052517607808\n",
      "Epoch: 143, Iteration: 13, Loss: 0.0019458779133856297\n",
      "Epoch: 143, Iteration: 14, Loss: 0.0017191555816680193\n",
      "Epoch: 143, Iteration: 15, Loss: 0.0017322448547929525\n",
      "Epoch: 143, Iteration: 16, Loss: 0.0020614394452422857\n",
      "Epoch: 143, Iteration: 17, Loss: 0.0018501183949410915\n",
      "Epoch: 143, Iteration: 18, Loss: 0.0020945065189152956\n",
      "Epoch: 143, Iteration: 19, Loss: 0.0018579810857772827\n",
      "Epoch: 143, Iteration: 20, Loss: 0.0017047461587935686\n",
      "Epoch: 143, Iteration: 21, Loss: 0.0018530040979385376\n",
      "Epoch: 143, Iteration: 22, Loss: 0.0019277664832770824\n",
      "Epoch: 143, Iteration: 23, Loss: 0.0019831012468785048\n",
      "Epoch: 143, Iteration: 24, Loss: 0.00216746237128973\n",
      "Epoch: 143, Iteration: 25, Loss: 0.0016649126773700118\n",
      "Epoch: 143, Iteration: 26, Loss: 0.0019202587427571416\n",
      "Epoch: 143, Iteration: 27, Loss: 0.00187406362965703\n",
      "Epoch: 143, Iteration: 28, Loss: 0.002033814787864685\n",
      "Epoch: 143, Iteration: 29, Loss: 0.0015718452632427216\n",
      "Epoch: 143, Iteration: 30, Loss: 0.0020705678034573793\n",
      "Epoch: 143, Iteration: 31, Loss: 0.002053592586889863\n",
      "Epoch: 143, Iteration: 32, Loss: 0.0018695271573960781\n",
      "Epoch: 143, Iteration: 33, Loss: 0.0018926638877019286\n",
      "Epoch: 143, Iteration: 34, Loss: 0.0018398463726043701\n",
      "Epoch: 143, Iteration: 35, Loss: 0.0017378123011440039\n",
      "Epoch: 143, Iteration: 36, Loss: 0.0018011860083788633\n",
      "Epoch: 143, Iteration: 37, Loss: 0.0019301524152979255\n",
      "Epoch: 143, Iteration: 38, Loss: 0.0017885826528072357\n",
      "Epoch: 143, Iteration: 39, Loss: 0.00198916532099247\n",
      "Epoch: 143, Iteration: 40, Loss: 0.0018224415834993124\n",
      "Epoch: 143, Iteration: 41, Loss: 0.0017493783961981535\n",
      "Epoch: 143, Iteration: 42, Loss: 0.0018792569171637297\n",
      "Epoch: 143, Iteration: 43, Loss: 0.0018384504364803433\n",
      "Epoch: 143, Iteration: 44, Loss: 0.002086323220282793\n",
      "Epoch: 143, Iteration: 45, Loss: 0.0017466590506955981\n",
      "Epoch: 143, Iteration: 46, Loss: 0.0020463212858885527\n",
      "Epoch: 143, Iteration: 47, Loss: 0.0017198070418089628\n",
      "Epoch: 143, Iteration: 48, Loss: 0.0020232463721185923\n",
      "Epoch: 143, Iteration: 49, Loss: 0.0018802240956574678\n",
      "Epoch: 143, Iteration: 50, Loss: 0.0018693883903324604\n",
      "Epoch: 143, Iteration: 51, Loss: 0.0019568619318306446\n",
      "Epoch: 143, Iteration: 52, Loss: 0.0018647245597094297\n",
      "Epoch: 143, Iteration: 53, Loss: 0.0018630421254783869\n",
      "Epoch: 143, Iteration: 54, Loss: 0.001782341510988772\n",
      "Epoch: 143, Iteration: 55, Loss: 0.0018181930063292384\n",
      "Epoch: 143, Iteration: 56, Loss: 0.0018392346100881696\n",
      "Epoch: 143, Iteration: 57, Loss: 0.0015360236866399646\n",
      "Epoch: 143, Iteration: 58, Loss: 0.0017599149141460657\n",
      "Epoch: 143, Iteration: 59, Loss: 0.0017316360026597977\n",
      "Epoch: 143, Iteration: 60, Loss: 0.0018519879085943103\n",
      "Epoch: 143, Iteration: 61, Loss: 0.0019156323978677392\n",
      "Epoch: 143, Iteration: 62, Loss: 0.001971396617591381\n",
      "Epoch: 143, Iteration: 63, Loss: 0.0020469236187636852\n",
      "Epoch: 143, Iteration: 64, Loss: 0.0017553695943206549\n",
      "Epoch: 143, Iteration: 65, Loss: 0.0017591688083484769\n",
      "Epoch: 143, Iteration: 66, Loss: 0.0019754725508391857\n",
      "Epoch: 143, Iteration: 67, Loss: 0.001930063241161406\n",
      "Epoch: 143, Iteration: 68, Loss: 0.0017902909312397242\n",
      "Epoch: 143, Iteration: 69, Loss: 0.0019684042781591415\n",
      "Epoch: 143, Iteration: 70, Loss: 0.0018610659753903747\n",
      "Epoch: 143, Iteration: 71, Loss: 0.0017358089098706841\n",
      "Epoch: 143, Iteration: 72, Loss: 0.001911070547066629\n",
      "Epoch: 143, Iteration: 73, Loss: 0.001985145267099142\n",
      "Epoch: 143, Iteration: 74, Loss: 0.0018493977840989828\n",
      "Epoch: 143, Iteration: 75, Loss: 0.0016001667827367783\n",
      "Epoch: 143, Iteration: 76, Loss: 0.001821545185521245\n",
      "Epoch: 143, Iteration: 77, Loss: 0.0018368702149018645\n",
      "Epoch: 143, Iteration: 78, Loss: 0.0018048902275040746\n",
      "Epoch: 143, Iteration: 79, Loss: 0.0020686809439212084\n",
      "Epoch: 143, Iteration: 80, Loss: 0.0018769915914162993\n",
      "Epoch: 143, Iteration: 81, Loss: 0.001682228408753872\n",
      "Epoch: 143, Iteration: 82, Loss: 0.0018640465568751097\n",
      "Epoch: 143, Iteration: 83, Loss: 0.0020278836600482464\n",
      "Epoch: 143, Iteration: 84, Loss: 0.002111967420205474\n",
      "Epoch: 143, Iteration: 85, Loss: 0.0020241127349436283\n",
      "Epoch: 143, Iteration: 86, Loss: 0.0019204032141715288\n",
      "Epoch: 143, Iteration: 87, Loss: 0.0020926338620483875\n",
      "Epoch: 143, Iteration: 88, Loss: 0.0021302755922079086\n",
      "Epoch: 143, Iteration: 89, Loss: 0.0015874754171818495\n",
      "Epoch: 143, Iteration: 90, Loss: 0.002097908640280366\n",
      "Epoch: 143, Iteration: 91, Loss: 0.0019913397263735533\n",
      "Epoch: 143, Iteration: 92, Loss: 0.0019559517968446016\n",
      "Epoch: 143, Iteration: 93, Loss: 0.0019808781798928976\n",
      "Epoch: 143, Iteration: 94, Loss: 0.0018578986637294292\n",
      "Epoch: 143, Iteration: 95, Loss: 0.002030152129009366\n",
      "Epoch: 143, Iteration: 96, Loss: 0.0018093405524268746\n",
      "Epoch: 143, Iteration: 97, Loss: 0.001774280215613544\n",
      "Epoch: 143, Iteration: 98, Loss: 0.0019205715507268906\n",
      "Epoch: 143, Iteration: 99, Loss: 0.0016230776673182845\n",
      "Epoch: 143, Iteration: 100, Loss: 0.0017478432273492217\n",
      "Epoch: 143, Iteration: 101, Loss: 0.0018503692699596286\n",
      "Epoch: 143, Iteration: 102, Loss: 0.002134393434971571\n",
      "Epoch: 143, Iteration: 103, Loss: 0.0018272027373313904\n",
      "Epoch: 143, Iteration: 104, Loss: 0.001759927487000823\n",
      "Epoch: 143, Iteration: 105, Loss: 0.0019131164299324155\n",
      "Epoch: 143, Iteration: 106, Loss: 0.0018564030760899186\n",
      "Epoch: 143, Iteration: 107, Loss: 0.001988973468542099\n",
      "Epoch: 143, Iteration: 108, Loss: 0.0020451967138797045\n",
      "Epoch: 143, Iteration: 109, Loss: 0.001897662179544568\n",
      "Epoch: 143, Iteration: 110, Loss: 0.001988425385206938\n",
      "Epoch: 143, Iteration: 111, Loss: 0.0019127405248582363\n",
      "Epoch: 143, Iteration: 112, Loss: 0.002032040385529399\n",
      "Epoch: 143, Iteration: 113, Loss: 0.00190494186244905\n",
      "Epoch: 143, Iteration: 114, Loss: 0.002052904572337866\n",
      "Epoch: 143, Iteration: 115, Loss: 0.0018097893334925175\n",
      "Epoch: 143, Iteration: 116, Loss: 0.0018515593837946653\n",
      "Epoch: 143, Iteration: 117, Loss: 0.0019195745699107647\n",
      "Epoch: 143, Iteration: 118, Loss: 0.0021458310075104237\n",
      "Epoch: 143, Iteration: 119, Loss: 0.0019466814119368792\n",
      "Epoch: 143, Iteration: 120, Loss: 0.0019096421310678124\n",
      "Epoch: 143, Iteration: 121, Loss: 0.0019028603564947844\n",
      "Epoch: 143, Iteration: 122, Loss: 0.0018981723114848137\n",
      "Epoch: 143, Iteration: 123, Loss: 0.0018152280244976282\n",
      "Epoch: 143, Iteration: 124, Loss: 0.0018337317742407322\n",
      "Epoch: 143, Iteration: 125, Loss: 0.0019567939452826977\n",
      "Epoch: 143, Iteration: 126, Loss: 0.0018934690160676837\n",
      "Epoch: 143, Iteration: 127, Loss: 0.001713325036689639\n",
      "Epoch: 143, Iteration: 128, Loss: 0.0015909536741673946\n",
      "Epoch: 143, Iteration: 129, Loss: 0.001777800265699625\n",
      "Epoch: 143, Iteration: 130, Loss: 0.0019403877668082714\n",
      "Epoch: 143, Iteration: 131, Loss: 0.0016338680870831013\n",
      "Epoch: 143, Iteration: 132, Loss: 0.0019346660701557994\n",
      "Epoch: 143, Iteration: 133, Loss: 0.001819566823542118\n",
      "Epoch: 143, Iteration: 134, Loss: 0.0020046455319970846\n",
      "Epoch: 143, Iteration: 135, Loss: 0.0018209702102467418\n",
      "Epoch: 143, Iteration: 136, Loss: 0.0019974922761321068\n",
      "Epoch: 143, Iteration: 137, Loss: 0.0018723946996033192\n",
      "Epoch: 143, Iteration: 138, Loss: 0.0021063985768705606\n",
      "Epoch: 143, Iteration: 139, Loss: 0.0019124859245494008\n",
      "Epoch: 143, Iteration: 140, Loss: 0.0019054887816309929\n",
      "Epoch: 143, Iteration: 141, Loss: 0.0018864774610847235\n",
      "Epoch: 143, Iteration: 142, Loss: 0.0018382983980700374\n",
      "Epoch: 143, Iteration: 143, Loss: 0.0016313360538333654\n",
      "Epoch: 143, Iteration: 144, Loss: 0.0018642769427970052\n",
      "Epoch: 143, Iteration: 145, Loss: 0.0020994627848267555\n",
      "Epoch: 143, Iteration: 146, Loss: 0.0018363541457802057\n",
      "Epoch: 143, Iteration: 147, Loss: 0.0017675908748060465\n",
      "Epoch: 143, Iteration: 148, Loss: 0.0017087029991671443\n",
      "Epoch: 143, Iteration: 149, Loss: 0.001992587000131607\n",
      "Epoch: 143, Iteration: 150, Loss: 0.0020173785742372274\n",
      "Epoch: 143, Iteration: 151, Loss: 0.0018382181879132986\n",
      "Epoch: 143, Iteration: 152, Loss: 0.0018287892453372478\n",
      "Epoch: 143, Iteration: 153, Loss: 0.002011480275541544\n",
      "Epoch: 143, Iteration: 154, Loss: 0.0017413985915482044\n",
      "Epoch: 143, Iteration: 155, Loss: 0.0018118182197213173\n",
      "Epoch: 143, Iteration: 156, Loss: 0.0015727775171399117\n",
      "Epoch: 143, Iteration: 157, Loss: 0.001966841984540224\n",
      "Epoch: 143, Iteration: 158, Loss: 0.002110895002260804\n",
      "Epoch: 143, Iteration: 159, Loss: 0.0019576100166887045\n",
      "Epoch: 143, Iteration: 160, Loss: 0.0019373970571905375\n",
      "Epoch: 143, Iteration: 161, Loss: 0.0020139918196946383\n",
      "Epoch: 143, Iteration: 162, Loss: 0.0019559310749173164\n",
      "Epoch: 143, Iteration: 163, Loss: 0.0019270137418061495\n",
      "Epoch: 143, Iteration: 164, Loss: 0.0017156143439933658\n",
      "Epoch: 143, Iteration: 165, Loss: 0.0018020874122157693\n",
      "Epoch: 143, Iteration: 166, Loss: 0.0019304589368402958\n",
      "Epoch: 143, Iteration: 167, Loss: 0.0018739902880042791\n",
      "Epoch: 143, Iteration: 168, Loss: 0.0018737101927399635\n",
      "Epoch: 143, Iteration: 169, Loss: 0.0022763912566006184\n",
      "Epoch: 143, Iteration: 170, Loss: 0.0019532449077814817\n",
      "Epoch: 143, Iteration: 171, Loss: 0.0018469048663973808\n",
      "Epoch: 143, Iteration: 172, Loss: 0.002091001719236374\n",
      "Epoch: 143, Iteration: 173, Loss: 0.0020624957978725433\n",
      "Epoch: 143, Iteration: 174, Loss: 0.0020762092899531126\n",
      "Epoch: 143, Iteration: 175, Loss: 0.0020463212858885527\n",
      "Epoch: 143, Iteration: 176, Loss: 0.0017946012085303664\n",
      "Epoch: 143, Iteration: 177, Loss: 0.0018080647569149733\n",
      "Epoch: 143, Iteration: 178, Loss: 0.00176902674138546\n",
      "Epoch: 143, Iteration: 179, Loss: 0.0015515933046117425\n",
      "Epoch: 143, Iteration: 180, Loss: 0.001886816811747849\n",
      "Epoch: 143, Iteration: 181, Loss: 0.0018733956385403872\n",
      "Epoch: 143, Iteration: 182, Loss: 0.0017393012531101704\n",
      "Epoch: 143, Iteration: 183, Loss: 0.0020130574703216553\n",
      "Epoch: 143, Iteration: 184, Loss: 0.0018270767759531736\n",
      "Epoch: 143, Iteration: 185, Loss: 0.0017932854825630784\n",
      "Epoch: 143, Iteration: 186, Loss: 0.0015917089767754078\n",
      "Epoch: 143, Iteration: 187, Loss: 0.0020314594730734825\n",
      "Epoch: 143, Iteration: 188, Loss: 0.0017851603915914893\n",
      "Epoch: 143, Iteration: 189, Loss: 0.001963149756193161\n",
      "Epoch: 143, Iteration: 190, Loss: 0.001788985449820757\n",
      "Epoch: 143, Iteration: 191, Loss: 0.001746087335050106\n",
      "Epoch: 143, Iteration: 192, Loss: 0.0019418821902945638\n",
      "Epoch: 143, Iteration: 193, Loss: 0.0018589626997709274\n",
      "Epoch: 143, Iteration: 194, Loss: 0.0018975203856825829\n",
      "Epoch: 143, Iteration: 195, Loss: 0.0019225692376494408\n",
      "Epoch: 143, Iteration: 196, Loss: 0.0018478750716894865\n",
      "Epoch: 143, Iteration: 197, Loss: 0.0018207335378974676\n",
      "Epoch: 143, Iteration: 198, Loss: 0.0016664338763803244\n",
      "Epoch: 143, Iteration: 199, Loss: 0.0017865427071228623\n",
      "Epoch: 143, Iteration: 200, Loss: 0.0018814646173268557\n",
      "Epoch: 143, Iteration: 201, Loss: 0.0019160609226673841\n",
      "Epoch: 143, Iteration: 202, Loss: 0.001777233206667006\n",
      "Epoch: 143, Iteration: 203, Loss: 0.00193010491784662\n",
      "Epoch: 143, Iteration: 204, Loss: 0.0018177886959165335\n",
      "Epoch: 143, Iteration: 205, Loss: 0.002052865456789732\n",
      "Epoch: 143, Iteration: 206, Loss: 0.0018579480238258839\n",
      "Epoch: 143, Iteration: 207, Loss: 0.0018075851257890463\n",
      "Epoch: 143, Iteration: 208, Loss: 0.0017823742236942053\n",
      "Epoch: 143, Iteration: 209, Loss: 0.0021131993271410465\n",
      "Epoch: 143, Iteration: 210, Loss: 0.0017350292764604092\n",
      "Epoch: 143, Iteration: 211, Loss: 0.0019654035568237305\n",
      "Epoch: 143, Iteration: 212, Loss: 0.001991495257243514\n",
      "Epoch: 143, Iteration: 213, Loss: 0.0019131272565573454\n",
      "Epoch: 143, Iteration: 214, Loss: 0.001547573832795024\n",
      "Epoch: 143, Iteration: 215, Loss: 0.001971854828298092\n",
      "Epoch: 143, Iteration: 216, Loss: 0.0019305985188111663\n",
      "Epoch: 143, Iteration: 217, Loss: 0.0017420453950762749\n",
      "Epoch: 143, Iteration: 218, Loss: 0.0017714351415634155\n",
      "Epoch: 143, Iteration: 219, Loss: 0.0020571250934153795\n",
      "Epoch: 143, Iteration: 220, Loss: 0.0019813477993011475\n",
      "Epoch: 143, Iteration: 221, Loss: 0.0017416662303730845\n",
      "Epoch: 143, Iteration: 222, Loss: 0.0016641479451209307\n",
      "Epoch: 143, Iteration: 223, Loss: 0.0016820793971419334\n",
      "Epoch: 143, Iteration: 224, Loss: 0.001839490607380867\n",
      "Epoch: 143, Iteration: 225, Loss: 0.0018571430118754506\n",
      "Epoch: 143, Iteration: 226, Loss: 0.0019895322620868683\n",
      "Epoch: 143, Iteration: 227, Loss: 0.001985640497878194\n",
      "Epoch: 143, Iteration: 228, Loss: 0.0018633671570569277\n",
      "Epoch: 143, Iteration: 229, Loss: 0.0021470056381076574\n",
      "Epoch: 143, Iteration: 230, Loss: 0.0018561417236924171\n",
      "Epoch: 143, Iteration: 231, Loss: 0.001814054325222969\n",
      "Epoch: 143, Iteration: 232, Loss: 0.0017317282035946846\n",
      "Epoch: 143, Iteration: 233, Loss: 0.001710893353447318\n",
      "Epoch: 143, Iteration: 234, Loss: 0.0019194803899154067\n",
      "Epoch: 143, Iteration: 235, Loss: 0.0019218511879444122\n",
      "Epoch: 143, Iteration: 236, Loss: 0.0017388809937983751\n",
      "Epoch: 143, Iteration: 237, Loss: 0.0020077312365174294\n",
      "Epoch: 143, Iteration: 238, Loss: 0.001680706744082272\n",
      "Epoch: 143, Iteration: 239, Loss: 0.001879823044873774\n",
      "Epoch: 143, Iteration: 240, Loss: 0.0018409002805128694\n",
      "Epoch: 143, Iteration: 241, Loss: 0.0018257127376273274\n",
      "Epoch: 143, Iteration: 242, Loss: 0.0018565836362540722\n",
      "Epoch: 143, Iteration: 243, Loss: 0.0018707581330090761\n",
      "Epoch: 143, Iteration: 244, Loss: 0.0021181651391088963\n",
      "Epoch: 143, Iteration: 245, Loss: 0.0017262394540011883\n",
      "Epoch: 143, Iteration: 246, Loss: 0.001637680921703577\n",
      "Epoch: 143, Iteration: 247, Loss: 0.001981510315090418\n",
      "Epoch: 143, Iteration: 248, Loss: 0.0016886895755305886\n",
      "Epoch: 143, Iteration: 249, Loss: 0.0018730901647359133\n",
      "Epoch: 143, Iteration: 250, Loss: 0.001856476766988635\n",
      "Epoch: 143, Iteration: 251, Loss: 0.0018530156230553985\n",
      "Epoch: 143, Iteration: 252, Loss: 0.0018752607284113765\n",
      "Epoch: 143, Iteration: 253, Loss: 0.0018877012189477682\n",
      "Epoch: 143, Iteration: 254, Loss: 0.0017820833018049598\n",
      "Epoch: 143, Iteration: 255, Loss: 0.0020593819208443165\n",
      "Epoch: 143, Iteration: 256, Loss: 0.0019039034377783537\n",
      "Epoch: 143, Iteration: 257, Loss: 0.0020223851315677166\n",
      "Epoch: 143, Iteration: 258, Loss: 0.0017226643394678831\n",
      "Epoch: 143, Iteration: 259, Loss: 0.0018144110217690468\n",
      "Epoch: 143, Iteration: 260, Loss: 0.0020654096733778715\n",
      "Epoch: 143, Iteration: 261, Loss: 0.0018243920058012009\n",
      "Epoch: 143, Iteration: 262, Loss: 0.0018469366477802396\n",
      "Epoch: 143, Iteration: 263, Loss: 0.0017176959663629532\n",
      "Epoch: 143, Iteration: 264, Loss: 0.002177306916564703\n",
      "Epoch: 143, Iteration: 265, Loss: 0.0017790398560464382\n",
      "Epoch: 143, Iteration: 266, Loss: 0.0020548119209706783\n",
      "Epoch: 143, Iteration: 267, Loss: 0.0017044320702552795\n",
      "Epoch: 143, Iteration: 268, Loss: 0.0020001153461635113\n",
      "Epoch: 143, Iteration: 269, Loss: 0.001900573493912816\n",
      "Epoch: 143, Iteration: 270, Loss: 0.0017820666544139385\n",
      "Epoch: 143, Iteration: 271, Loss: 0.001914717722684145\n",
      "Epoch: 143, Iteration: 272, Loss: 0.0015864368760958314\n",
      "Epoch: 143, Iteration: 273, Loss: 0.0017936921212822199\n",
      "Epoch: 143, Iteration: 274, Loss: 0.0018150042742490768\n",
      "Epoch: 143 Loss: 0.0018766688166114104\n",
      "Epoch: 144, Iteration: 0, Loss: 0.0017298986203968525\n",
      "Epoch: 144, Iteration: 1, Loss: 0.0019091276917606592\n",
      "Epoch: 144, Iteration: 2, Loss: 0.0018491348018869758\n",
      "Epoch: 144, Iteration: 3, Loss: 0.0017983149737119675\n",
      "Epoch: 144, Iteration: 4, Loss: 0.0017981997225433588\n",
      "Epoch: 144, Iteration: 5, Loss: 0.001605670666322112\n",
      "Epoch: 144, Iteration: 6, Loss: 0.0017222148599103093\n",
      "Epoch: 144, Iteration: 7, Loss: 0.0019881450571119785\n",
      "Epoch: 144, Iteration: 8, Loss: 0.001813220325857401\n",
      "Epoch: 144, Iteration: 9, Loss: 0.001695603015832603\n",
      "Epoch: 144, Iteration: 10, Loss: 0.0019058152101933956\n",
      "Epoch: 144, Iteration: 11, Loss: 0.0018560506869107485\n",
      "Epoch: 144, Iteration: 12, Loss: 0.0018181067425757647\n",
      "Epoch: 144, Iteration: 13, Loss: 0.0018843596335500479\n",
      "Epoch: 144, Iteration: 14, Loss: 0.0020733147393912077\n",
      "Epoch: 144, Iteration: 15, Loss: 0.002030839677900076\n",
      "Epoch: 144, Iteration: 16, Loss: 0.0018961576279252768\n",
      "Epoch: 144, Iteration: 17, Loss: 0.001953400671482086\n",
      "Epoch: 144, Iteration: 18, Loss: 0.001915415283292532\n",
      "Epoch: 144, Iteration: 19, Loss: 0.001680441084317863\n",
      "Epoch: 144, Iteration: 20, Loss: 0.0018635555170476437\n",
      "Epoch: 144, Iteration: 21, Loss: 0.002177224028855562\n",
      "Epoch: 144, Iteration: 22, Loss: 0.001744413049891591\n",
      "Epoch: 144, Iteration: 23, Loss: 0.0022381343878805637\n",
      "Epoch: 144, Iteration: 24, Loss: 0.001677049556747079\n",
      "Epoch: 144, Iteration: 25, Loss: 0.0017855793703347445\n",
      "Epoch: 144, Iteration: 26, Loss: 0.0019599797669798136\n",
      "Epoch: 144, Iteration: 27, Loss: 0.00214290339499712\n",
      "Epoch: 144, Iteration: 28, Loss: 0.0019585127010941505\n",
      "Epoch: 144, Iteration: 29, Loss: 0.0019327939953655005\n",
      "Epoch: 144, Iteration: 30, Loss: 0.0019929897971451283\n",
      "Epoch: 144, Iteration: 31, Loss: 0.0019515582825988531\n",
      "Epoch: 144, Iteration: 32, Loss: 0.001934280153363943\n",
      "Epoch: 144, Iteration: 33, Loss: 0.001944981748238206\n",
      "Epoch: 144, Iteration: 34, Loss: 0.002051758114248514\n",
      "Epoch: 144, Iteration: 35, Loss: 0.0019330903887748718\n",
      "Epoch: 144, Iteration: 36, Loss: 0.0020155110396444798\n",
      "Epoch: 144, Iteration: 37, Loss: 0.0016551981680095196\n",
      "Epoch: 144, Iteration: 38, Loss: 0.0017801972571760416\n",
      "Epoch: 144, Iteration: 39, Loss: 0.0021459173876792192\n",
      "Epoch: 144, Iteration: 40, Loss: 0.0019005218055099249\n",
      "Epoch: 144, Iteration: 41, Loss: 0.002000275766476989\n",
      "Epoch: 144, Iteration: 42, Loss: 0.0017958495300263166\n",
      "Epoch: 144, Iteration: 43, Loss: 0.001839078962802887\n",
      "Epoch: 144, Iteration: 44, Loss: 0.0020263860933482647\n",
      "Epoch: 144, Iteration: 45, Loss: 0.002057966310530901\n",
      "Epoch: 144, Iteration: 46, Loss: 0.0020250938832759857\n",
      "Epoch: 144, Iteration: 47, Loss: 0.0018679214408621192\n",
      "Epoch: 144, Iteration: 48, Loss: 0.001762701547704637\n",
      "Epoch: 144, Iteration: 49, Loss: 0.0017117070965468884\n",
      "Epoch: 144, Iteration: 50, Loss: 0.001975397579371929\n",
      "Epoch: 144, Iteration: 51, Loss: 0.0019791594240814447\n",
      "Epoch: 144, Iteration: 52, Loss: 0.0019721202552318573\n",
      "Epoch: 144, Iteration: 53, Loss: 0.002097552875056863\n",
      "Epoch: 144, Iteration: 54, Loss: 0.001970288809388876\n",
      "Epoch: 144, Iteration: 55, Loss: 0.0018965242197737098\n",
      "Epoch: 144, Iteration: 56, Loss: 0.0016361826565116644\n",
      "Epoch: 144, Iteration: 57, Loss: 0.0018725061090663075\n",
      "Epoch: 144, Iteration: 58, Loss: 0.0020748646929860115\n",
      "Epoch: 144, Iteration: 59, Loss: 0.0017334287986159325\n",
      "Epoch: 144, Iteration: 60, Loss: 0.0018404793227091432\n",
      "Epoch: 144, Iteration: 61, Loss: 0.0016551923472434282\n",
      "Epoch: 144, Iteration: 62, Loss: 0.0017766033997759223\n",
      "Epoch: 144, Iteration: 63, Loss: 0.002017043065279722\n",
      "Epoch: 144, Iteration: 64, Loss: 0.0018490144284442067\n",
      "Epoch: 144, Iteration: 65, Loss: 0.0018303979886695743\n",
      "Epoch: 144, Iteration: 66, Loss: 0.0020178151316940784\n",
      "Epoch: 144, Iteration: 67, Loss: 0.0019814944826066494\n",
      "Epoch: 144, Iteration: 68, Loss: 0.001787083689123392\n",
      "Epoch: 144, Iteration: 69, Loss: 0.0018381814006716013\n",
      "Epoch: 144, Iteration: 70, Loss: 0.0020131818018853664\n",
      "Epoch: 144, Iteration: 71, Loss: 0.0015700774965807796\n",
      "Epoch: 144, Iteration: 72, Loss: 0.001832120236940682\n",
      "Epoch: 144, Iteration: 73, Loss: 0.001788723049685359\n",
      "Epoch: 144, Iteration: 74, Loss: 0.0020362420473247766\n",
      "Epoch: 144, Iteration: 75, Loss: 0.0022134629543870687\n",
      "Epoch: 144, Iteration: 76, Loss: 0.001870592124760151\n",
      "Epoch: 144, Iteration: 77, Loss: 0.0019546239636838436\n",
      "Epoch: 144, Iteration: 78, Loss: 0.0019931080751121044\n",
      "Epoch: 144, Iteration: 79, Loss: 0.001867301994934678\n",
      "Epoch: 144, Iteration: 80, Loss: 0.001818184508010745\n",
      "Epoch: 144, Iteration: 81, Loss: 0.0019461811752989888\n",
      "Epoch: 144, Iteration: 82, Loss: 0.0017932982882484794\n",
      "Epoch: 144, Iteration: 83, Loss: 0.0019480581395328045\n",
      "Epoch: 144, Iteration: 84, Loss: 0.0017830906435847282\n",
      "Epoch: 144, Iteration: 85, Loss: 0.002077893353998661\n",
      "Epoch: 144, Iteration: 86, Loss: 0.0017143911682069302\n",
      "Epoch: 144, Iteration: 87, Loss: 0.0021605072543025017\n",
      "Epoch: 144, Iteration: 88, Loss: 0.00186790875159204\n",
      "Epoch: 144, Iteration: 89, Loss: 0.0017734799766913056\n",
      "Epoch: 144, Iteration: 90, Loss: 0.002047186251729727\n",
      "Epoch: 144, Iteration: 91, Loss: 0.0017928456654772162\n",
      "Epoch: 144, Iteration: 92, Loss: 0.001740040723234415\n",
      "Epoch: 144, Iteration: 93, Loss: 0.0017335517331957817\n",
      "Epoch: 144, Iteration: 94, Loss: 0.0018880966817960143\n",
      "Epoch: 144, Iteration: 95, Loss: 0.0019360221922397614\n",
      "Epoch: 144, Iteration: 96, Loss: 0.0018424225272610784\n",
      "Epoch: 144, Iteration: 97, Loss: 0.0020126323215663433\n",
      "Epoch: 144, Iteration: 98, Loss: 0.0018416109960526228\n",
      "Epoch: 144, Iteration: 99, Loss: 0.0019437557784840465\n",
      "Epoch: 144, Iteration: 100, Loss: 0.0018595466390252113\n",
      "Epoch: 144, Iteration: 101, Loss: 0.0018957187421619892\n",
      "Epoch: 144, Iteration: 102, Loss: 0.001964121125638485\n",
      "Epoch: 144, Iteration: 103, Loss: 0.0017646555788815022\n",
      "Epoch: 144, Iteration: 104, Loss: 0.001781859784387052\n",
      "Epoch: 144, Iteration: 105, Loss: 0.001894211396574974\n",
      "Epoch: 144, Iteration: 106, Loss: 0.0016673108329996467\n",
      "Epoch: 144, Iteration: 107, Loss: 0.0019007197115570307\n",
      "Epoch: 144, Iteration: 108, Loss: 0.0016938250046223402\n",
      "Epoch: 144, Iteration: 109, Loss: 0.001705904258415103\n",
      "Epoch: 144, Iteration: 110, Loss: 0.0018215710297226906\n",
      "Epoch: 144, Iteration: 111, Loss: 0.0020724600180983543\n",
      "Epoch: 144, Iteration: 112, Loss: 0.0018807167652994394\n",
      "Epoch: 144, Iteration: 113, Loss: 0.0017706567887216806\n",
      "Epoch: 144, Iteration: 114, Loss: 0.0019313234370201826\n",
      "Epoch: 144, Iteration: 115, Loss: 0.0019859664607793093\n",
      "Epoch: 144, Iteration: 116, Loss: 0.0017982834251597524\n",
      "Epoch: 144, Iteration: 117, Loss: 0.0019606337882578373\n",
      "Epoch: 144, Iteration: 118, Loss: 0.001541768666356802\n",
      "Epoch: 144, Iteration: 119, Loss: 0.0019498016918078065\n",
      "Epoch: 144, Iteration: 120, Loss: 0.0018997807055711746\n",
      "Epoch: 144, Iteration: 121, Loss: 0.0019196411594748497\n",
      "Epoch: 144, Iteration: 122, Loss: 0.0016386386705562472\n",
      "Epoch: 144, Iteration: 123, Loss: 0.0017064872663468122\n",
      "Epoch: 144, Iteration: 124, Loss: 0.0018230389105156064\n",
      "Epoch: 144, Iteration: 125, Loss: 0.0018329783342778683\n",
      "Epoch: 144, Iteration: 126, Loss: 0.001967374002560973\n",
      "Epoch: 144, Iteration: 127, Loss: 0.0018379458924755454\n",
      "Epoch: 144, Iteration: 128, Loss: 0.0018534839618951082\n",
      "Epoch: 144, Iteration: 129, Loss: 0.0018556250724941492\n",
      "Epoch: 144, Iteration: 130, Loss: 0.0019556740298867226\n",
      "Epoch: 144, Iteration: 131, Loss: 0.001802300103008747\n",
      "Epoch: 144, Iteration: 132, Loss: 0.0020179825369268656\n",
      "Epoch: 144, Iteration: 133, Loss: 0.001857099705375731\n",
      "Epoch: 144, Iteration: 134, Loss: 0.0016699450789019465\n",
      "Epoch: 144, Iteration: 135, Loss: 0.001771451672539115\n",
      "Epoch: 144, Iteration: 136, Loss: 0.0015900449361652136\n",
      "Epoch: 144, Iteration: 137, Loss: 0.0017702963668853045\n",
      "Epoch: 144, Iteration: 138, Loss: 0.0018347186269238591\n",
      "Epoch: 144, Iteration: 139, Loss: 0.001996648497879505\n",
      "Epoch: 144, Iteration: 140, Loss: 0.0020119037944823503\n",
      "Epoch: 144, Iteration: 141, Loss: 0.0018324201228097081\n",
      "Epoch: 144, Iteration: 142, Loss: 0.0018653154838830233\n",
      "Epoch: 144, Iteration: 143, Loss: 0.0019533024169504642\n",
      "Epoch: 144, Iteration: 144, Loss: 0.001747939852066338\n",
      "Epoch: 144, Iteration: 145, Loss: 0.0019284577574580908\n",
      "Epoch: 144, Iteration: 146, Loss: 0.0018621661001816392\n",
      "Epoch: 144, Iteration: 147, Loss: 0.0020066043362021446\n",
      "Epoch: 144, Iteration: 148, Loss: 0.0018146871589124203\n",
      "Epoch: 144, Iteration: 149, Loss: 0.0019434296991676092\n",
      "Epoch: 144, Iteration: 150, Loss: 0.0017043864354491234\n",
      "Epoch: 144, Iteration: 151, Loss: 0.0020188381895422935\n",
      "Epoch: 144, Iteration: 152, Loss: 0.001749436603859067\n",
      "Epoch: 144, Iteration: 153, Loss: 0.001776687684468925\n",
      "Epoch: 144, Iteration: 154, Loss: 0.001731385476887226\n",
      "Epoch: 144, Iteration: 155, Loss: 0.0018601508345454931\n",
      "Epoch: 144, Iteration: 156, Loss: 0.0017859379295259714\n",
      "Epoch: 144, Iteration: 157, Loss: 0.002169906860217452\n",
      "Epoch: 144, Iteration: 158, Loss: 0.001941522816196084\n",
      "Epoch: 144, Iteration: 159, Loss: 0.0019302999135106802\n",
      "Epoch: 144, Iteration: 160, Loss: 0.001941734692081809\n",
      "Epoch: 144, Iteration: 161, Loss: 0.0019571329466998577\n",
      "Epoch: 144, Iteration: 162, Loss: 0.0017584793968126178\n",
      "Epoch: 144, Iteration: 163, Loss: 0.0017488776938989758\n",
      "Epoch: 144, Iteration: 164, Loss: 0.0018183651845902205\n",
      "Epoch: 144, Iteration: 165, Loss: 0.0017213658429682255\n",
      "Epoch: 144, Iteration: 166, Loss: 0.0019723596051335335\n",
      "Epoch: 144, Iteration: 167, Loss: 0.001893814536742866\n",
      "Epoch: 144, Iteration: 168, Loss: 0.0016910656122490764\n",
      "Epoch: 144, Iteration: 169, Loss: 0.001968656200915575\n",
      "Epoch: 144, Iteration: 170, Loss: 0.0016465606167912483\n",
      "Epoch: 144, Iteration: 171, Loss: 0.0017901742830872536\n",
      "Epoch: 144, Iteration: 172, Loss: 0.002111518057063222\n",
      "Epoch: 144, Iteration: 173, Loss: 0.0017586915055289865\n",
      "Epoch: 144, Iteration: 174, Loss: 0.0020007258281111717\n",
      "Epoch: 144, Iteration: 175, Loss: 0.0018704216927289963\n",
      "Epoch: 144, Iteration: 176, Loss: 0.001942901173606515\n",
      "Epoch: 144, Iteration: 177, Loss: 0.0017698274459689856\n",
      "Epoch: 144, Iteration: 178, Loss: 0.0020074888598173857\n",
      "Epoch: 144, Iteration: 179, Loss: 0.001875234069302678\n",
      "Epoch: 144, Iteration: 180, Loss: 0.0016455446602776647\n",
      "Epoch: 144, Iteration: 181, Loss: 0.001813977723941207\n",
      "Epoch: 144, Iteration: 182, Loss: 0.0016986310947686434\n",
      "Epoch: 144, Iteration: 183, Loss: 0.0017309177201241255\n",
      "Epoch: 144, Iteration: 184, Loss: 0.002003021538257599\n",
      "Epoch: 144, Iteration: 185, Loss: 0.0020752684213221073\n",
      "Epoch: 144, Iteration: 186, Loss: 0.0020179888233542442\n",
      "Epoch: 144, Iteration: 187, Loss: 0.001666185911744833\n",
      "Epoch: 144, Iteration: 188, Loss: 0.0018598517635837197\n",
      "Epoch: 144, Iteration: 189, Loss: 0.001916192937642336\n",
      "Epoch: 144, Iteration: 190, Loss: 0.0018679494969546795\n",
      "Epoch: 144, Iteration: 191, Loss: 0.0019690515473484993\n",
      "Epoch: 144, Iteration: 192, Loss: 0.0018281094962731004\n",
      "Epoch: 144, Iteration: 193, Loss: 0.0017847943818196654\n",
      "Epoch: 144, Iteration: 194, Loss: 0.0016494179144501686\n",
      "Epoch: 144, Iteration: 195, Loss: 0.002001808024942875\n",
      "Epoch: 144, Iteration: 196, Loss: 0.001766733592376113\n",
      "Epoch: 144, Iteration: 197, Loss: 0.001823886064812541\n",
      "Epoch: 144, Iteration: 198, Loss: 0.0020907942671328783\n",
      "Epoch: 144, Iteration: 199, Loss: 0.0018127004150301218\n",
      "Epoch: 144, Iteration: 200, Loss: 0.001887960941530764\n",
      "Epoch: 144, Iteration: 201, Loss: 0.001990402117371559\n",
      "Epoch: 144, Iteration: 202, Loss: 0.0021208995021879673\n",
      "Epoch: 144, Iteration: 203, Loss: 0.0018695988692343235\n",
      "Epoch: 144, Iteration: 204, Loss: 0.0017296619480475783\n",
      "Epoch: 144, Iteration: 205, Loss: 0.001896570436656475\n",
      "Epoch: 144, Iteration: 206, Loss: 0.0017235104460269213\n",
      "Epoch: 144, Iteration: 207, Loss: 0.0017738862661644816\n",
      "Epoch: 144, Iteration: 208, Loss: 0.0018781325779855251\n",
      "Epoch: 144, Iteration: 209, Loss: 0.0018716513877734542\n",
      "Epoch: 144, Iteration: 210, Loss: 0.0018437743419781327\n",
      "Epoch: 144, Iteration: 211, Loss: 0.0017603409942239523\n",
      "Epoch: 144, Iteration: 212, Loss: 0.001994282240048051\n",
      "Epoch: 144, Iteration: 213, Loss: 0.0018045632168650627\n",
      "Epoch: 144, Iteration: 214, Loss: 0.0017880505183711648\n",
      "Epoch: 144, Iteration: 215, Loss: 0.002094954252243042\n",
      "Epoch: 144, Iteration: 216, Loss: 0.0019214414060115814\n",
      "Epoch: 144, Iteration: 217, Loss: 0.001988791860640049\n",
      "Epoch: 144, Iteration: 218, Loss: 0.001964894123375416\n",
      "Epoch: 144, Iteration: 219, Loss: 0.0020405868999660015\n",
      "Epoch: 144, Iteration: 220, Loss: 0.0016707058530300856\n",
      "Epoch: 144, Iteration: 221, Loss: 0.0019087165128439665\n",
      "Epoch: 144, Iteration: 222, Loss: 0.0015560404863208532\n",
      "Epoch: 144, Iteration: 223, Loss: 0.002038146136328578\n",
      "Epoch: 144, Iteration: 224, Loss: 0.001975506078451872\n",
      "Epoch: 144, Iteration: 225, Loss: 0.001912658684886992\n",
      "Epoch: 144, Iteration: 226, Loss: 0.0018871917854994535\n",
      "Epoch: 144, Iteration: 227, Loss: 0.0018881296273320913\n",
      "Epoch: 144, Iteration: 228, Loss: 0.0019642887637019157\n",
      "Epoch: 144, Iteration: 229, Loss: 0.0018395587103441358\n",
      "Epoch: 144, Iteration: 230, Loss: 0.0017786243697628379\n",
      "Epoch: 144, Iteration: 231, Loss: 0.001769511727616191\n",
      "Epoch: 144, Iteration: 232, Loss: 0.0018594586290419102\n",
      "Epoch: 144, Iteration: 233, Loss: 0.0020204447209835052\n",
      "Epoch: 144, Iteration: 234, Loss: 0.0019018988823518157\n",
      "Epoch: 144, Iteration: 235, Loss: 0.0015462050214409828\n",
      "Epoch: 144, Iteration: 236, Loss: 0.0017471241299062967\n",
      "Epoch: 144, Iteration: 237, Loss: 0.0021680588833987713\n",
      "Epoch: 144, Iteration: 238, Loss: 0.0019217185908928514\n",
      "Epoch: 144, Iteration: 239, Loss: 0.0018572877161204815\n",
      "Epoch: 144, Iteration: 240, Loss: 0.002023332752287388\n",
      "Epoch: 144, Iteration: 241, Loss: 0.0018224597442895174\n",
      "Epoch: 144, Iteration: 242, Loss: 0.001921447808854282\n",
      "Epoch: 144, Iteration: 243, Loss: 0.0018506007036194205\n",
      "Epoch: 144, Iteration: 244, Loss: 0.002075992990285158\n",
      "Epoch: 144, Iteration: 245, Loss: 0.0019154628971591592\n",
      "Epoch: 144, Iteration: 246, Loss: 0.001788784982636571\n",
      "Epoch: 144, Iteration: 247, Loss: 0.0016756810946390033\n",
      "Epoch: 144, Iteration: 248, Loss: 0.0018992508994415402\n",
      "Epoch: 144, Iteration: 249, Loss: 0.002028833841904998\n",
      "Epoch: 144, Iteration: 250, Loss: 0.001681940397247672\n",
      "Epoch: 144, Iteration: 251, Loss: 0.0017977994866669178\n",
      "Epoch: 144, Iteration: 252, Loss: 0.001718559069558978\n",
      "Epoch: 144, Iteration: 253, Loss: 0.0018040152499452233\n",
      "Epoch: 144, Iteration: 254, Loss: 0.0021075874101370573\n",
      "Epoch: 144, Iteration: 255, Loss: 0.0016579173970967531\n",
      "Epoch: 144, Iteration: 256, Loss: 0.002050370443612337\n",
      "Epoch: 144, Iteration: 257, Loss: 0.0020733224228024483\n",
      "Epoch: 144, Iteration: 258, Loss: 0.001969264354556799\n",
      "Epoch: 144, Iteration: 259, Loss: 0.0017380807548761368\n",
      "Epoch: 144, Iteration: 260, Loss: 0.0017215044936165214\n",
      "Epoch: 144, Iteration: 261, Loss: 0.0018823768477886915\n",
      "Epoch: 144, Iteration: 262, Loss: 0.0016848286613821983\n",
      "Epoch: 144, Iteration: 263, Loss: 0.0020885244011878967\n",
      "Epoch: 144, Iteration: 264, Loss: 0.0018458563135936856\n",
      "Epoch: 144, Iteration: 265, Loss: 0.0019682582933455706\n",
      "Epoch: 144, Iteration: 266, Loss: 0.002015751786530018\n",
      "Epoch: 144, Iteration: 267, Loss: 0.002295631216838956\n",
      "Epoch: 144, Iteration: 268, Loss: 0.0020466658752411604\n",
      "Epoch: 144, Iteration: 269, Loss: 0.0019587527494877577\n",
      "Epoch: 144, Iteration: 270, Loss: 0.0018448114860802889\n",
      "Epoch: 144, Iteration: 271, Loss: 0.0016403761692345142\n",
      "Epoch: 144, Iteration: 272, Loss: 0.0016689857002347708\n",
      "Epoch: 144, Iteration: 273, Loss: 0.0018605963559821248\n",
      "Epoch: 144, Iteration: 274, Loss: 0.0017628488130867481\n",
      "Epoch: 144 Loss: 0.0018765428644850054\n",
      "Epoch: 145, Iteration: 0, Loss: 0.0018463227897882462\n",
      "Epoch: 145, Iteration: 1, Loss: 0.0019323433516547084\n",
      "Epoch: 145, Iteration: 2, Loss: 0.0020451671443879604\n",
      "Epoch: 145, Iteration: 3, Loss: 0.001985440496355295\n",
      "Epoch: 145, Iteration: 4, Loss: 0.0017434112960472703\n",
      "Epoch: 145, Iteration: 5, Loss: 0.0018983704503625631\n",
      "Epoch: 145, Iteration: 6, Loss: 0.0018951281672343612\n",
      "Epoch: 145, Iteration: 7, Loss: 0.0021060514263808727\n",
      "Epoch: 145, Iteration: 8, Loss: 0.0019137740600854158\n",
      "Epoch: 145, Iteration: 9, Loss: 0.001983476337045431\n",
      "Epoch: 145, Iteration: 10, Loss: 0.0021443343721330166\n",
      "Epoch: 145, Iteration: 11, Loss: 0.001942685805261135\n",
      "Epoch: 145, Iteration: 12, Loss: 0.0019500451162457466\n",
      "Epoch: 145, Iteration: 13, Loss: 0.0018951999954879284\n",
      "Epoch: 145, Iteration: 14, Loss: 0.0018985196948051453\n",
      "Epoch: 145, Iteration: 15, Loss: 0.0018308967119082808\n",
      "Epoch: 145, Iteration: 16, Loss: 0.001892442931421101\n",
      "Epoch: 145, Iteration: 17, Loss: 0.001670485595241189\n",
      "Epoch: 145, Iteration: 18, Loss: 0.002020762301981449\n",
      "Epoch: 145, Iteration: 19, Loss: 0.0017510064644739032\n",
      "Epoch: 145, Iteration: 20, Loss: 0.001867663231678307\n",
      "Epoch: 145, Iteration: 21, Loss: 0.0019462825730443\n",
      "Epoch: 145, Iteration: 22, Loss: 0.0018308801809325814\n",
      "Epoch: 145, Iteration: 23, Loss: 0.0017427275888621807\n",
      "Epoch: 145, Iteration: 24, Loss: 0.0017925800057128072\n",
      "Epoch: 145, Iteration: 25, Loss: 0.001805714564397931\n",
      "Epoch: 145, Iteration: 26, Loss: 0.0016965845134109259\n",
      "Epoch: 145, Iteration: 27, Loss: 0.0019553557503968477\n",
      "Epoch: 145, Iteration: 28, Loss: 0.001897461130283773\n",
      "Epoch: 145, Iteration: 29, Loss: 0.0020458130165934563\n",
      "Epoch: 145, Iteration: 30, Loss: 0.0019183226395398378\n",
      "Epoch: 145, Iteration: 31, Loss: 0.0019101203652098775\n",
      "Epoch: 145, Iteration: 32, Loss: 0.0017896054778248072\n",
      "Epoch: 145, Iteration: 33, Loss: 0.0019279246916994452\n",
      "Epoch: 145, Iteration: 34, Loss: 0.0017280515749007463\n",
      "Epoch: 145, Iteration: 35, Loss: 0.0018368430901318789\n",
      "Epoch: 145, Iteration: 36, Loss: 0.0018666001269593835\n",
      "Epoch: 145, Iteration: 37, Loss: 0.0019619264639914036\n",
      "Epoch: 145, Iteration: 38, Loss: 0.0017743641510605812\n",
      "Epoch: 145, Iteration: 39, Loss: 0.0019532518927007914\n",
      "Epoch: 145, Iteration: 40, Loss: 0.0018952582031488419\n",
      "Epoch: 145, Iteration: 41, Loss: 0.0018864041194319725\n",
      "Epoch: 145, Iteration: 42, Loss: 0.0019147086422890425\n",
      "Epoch: 145, Iteration: 43, Loss: 0.0018724252004176378\n",
      "Epoch: 145, Iteration: 44, Loss: 0.002018383704125881\n",
      "Epoch: 145, Iteration: 45, Loss: 0.0019816686399281025\n",
      "Epoch: 145, Iteration: 46, Loss: 0.0018392588244751096\n",
      "Epoch: 145, Iteration: 47, Loss: 0.0016388334333896637\n",
      "Epoch: 145, Iteration: 48, Loss: 0.001949642668478191\n",
      "Epoch: 145, Iteration: 49, Loss: 0.0016965067479759455\n",
      "Epoch: 145, Iteration: 50, Loss: 0.001902855234220624\n",
      "Epoch: 145, Iteration: 51, Loss: 0.0018973768455907702\n",
      "Epoch: 145, Iteration: 52, Loss: 0.0018880674615502357\n",
      "Epoch: 145, Iteration: 53, Loss: 0.0019031247356906533\n",
      "Epoch: 145, Iteration: 54, Loss: 0.0019712073262780905\n",
      "Epoch: 145, Iteration: 55, Loss: 0.0016619020607322454\n",
      "Epoch: 145, Iteration: 56, Loss: 0.0016847135266289115\n",
      "Epoch: 145, Iteration: 57, Loss: 0.0017511765472590923\n",
      "Epoch: 145, Iteration: 58, Loss: 0.0018203540239483118\n",
      "Epoch: 145, Iteration: 59, Loss: 0.0017548426985740662\n",
      "Epoch: 145, Iteration: 60, Loss: 0.001677977037616074\n",
      "Epoch: 145, Iteration: 61, Loss: 0.001807473716326058\n",
      "Epoch: 145, Iteration: 62, Loss: 0.0020576082170009613\n",
      "Epoch: 145, Iteration: 63, Loss: 0.0018878714181482792\n",
      "Epoch: 145, Iteration: 64, Loss: 0.0019093097653239965\n",
      "Epoch: 145, Iteration: 65, Loss: 0.0017768058460205793\n",
      "Epoch: 145, Iteration: 66, Loss: 0.0018273573368787766\n",
      "Epoch: 145, Iteration: 67, Loss: 0.0018385930452495813\n",
      "Epoch: 145, Iteration: 68, Loss: 0.0017583353910595179\n",
      "Epoch: 145, Iteration: 69, Loss: 0.0017652743263170123\n",
      "Epoch: 145, Iteration: 70, Loss: 0.0019430351676419377\n",
      "Epoch: 145, Iteration: 71, Loss: 0.0017400598153471947\n",
      "Epoch: 145, Iteration: 72, Loss: 0.0016876817680895329\n",
      "Epoch: 145, Iteration: 73, Loss: 0.0020107997115701437\n",
      "Epoch: 145, Iteration: 74, Loss: 0.0018446124158799648\n",
      "Epoch: 145, Iteration: 75, Loss: 0.001778175588697195\n",
      "Epoch: 145, Iteration: 76, Loss: 0.001792788039892912\n",
      "Epoch: 145, Iteration: 77, Loss: 0.0019012680277228355\n",
      "Epoch: 145, Iteration: 78, Loss: 0.0022821694146841764\n",
      "Epoch: 145, Iteration: 79, Loss: 0.0017679278971627355\n",
      "Epoch: 145, Iteration: 80, Loss: 0.0017671056557446718\n",
      "Epoch: 145, Iteration: 81, Loss: 0.001971746329218149\n",
      "Epoch: 145, Iteration: 82, Loss: 0.00182914303150028\n",
      "Epoch: 145, Iteration: 83, Loss: 0.001747317612171173\n",
      "Epoch: 145, Iteration: 84, Loss: 0.0018274817848578095\n",
      "Epoch: 145, Iteration: 85, Loss: 0.0020959549583494663\n",
      "Epoch: 145, Iteration: 86, Loss: 0.002064089523628354\n",
      "Epoch: 145, Iteration: 87, Loss: 0.0018158118473365903\n",
      "Epoch: 145, Iteration: 88, Loss: 0.0018667285330593586\n",
      "Epoch: 145, Iteration: 89, Loss: 0.0018110822420567274\n",
      "Epoch: 145, Iteration: 90, Loss: 0.0017929987516254187\n",
      "Epoch: 145, Iteration: 91, Loss: 0.0020270170643925667\n",
      "Epoch: 145, Iteration: 92, Loss: 0.002156311646103859\n",
      "Epoch: 145, Iteration: 93, Loss: 0.0017921316903084517\n",
      "Epoch: 145, Iteration: 94, Loss: 0.0018216882599517703\n",
      "Epoch: 145, Iteration: 95, Loss: 0.0016863350756466389\n",
      "Epoch: 145, Iteration: 96, Loss: 0.0019183647818863392\n",
      "Epoch: 145, Iteration: 97, Loss: 0.0018357501830905676\n",
      "Epoch: 145, Iteration: 98, Loss: 0.0017667455831542611\n",
      "Epoch: 145, Iteration: 99, Loss: 0.002014871221035719\n",
      "Epoch: 145, Iteration: 100, Loss: 0.0019473173888400197\n",
      "Epoch: 145, Iteration: 101, Loss: 0.0019573988392949104\n",
      "Epoch: 145, Iteration: 102, Loss: 0.001932320767082274\n",
      "Epoch: 145, Iteration: 103, Loss: 0.0018378577660769224\n",
      "Epoch: 145, Iteration: 104, Loss: 0.0018758405931293964\n",
      "Epoch: 145, Iteration: 105, Loss: 0.0018910274375230074\n",
      "Epoch: 145, Iteration: 106, Loss: 0.0017480612732470036\n",
      "Epoch: 145, Iteration: 107, Loss: 0.0020507858134806156\n",
      "Epoch: 145, Iteration: 108, Loss: 0.001637004199437797\n",
      "Epoch: 145, Iteration: 109, Loss: 0.0018317141802981496\n",
      "Epoch: 145, Iteration: 110, Loss: 0.0016154740005731583\n",
      "Epoch: 145, Iteration: 111, Loss: 0.0017345271771773696\n",
      "Epoch: 145, Iteration: 112, Loss: 0.0020821543876081705\n",
      "Epoch: 145, Iteration: 113, Loss: 0.001976499566808343\n",
      "Epoch: 145, Iteration: 114, Loss: 0.0018282304517924786\n",
      "Epoch: 145, Iteration: 115, Loss: 0.0017954169306904078\n",
      "Epoch: 145, Iteration: 116, Loss: 0.0019239058019593358\n",
      "Epoch: 145, Iteration: 117, Loss: 0.0018647224642336369\n",
      "Epoch: 145, Iteration: 118, Loss: 0.0018542984034866095\n",
      "Epoch: 145, Iteration: 119, Loss: 0.0019992352463304996\n",
      "Epoch: 145, Iteration: 120, Loss: 0.0020656485576182604\n",
      "Epoch: 145, Iteration: 121, Loss: 0.0020371137652546167\n",
      "Epoch: 145, Iteration: 122, Loss: 0.001987012103199959\n",
      "Epoch: 145, Iteration: 123, Loss: 0.0018047313205897808\n",
      "Epoch: 145, Iteration: 124, Loss: 0.001780269667506218\n",
      "Epoch: 145, Iteration: 125, Loss: 0.0017835297621786594\n",
      "Epoch: 145, Iteration: 126, Loss: 0.0016246330924332142\n",
      "Epoch: 145, Iteration: 127, Loss: 0.001755815465003252\n",
      "Epoch: 145, Iteration: 128, Loss: 0.0017920585814863443\n",
      "Epoch: 145, Iteration: 129, Loss: 0.0018121288157999516\n",
      "Epoch: 145, Iteration: 130, Loss: 0.0020527078304439783\n",
      "Epoch: 145, Iteration: 131, Loss: 0.0018948725191876292\n",
      "Epoch: 145, Iteration: 132, Loss: 0.0017817143816500902\n",
      "Epoch: 145, Iteration: 133, Loss: 0.0019930743146687746\n",
      "Epoch: 145, Iteration: 134, Loss: 0.0019401205936446786\n",
      "Epoch: 145, Iteration: 135, Loss: 0.001794560463167727\n",
      "Epoch: 145, Iteration: 136, Loss: 0.0018690431024879217\n",
      "Epoch: 145, Iteration: 137, Loss: 0.0018736675847321749\n",
      "Epoch: 145, Iteration: 138, Loss: 0.0019207298755645752\n",
      "Epoch: 145, Iteration: 139, Loss: 0.0015684148529544473\n",
      "Epoch: 145, Iteration: 140, Loss: 0.0018409342737868428\n",
      "Epoch: 145, Iteration: 141, Loss: 0.001976425526663661\n",
      "Epoch: 145, Iteration: 142, Loss: 0.0018932449165731668\n",
      "Epoch: 145, Iteration: 143, Loss: 0.001959839602932334\n",
      "Epoch: 145, Iteration: 144, Loss: 0.0020200402941554785\n",
      "Epoch: 145, Iteration: 145, Loss: 0.0019866982474923134\n",
      "Epoch: 145, Iteration: 146, Loss: 0.0018350869650021195\n",
      "Epoch: 145, Iteration: 147, Loss: 0.0017051156610250473\n",
      "Epoch: 145, Iteration: 148, Loss: 0.0019503943622112274\n",
      "Epoch: 145, Iteration: 149, Loss: 0.002158215269446373\n",
      "Epoch: 145, Iteration: 150, Loss: 0.0021147383376955986\n",
      "Epoch: 145, Iteration: 151, Loss: 0.0020390315912663937\n",
      "Epoch: 145, Iteration: 152, Loss: 0.001959612825885415\n",
      "Epoch: 145, Iteration: 153, Loss: 0.0018339045345783234\n",
      "Epoch: 145, Iteration: 154, Loss: 0.0017814222956076264\n",
      "Epoch: 145, Iteration: 155, Loss: 0.0018938416615128517\n",
      "Epoch: 145, Iteration: 156, Loss: 0.001600360032171011\n",
      "Epoch: 145, Iteration: 157, Loss: 0.001927003962919116\n",
      "Epoch: 145, Iteration: 158, Loss: 0.0017105690203607082\n",
      "Epoch: 145, Iteration: 159, Loss: 0.0020085377618670464\n",
      "Epoch: 145, Iteration: 160, Loss: 0.001994131598621607\n",
      "Epoch: 145, Iteration: 161, Loss: 0.0017096679657697678\n",
      "Epoch: 145, Iteration: 162, Loss: 0.0020040548406541348\n",
      "Epoch: 145, Iteration: 163, Loss: 0.002052489435300231\n",
      "Epoch: 145, Iteration: 164, Loss: 0.0017484072595834732\n",
      "Epoch: 145, Iteration: 165, Loss: 0.001826232299208641\n",
      "Epoch: 145, Iteration: 166, Loss: 0.0017799625638872385\n",
      "Epoch: 145, Iteration: 167, Loss: 0.0019484515069052577\n",
      "Epoch: 145, Iteration: 168, Loss: 0.002128293039277196\n",
      "Epoch: 145, Iteration: 169, Loss: 0.0018168202368542552\n",
      "Epoch: 145, Iteration: 170, Loss: 0.0018288661958649755\n",
      "Epoch: 145, Iteration: 171, Loss: 0.0018531596288084984\n",
      "Epoch: 145, Iteration: 172, Loss: 0.002053569070994854\n",
      "Epoch: 145, Iteration: 173, Loss: 0.0019816397689282894\n",
      "Epoch: 145, Iteration: 174, Loss: 0.0018890633946284652\n",
      "Epoch: 145, Iteration: 175, Loss: 0.001845686580054462\n",
      "Epoch: 145, Iteration: 176, Loss: 0.0015691894805058837\n",
      "Epoch: 145, Iteration: 177, Loss: 0.0019580950029194355\n",
      "Epoch: 145, Iteration: 178, Loss: 0.0017058259109035134\n",
      "Epoch: 145, Iteration: 179, Loss: 0.002017844468355179\n",
      "Epoch: 145, Iteration: 180, Loss: 0.001977505162358284\n",
      "Epoch: 145, Iteration: 181, Loss: 0.0018559491727501154\n",
      "Epoch: 145, Iteration: 182, Loss: 0.0021755287889391184\n",
      "Epoch: 145, Iteration: 183, Loss: 0.0018856233218684793\n",
      "Epoch: 145, Iteration: 184, Loss: 0.001775127137079835\n",
      "Epoch: 145, Iteration: 185, Loss: 0.0018271553562954068\n",
      "Epoch: 145, Iteration: 186, Loss: 0.0018966956995427608\n",
      "Epoch: 145, Iteration: 187, Loss: 0.0019162239041179419\n",
      "Epoch: 145, Iteration: 188, Loss: 0.0018318991642445326\n",
      "Epoch: 145, Iteration: 189, Loss: 0.001933454885147512\n",
      "Epoch: 145, Iteration: 190, Loss: 0.0016813676338642836\n",
      "Epoch: 145, Iteration: 191, Loss: 0.0017207683995366096\n",
      "Epoch: 145, Iteration: 192, Loss: 0.0019004200585186481\n",
      "Epoch: 145, Iteration: 193, Loss: 0.0018947725184261799\n",
      "Epoch: 145, Iteration: 194, Loss: 0.0019926060922443867\n",
      "Epoch: 145, Iteration: 195, Loss: 0.0018139254534617066\n",
      "Epoch: 145, Iteration: 196, Loss: 0.0016879844479262829\n",
      "Epoch: 145, Iteration: 197, Loss: 0.0018667749827727675\n",
      "Epoch: 145, Iteration: 198, Loss: 0.0019635306671261787\n",
      "Epoch: 145, Iteration: 199, Loss: 0.0019326945766806602\n",
      "Epoch: 145, Iteration: 200, Loss: 0.0018574203131720424\n",
      "Epoch: 145, Iteration: 201, Loss: 0.0018137035658583045\n",
      "Epoch: 145, Iteration: 202, Loss: 0.002019567647948861\n",
      "Epoch: 145, Iteration: 203, Loss: 0.0021060281433165073\n",
      "Epoch: 145, Iteration: 204, Loss: 0.0018372081685811281\n",
      "Epoch: 145, Iteration: 205, Loss: 0.0019591976888477802\n",
      "Epoch: 145, Iteration: 206, Loss: 0.0019021757179871202\n",
      "Epoch: 145, Iteration: 207, Loss: 0.001881544478237629\n",
      "Epoch: 145, Iteration: 208, Loss: 0.0019533103331923485\n",
      "Epoch: 145, Iteration: 209, Loss: 0.0020593495573848486\n",
      "Epoch: 145, Iteration: 210, Loss: 0.0019973558373749256\n",
      "Epoch: 145, Iteration: 211, Loss: 0.0017877959180623293\n",
      "Epoch: 145, Iteration: 212, Loss: 0.0019577061757445335\n",
      "Epoch: 145, Iteration: 213, Loss: 0.0017234192928299308\n",
      "Epoch: 145, Iteration: 214, Loss: 0.0016770437359809875\n",
      "Epoch: 145, Iteration: 215, Loss: 0.001885966514237225\n",
      "Epoch: 145, Iteration: 216, Loss: 0.0017467787256464362\n",
      "Epoch: 145, Iteration: 217, Loss: 0.0020655710250139236\n",
      "Epoch: 145, Iteration: 218, Loss: 0.001754714990966022\n",
      "Epoch: 145, Iteration: 219, Loss: 0.0018706634873524308\n",
      "Epoch: 145, Iteration: 220, Loss: 0.0017382698133587837\n",
      "Epoch: 145, Iteration: 221, Loss: 0.0020799911580979824\n",
      "Epoch: 145, Iteration: 222, Loss: 0.001782174571417272\n",
      "Epoch: 145, Iteration: 223, Loss: 0.0018678971100598574\n",
      "Epoch: 145, Iteration: 224, Loss: 0.0019413224654272199\n",
      "Epoch: 145, Iteration: 225, Loss: 0.001846739323809743\n",
      "Epoch: 145, Iteration: 226, Loss: 0.0018483555177226663\n",
      "Epoch: 145, Iteration: 227, Loss: 0.0016784684266895056\n",
      "Epoch: 145, Iteration: 228, Loss: 0.0019963858649134636\n",
      "Epoch: 145, Iteration: 229, Loss: 0.0017350264824926853\n",
      "Epoch: 145, Iteration: 230, Loss: 0.0019304538145661354\n",
      "Epoch: 145, Iteration: 231, Loss: 0.0017502086702734232\n",
      "Epoch: 145, Iteration: 232, Loss: 0.0017055114731192589\n",
      "Epoch: 145, Iteration: 233, Loss: 0.001996746053919196\n",
      "Epoch: 145, Iteration: 234, Loss: 0.0018321657553315163\n",
      "Epoch: 145, Iteration: 235, Loss: 0.0019673865754157305\n",
      "Epoch: 145, Iteration: 236, Loss: 0.001683864975348115\n",
      "Epoch: 145, Iteration: 237, Loss: 0.0017136167734861374\n",
      "Epoch: 145, Iteration: 238, Loss: 0.0019962755031883717\n",
      "Epoch: 145, Iteration: 239, Loss: 0.0016372716054320335\n",
      "Epoch: 145, Iteration: 240, Loss: 0.0017959456890821457\n",
      "Epoch: 145, Iteration: 241, Loss: 0.0017714080167934299\n",
      "Epoch: 145, Iteration: 242, Loss: 0.0018015159294009209\n",
      "Epoch: 145, Iteration: 243, Loss: 0.002073311246931553\n",
      "Epoch: 145, Iteration: 244, Loss: 0.0019650603644549847\n",
      "Epoch: 145, Iteration: 245, Loss: 0.0018442291766405106\n",
      "Epoch: 145, Iteration: 246, Loss: 0.0018976645078510046\n",
      "Epoch: 145, Iteration: 247, Loss: 0.0020821932703256607\n",
      "Epoch: 145, Iteration: 248, Loss: 0.0019918596372008324\n",
      "Epoch: 145, Iteration: 249, Loss: 0.0018542003817856312\n",
      "Epoch: 145, Iteration: 250, Loss: 0.002006641123443842\n",
      "Epoch: 145, Iteration: 251, Loss: 0.0016334375832229853\n",
      "Epoch: 145, Iteration: 252, Loss: 0.0018267936538904905\n",
      "Epoch: 145, Iteration: 253, Loss: 0.0017197281122207642\n",
      "Epoch: 145, Iteration: 254, Loss: 0.0020554426591843367\n",
      "Epoch: 145, Iteration: 255, Loss: 0.0017442815005779266\n",
      "Epoch: 145, Iteration: 256, Loss: 0.0016255849041044712\n",
      "Epoch: 145, Iteration: 257, Loss: 0.0018541248282417655\n",
      "Epoch: 145, Iteration: 258, Loss: 0.0019847601652145386\n",
      "Epoch: 145, Iteration: 259, Loss: 0.001906800433062017\n",
      "Epoch: 145, Iteration: 260, Loss: 0.0017353855073451996\n",
      "Epoch: 145, Iteration: 261, Loss: 0.0017505073919892311\n",
      "Epoch: 145, Iteration: 262, Loss: 0.002097614575177431\n",
      "Epoch: 145, Iteration: 263, Loss: 0.0018553967820480466\n",
      "Epoch: 145, Iteration: 264, Loss: 0.0017733252607285976\n",
      "Epoch: 145, Iteration: 265, Loss: 0.002073629293590784\n",
      "Epoch: 145, Iteration: 266, Loss: 0.0018710094736889005\n",
      "Epoch: 145, Iteration: 267, Loss: 0.0018910651560872793\n",
      "Epoch: 145, Iteration: 268, Loss: 0.0016939929919317365\n",
      "Epoch: 145, Iteration: 269, Loss: 0.0018205383094027638\n",
      "Epoch: 145, Iteration: 270, Loss: 0.002127411775290966\n",
      "Epoch: 145, Iteration: 271, Loss: 0.001895940862596035\n",
      "Epoch: 145, Iteration: 272, Loss: 0.002010757103562355\n",
      "Epoch: 145, Iteration: 273, Loss: 0.0022348773200064898\n",
      "Epoch: 145, Iteration: 274, Loss: 0.0018066251650452614\n",
      "Epoch: 145 Loss: 0.0018765824358734763\n",
      "Epoch: 146, Iteration: 0, Loss: 0.0018855354283005\n",
      "Epoch: 146, Iteration: 1, Loss: 0.0016522828955203295\n",
      "Epoch: 146, Iteration: 2, Loss: 0.0017756614834070206\n",
      "Epoch: 146, Iteration: 3, Loss: 0.0018970234086737037\n",
      "Epoch: 146, Iteration: 4, Loss: 0.0020538694225251675\n",
      "Epoch: 146, Iteration: 5, Loss: 0.0017173169180750847\n",
      "Epoch: 146, Iteration: 6, Loss: 0.0015477753477171063\n",
      "Epoch: 146, Iteration: 7, Loss: 0.002138076815754175\n",
      "Epoch: 146, Iteration: 8, Loss: 0.0016127561684697866\n",
      "Epoch: 146, Iteration: 9, Loss: 0.0019752089865505695\n",
      "Epoch: 146, Iteration: 10, Loss: 0.0018057005945593119\n",
      "Epoch: 146, Iteration: 11, Loss: 0.0022076969034969807\n",
      "Epoch: 146, Iteration: 12, Loss: 0.001921378425322473\n",
      "Epoch: 146, Iteration: 13, Loss: 0.001665992196649313\n",
      "Epoch: 146, Iteration: 14, Loss: 0.0021069315262138844\n",
      "Epoch: 146, Iteration: 15, Loss: 0.0018500364385545254\n",
      "Epoch: 146, Iteration: 16, Loss: 0.0017496682703495026\n",
      "Epoch: 146, Iteration: 17, Loss: 0.0018791214097291231\n",
      "Epoch: 146, Iteration: 18, Loss: 0.0019282873254269361\n",
      "Epoch: 146, Iteration: 19, Loss: 0.002030970063060522\n",
      "Epoch: 146, Iteration: 20, Loss: 0.001911998726427555\n",
      "Epoch: 146, Iteration: 21, Loss: 0.0018144643399864435\n",
      "Epoch: 146, Iteration: 22, Loss: 0.001737802755087614\n",
      "Epoch: 146, Iteration: 23, Loss: 0.0016962450463324785\n",
      "Epoch: 146, Iteration: 24, Loss: 0.0017270889366045594\n",
      "Epoch: 146, Iteration: 25, Loss: 0.001890647574327886\n",
      "Epoch: 146, Iteration: 26, Loss: 0.0018083908362314105\n",
      "Epoch: 146, Iteration: 27, Loss: 0.0017413708847016096\n",
      "Epoch: 146, Iteration: 28, Loss: 0.0020050539169460535\n",
      "Epoch: 146, Iteration: 29, Loss: 0.0019385367631912231\n",
      "Epoch: 146, Iteration: 30, Loss: 0.001999227562919259\n",
      "Epoch: 146, Iteration: 31, Loss: 0.0017113378271460533\n",
      "Epoch: 146, Iteration: 32, Loss: 0.0016890168190002441\n",
      "Epoch: 146, Iteration: 33, Loss: 0.0017440780065953732\n",
      "Epoch: 146, Iteration: 34, Loss: 0.0018209802219644189\n",
      "Epoch: 146, Iteration: 35, Loss: 0.0018734431359916925\n",
      "Epoch: 146, Iteration: 36, Loss: 0.0018129432573914528\n",
      "Epoch: 146, Iteration: 37, Loss: 0.0020504938438534737\n",
      "Epoch: 146, Iteration: 38, Loss: 0.0018324942793697119\n",
      "Epoch: 146, Iteration: 39, Loss: 0.0016896442975848913\n",
      "Epoch: 146, Iteration: 40, Loss: 0.0019505203235894442\n",
      "Epoch: 146, Iteration: 41, Loss: 0.001727570896036923\n",
      "Epoch: 146, Iteration: 42, Loss: 0.0019479505717754364\n",
      "Epoch: 146, Iteration: 43, Loss: 0.0018861053977161646\n",
      "Epoch: 146, Iteration: 44, Loss: 0.0019392309477552772\n",
      "Epoch: 146, Iteration: 45, Loss: 0.0020466966088861227\n",
      "Epoch: 146, Iteration: 46, Loss: 0.0018172190757468343\n",
      "Epoch: 146, Iteration: 47, Loss: 0.0019990222062915564\n",
      "Epoch: 146, Iteration: 48, Loss: 0.0020268457010388374\n",
      "Epoch: 146, Iteration: 49, Loss: 0.001895363675430417\n",
      "Epoch: 146, Iteration: 50, Loss: 0.001808085711672902\n",
      "Epoch: 146, Iteration: 51, Loss: 0.0018746014684438705\n",
      "Epoch: 146, Iteration: 52, Loss: 0.001784594962373376\n",
      "Epoch: 146, Iteration: 53, Loss: 0.00163556938059628\n",
      "Epoch: 146, Iteration: 54, Loss: 0.0019030841067433357\n",
      "Epoch: 146, Iteration: 55, Loss: 0.001999423373490572\n",
      "Epoch: 146, Iteration: 56, Loss: 0.001844688318669796\n",
      "Epoch: 146, Iteration: 57, Loss: 0.0018207939574494958\n",
      "Epoch: 146, Iteration: 58, Loss: 0.001701336819678545\n",
      "Epoch: 146, Iteration: 59, Loss: 0.001756979268975556\n",
      "Epoch: 146, Iteration: 60, Loss: 0.00197106646373868\n",
      "Epoch: 146, Iteration: 61, Loss: 0.0019012048142030835\n",
      "Epoch: 146, Iteration: 62, Loss: 0.0017795704770833254\n",
      "Epoch: 146, Iteration: 63, Loss: 0.0018797386437654495\n",
      "Epoch: 146, Iteration: 64, Loss: 0.0017644022591412067\n",
      "Epoch: 146, Iteration: 65, Loss: 0.00188321596942842\n",
      "Epoch: 146, Iteration: 66, Loss: 0.0017981105484068394\n",
      "Epoch: 146, Iteration: 67, Loss: 0.001760307583026588\n",
      "Epoch: 146, Iteration: 68, Loss: 0.0018987939693033695\n",
      "Epoch: 146, Iteration: 69, Loss: 0.0016831159591674805\n",
      "Epoch: 146, Iteration: 70, Loss: 0.0018930906662717462\n",
      "Epoch: 146, Iteration: 71, Loss: 0.0018622069619596004\n",
      "Epoch: 146, Iteration: 72, Loss: 0.0018832937348634005\n",
      "Epoch: 146, Iteration: 73, Loss: 0.001994058955460787\n",
      "Epoch: 146, Iteration: 74, Loss: 0.0018742664251476526\n",
      "Epoch: 146, Iteration: 75, Loss: 0.0018038606503978372\n",
      "Epoch: 146, Iteration: 76, Loss: 0.0018259893404319882\n",
      "Epoch: 146, Iteration: 77, Loss: 0.0019937255419790745\n",
      "Epoch: 146, Iteration: 78, Loss: 0.0020491124596446753\n",
      "Epoch: 146, Iteration: 79, Loss: 0.0019625856075435877\n",
      "Epoch: 146, Iteration: 80, Loss: 0.0019155567279085517\n",
      "Epoch: 146, Iteration: 81, Loss: 0.001790671027265489\n",
      "Epoch: 146, Iteration: 82, Loss: 0.001729617710225284\n",
      "Epoch: 146, Iteration: 83, Loss: 0.001904396922327578\n",
      "Epoch: 146, Iteration: 84, Loss: 0.001828499254770577\n",
      "Epoch: 146, Iteration: 85, Loss: 0.0021229307167232037\n",
      "Epoch: 146, Iteration: 86, Loss: 0.0016228251624852419\n",
      "Epoch: 146, Iteration: 87, Loss: 0.0016963519155979156\n",
      "Epoch: 146, Iteration: 88, Loss: 0.0019613574258983135\n",
      "Epoch: 146, Iteration: 89, Loss: 0.0017585858004167676\n",
      "Epoch: 146, Iteration: 90, Loss: 0.0020321430638432503\n",
      "Epoch: 146, Iteration: 91, Loss: 0.0017512256745249033\n",
      "Epoch: 146, Iteration: 92, Loss: 0.0019448131788522005\n",
      "Epoch: 146, Iteration: 93, Loss: 0.0020136702805757523\n",
      "Epoch: 146, Iteration: 94, Loss: 0.0017805049428716302\n",
      "Epoch: 146, Iteration: 95, Loss: 0.001849150750786066\n",
      "Epoch: 146, Iteration: 96, Loss: 0.001916264183819294\n",
      "Epoch: 146, Iteration: 97, Loss: 0.0018822841811925173\n",
      "Epoch: 146, Iteration: 98, Loss: 0.001944489311426878\n",
      "Epoch: 146, Iteration: 99, Loss: 0.002039303071796894\n",
      "Epoch: 146, Iteration: 100, Loss: 0.0016743623418733478\n",
      "Epoch: 146, Iteration: 101, Loss: 0.0017913560150191188\n",
      "Epoch: 146, Iteration: 102, Loss: 0.0015603750944137573\n",
      "Epoch: 146, Iteration: 103, Loss: 0.0018755939090624452\n",
      "Epoch: 146, Iteration: 104, Loss: 0.0018300563097000122\n",
      "Epoch: 146, Iteration: 105, Loss: 0.0018641778733581305\n",
      "Epoch: 146, Iteration: 106, Loss: 0.0016515201423317194\n",
      "Epoch: 146, Iteration: 107, Loss: 0.0015866332687437534\n",
      "Epoch: 146, Iteration: 108, Loss: 0.001883000717498362\n",
      "Epoch: 146, Iteration: 109, Loss: 0.0018720274092629552\n",
      "Epoch: 146, Iteration: 110, Loss: 0.0019882586784660816\n",
      "Epoch: 146, Iteration: 111, Loss: 0.0016185189597308636\n",
      "Epoch: 146, Iteration: 112, Loss: 0.001992232631891966\n",
      "Epoch: 146, Iteration: 113, Loss: 0.0018839356489479542\n",
      "Epoch: 146, Iteration: 114, Loss: 0.0016833397094160318\n",
      "Epoch: 146, Iteration: 115, Loss: 0.002089991234242916\n",
      "Epoch: 146, Iteration: 116, Loss: 0.0018515716074034572\n",
      "Epoch: 146, Iteration: 117, Loss: 0.0018743711989372969\n",
      "Epoch: 146, Iteration: 118, Loss: 0.0016730631468817592\n",
      "Epoch: 146, Iteration: 119, Loss: 0.001931049395352602\n",
      "Epoch: 146, Iteration: 120, Loss: 0.0020072627812623978\n",
      "Epoch: 146, Iteration: 121, Loss: 0.0018398752436041832\n",
      "Epoch: 146, Iteration: 122, Loss: 0.0019840216264128685\n",
      "Epoch: 146, Iteration: 123, Loss: 0.0019744387827813625\n",
      "Epoch: 146, Iteration: 124, Loss: 0.0017216887790709734\n",
      "Epoch: 146, Iteration: 125, Loss: 0.0018250442808493972\n",
      "Epoch: 146, Iteration: 126, Loss: 0.001906899269670248\n",
      "Epoch: 146, Iteration: 127, Loss: 0.001978042535483837\n",
      "Epoch: 146, Iteration: 128, Loss: 0.0018855512607842684\n",
      "Epoch: 146, Iteration: 129, Loss: 0.0019997842609882355\n",
      "Epoch: 146, Iteration: 130, Loss: 0.0019796337001025677\n",
      "Epoch: 146, Iteration: 131, Loss: 0.0017657015705481172\n",
      "Epoch: 146, Iteration: 132, Loss: 0.001945190248079598\n",
      "Epoch: 146, Iteration: 133, Loss: 0.0018895854009315372\n",
      "Epoch: 146, Iteration: 134, Loss: 0.002151853172108531\n",
      "Epoch: 146, Iteration: 135, Loss: 0.002030058763921261\n",
      "Epoch: 146, Iteration: 136, Loss: 0.0019503467483446002\n",
      "Epoch: 146, Iteration: 137, Loss: 0.0019084312953054905\n",
      "Epoch: 146, Iteration: 138, Loss: 0.0019041515188291669\n",
      "Epoch: 146, Iteration: 139, Loss: 0.0020343444775789976\n",
      "Epoch: 146, Iteration: 140, Loss: 0.0020057233050465584\n",
      "Epoch: 146, Iteration: 141, Loss: 0.001779479905962944\n",
      "Epoch: 146, Iteration: 142, Loss: 0.0016590665327385068\n",
      "Epoch: 146, Iteration: 143, Loss: 0.001747032511048019\n",
      "Epoch: 146, Iteration: 144, Loss: 0.001934911822900176\n",
      "Epoch: 146, Iteration: 145, Loss: 0.0018703060923144221\n",
      "Epoch: 146, Iteration: 146, Loss: 0.002017606981098652\n",
      "Epoch: 146, Iteration: 147, Loss: 0.0016383351758122444\n",
      "Epoch: 146, Iteration: 148, Loss: 0.0020351826678961515\n",
      "Epoch: 146, Iteration: 149, Loss: 0.001981123350560665\n",
      "Epoch: 146, Iteration: 150, Loss: 0.0018427739851176739\n",
      "Epoch: 146, Iteration: 151, Loss: 0.001995737198740244\n",
      "Epoch: 146, Iteration: 152, Loss: 0.0018177905585616827\n",
      "Epoch: 146, Iteration: 153, Loss: 0.0017132461071014404\n",
      "Epoch: 146, Iteration: 154, Loss: 0.0020665526390075684\n",
      "Epoch: 146, Iteration: 155, Loss: 0.002188151702284813\n",
      "Epoch: 146, Iteration: 156, Loss: 0.0020044008269906044\n",
      "Epoch: 146, Iteration: 157, Loss: 0.0018629176774993539\n",
      "Epoch: 146, Iteration: 158, Loss: 0.0019045050721615553\n",
      "Epoch: 146, Iteration: 159, Loss: 0.0017451413441449404\n",
      "Epoch: 146, Iteration: 160, Loss: 0.002011605305597186\n",
      "Epoch: 146, Iteration: 161, Loss: 0.0017851695884019136\n",
      "Epoch: 146, Iteration: 162, Loss: 0.0017360239289700985\n",
      "Epoch: 146, Iteration: 163, Loss: 0.0019005611538887024\n",
      "Epoch: 146, Iteration: 164, Loss: 0.0017218872671946883\n",
      "Epoch: 146, Iteration: 165, Loss: 0.0020301025360822678\n",
      "Epoch: 146, Iteration: 166, Loss: 0.0019056687597185373\n",
      "Epoch: 146, Iteration: 167, Loss: 0.001785761909559369\n",
      "Epoch: 146, Iteration: 168, Loss: 0.001732404576614499\n",
      "Epoch: 146, Iteration: 169, Loss: 0.0019480345072224736\n",
      "Epoch: 146, Iteration: 170, Loss: 0.0017696956638246775\n",
      "Epoch: 146, Iteration: 171, Loss: 0.0018600176554173231\n",
      "Epoch: 146, Iteration: 172, Loss: 0.0019861990585923195\n",
      "Epoch: 146, Iteration: 173, Loss: 0.002166356658563018\n",
      "Epoch: 146, Iteration: 174, Loss: 0.0017835386097431183\n",
      "Epoch: 146, Iteration: 175, Loss: 0.0017544673755764961\n",
      "Epoch: 146, Iteration: 176, Loss: 0.002067179186269641\n",
      "Epoch: 146, Iteration: 177, Loss: 0.0019953041337430477\n",
      "Epoch: 146, Iteration: 178, Loss: 0.0019028524402529001\n",
      "Epoch: 146, Iteration: 179, Loss: 0.0018450372153893113\n",
      "Epoch: 146, Iteration: 180, Loss: 0.0017891207244247198\n",
      "Epoch: 146, Iteration: 181, Loss: 0.0016984253888949752\n",
      "Epoch: 146, Iteration: 182, Loss: 0.0017104189610108733\n",
      "Epoch: 146, Iteration: 183, Loss: 0.0019924102816730738\n",
      "Epoch: 146, Iteration: 184, Loss: 0.0019689826294779778\n",
      "Epoch: 146, Iteration: 185, Loss: 0.0018837249372154474\n",
      "Epoch: 146, Iteration: 186, Loss: 0.001978897722437978\n",
      "Epoch: 146, Iteration: 187, Loss: 0.001897945418022573\n",
      "Epoch: 146, Iteration: 188, Loss: 0.0016761330189183354\n",
      "Epoch: 146, Iteration: 189, Loss: 0.001964663853868842\n",
      "Epoch: 146, Iteration: 190, Loss: 0.0016437391750514507\n",
      "Epoch: 146, Iteration: 191, Loss: 0.0018710565054789186\n",
      "Epoch: 146, Iteration: 192, Loss: 0.0018513256218284369\n",
      "Epoch: 146, Iteration: 193, Loss: 0.0017221898306161165\n",
      "Epoch: 146, Iteration: 194, Loss: 0.0021118270233273506\n",
      "Epoch: 146, Iteration: 195, Loss: 0.0018240346107631922\n",
      "Epoch: 146, Iteration: 196, Loss: 0.001836557756178081\n",
      "Epoch: 146, Iteration: 197, Loss: 0.0018741724779829383\n",
      "Epoch: 146, Iteration: 198, Loss: 0.0018429027404636145\n",
      "Epoch: 146, Iteration: 199, Loss: 0.00185720669105649\n",
      "Epoch: 146, Iteration: 200, Loss: 0.001705150818452239\n",
      "Epoch: 146, Iteration: 201, Loss: 0.0017578995320945978\n",
      "Epoch: 146, Iteration: 202, Loss: 0.00190895632840693\n",
      "Epoch: 146, Iteration: 203, Loss: 0.0021305293776094913\n",
      "Epoch: 146, Iteration: 204, Loss: 0.0019273769576102495\n",
      "Epoch: 146, Iteration: 205, Loss: 0.0019185581477358937\n",
      "Epoch: 146, Iteration: 206, Loss: 0.0019739149138331413\n",
      "Epoch: 146, Iteration: 207, Loss: 0.001800086349248886\n",
      "Epoch: 146, Iteration: 208, Loss: 0.001740346779115498\n",
      "Epoch: 146, Iteration: 209, Loss: 0.0020277437288314104\n",
      "Epoch: 146, Iteration: 210, Loss: 0.0019501851638779044\n",
      "Epoch: 146, Iteration: 211, Loss: 0.001698099309578538\n",
      "Epoch: 146, Iteration: 212, Loss: 0.0017499167006462812\n",
      "Epoch: 146, Iteration: 213, Loss: 0.0019504657248035073\n",
      "Epoch: 146, Iteration: 214, Loss: 0.0018434168305248022\n",
      "Epoch: 146, Iteration: 215, Loss: 0.0018933545798063278\n",
      "Epoch: 146, Iteration: 216, Loss: 0.0020976769737899303\n",
      "Epoch: 146, Iteration: 217, Loss: 0.0020689847879111767\n",
      "Epoch: 146, Iteration: 218, Loss: 0.002090261783450842\n",
      "Epoch: 146, Iteration: 219, Loss: 0.0017775355372577906\n",
      "Epoch: 146, Iteration: 220, Loss: 0.0017881887033581734\n",
      "Epoch: 146, Iteration: 221, Loss: 0.0019287478644400835\n",
      "Epoch: 146, Iteration: 222, Loss: 0.0018339999951422215\n",
      "Epoch: 146, Iteration: 223, Loss: 0.002009454183280468\n",
      "Epoch: 146, Iteration: 224, Loss: 0.0018805271247401834\n",
      "Epoch: 146, Iteration: 225, Loss: 0.0019003545166924596\n",
      "Epoch: 146, Iteration: 226, Loss: 0.0017980601405724883\n",
      "Epoch: 146, Iteration: 227, Loss: 0.0018223851220682263\n",
      "Epoch: 146, Iteration: 228, Loss: 0.0018631573766469955\n",
      "Epoch: 146, Iteration: 229, Loss: 0.0020416725892573595\n",
      "Epoch: 146, Iteration: 230, Loss: 0.0017977128736674786\n",
      "Epoch: 146, Iteration: 231, Loss: 0.0017444747500121593\n",
      "Epoch: 146, Iteration: 232, Loss: 0.0016362500609830022\n",
      "Epoch: 146, Iteration: 233, Loss: 0.0018592318519949913\n",
      "Epoch: 146, Iteration: 234, Loss: 0.0018896579276770353\n",
      "Epoch: 146, Iteration: 235, Loss: 0.0017595759127289057\n",
      "Epoch: 146, Iteration: 236, Loss: 0.001709925476461649\n",
      "Epoch: 146, Iteration: 237, Loss: 0.0020082835108041763\n",
      "Epoch: 146, Iteration: 238, Loss: 0.0017857681959867477\n",
      "Epoch: 146, Iteration: 239, Loss: 0.0018020374700427055\n",
      "Epoch: 146, Iteration: 240, Loss: 0.00184819963760674\n",
      "Epoch: 146, Iteration: 241, Loss: 0.0017407122068107128\n",
      "Epoch: 146, Iteration: 242, Loss: 0.0018684789538383484\n",
      "Epoch: 146, Iteration: 243, Loss: 0.0021265081595629454\n",
      "Epoch: 146, Iteration: 244, Loss: 0.001850392553023994\n",
      "Epoch: 146, Iteration: 245, Loss: 0.002071432303637266\n",
      "Epoch: 146, Iteration: 246, Loss: 0.0017958696698769927\n",
      "Epoch: 146, Iteration: 247, Loss: 0.00187086989171803\n",
      "Epoch: 146, Iteration: 248, Loss: 0.0022242283448576927\n",
      "Epoch: 146, Iteration: 249, Loss: 0.0018767876317724586\n",
      "Epoch: 146, Iteration: 250, Loss: 0.0018265750259160995\n",
      "Epoch: 146, Iteration: 251, Loss: 0.00198399112559855\n",
      "Epoch: 146, Iteration: 252, Loss: 0.0019512821454554796\n",
      "Epoch: 146, Iteration: 253, Loss: 0.0019846111536026\n",
      "Epoch: 146, Iteration: 254, Loss: 0.001920824870467186\n",
      "Epoch: 146, Iteration: 255, Loss: 0.0019464150536805391\n",
      "Epoch: 146, Iteration: 256, Loss: 0.0018396625528112054\n",
      "Epoch: 146, Iteration: 257, Loss: 0.0018641500500962138\n",
      "Epoch: 146, Iteration: 258, Loss: 0.0016923480434343219\n",
      "Epoch: 146, Iteration: 259, Loss: 0.0018852191278710961\n",
      "Epoch: 146, Iteration: 260, Loss: 0.002239149995148182\n",
      "Epoch: 146, Iteration: 261, Loss: 0.0016923192888498306\n",
      "Epoch: 146, Iteration: 262, Loss: 0.002115923911333084\n",
      "Epoch: 146, Iteration: 263, Loss: 0.0018817645031958818\n",
      "Epoch: 146, Iteration: 264, Loss: 0.0019773298408836126\n",
      "Epoch: 146, Iteration: 265, Loss: 0.0020744665525853634\n",
      "Epoch: 146, Iteration: 266, Loss: 0.00166227831505239\n",
      "Epoch: 146, Iteration: 267, Loss: 0.001933865249156952\n",
      "Epoch: 146, Iteration: 268, Loss: 0.0017609380884096026\n",
      "Epoch: 146, Iteration: 269, Loss: 0.0018287913408130407\n",
      "Epoch: 146, Iteration: 270, Loss: 0.0017973403446376324\n",
      "Epoch: 146, Iteration: 271, Loss: 0.0020202388986945152\n",
      "Epoch: 146, Iteration: 272, Loss: 0.0019122690428048372\n",
      "Epoch: 146, Iteration: 273, Loss: 0.0018388063181191683\n",
      "Epoch: 146, Iteration: 274, Loss: 0.0018739807419478893\n",
      "Epoch: 146 Loss: 0.0018744402166982663\n",
      "Epoch: 147, Iteration: 0, Loss: 0.0017092095222324133\n",
      "Epoch: 147, Iteration: 1, Loss: 0.00174378277733922\n",
      "Epoch: 147, Iteration: 2, Loss: 0.0018499201396480203\n",
      "Epoch: 147, Iteration: 3, Loss: 0.0023207536432892084\n",
      "Epoch: 147, Iteration: 4, Loss: 0.0017201406881213188\n",
      "Epoch: 147, Iteration: 5, Loss: 0.001952198101207614\n",
      "Epoch: 147, Iteration: 6, Loss: 0.0019239849643781781\n",
      "Epoch: 147, Iteration: 7, Loss: 0.0021524447947740555\n",
      "Epoch: 147, Iteration: 8, Loss: 0.0016917912289500237\n",
      "Epoch: 147, Iteration: 9, Loss: 0.0017951696645468473\n",
      "Epoch: 147, Iteration: 10, Loss: 0.0018366394797340035\n",
      "Epoch: 147, Iteration: 11, Loss: 0.001968913245946169\n",
      "Epoch: 147, Iteration: 12, Loss: 0.0020489373710006475\n",
      "Epoch: 147, Iteration: 13, Loss: 0.0018940172158181667\n",
      "Epoch: 147, Iteration: 14, Loss: 0.0017369369743391871\n",
      "Epoch: 147, Iteration: 15, Loss: 0.0019254274666309357\n",
      "Epoch: 147, Iteration: 16, Loss: 0.00217561237514019\n",
      "Epoch: 147, Iteration: 17, Loss: 0.0020666171330958605\n",
      "Epoch: 147, Iteration: 18, Loss: 0.0017372423317283392\n",
      "Epoch: 147, Iteration: 19, Loss: 0.0017208210192620754\n",
      "Epoch: 147, Iteration: 20, Loss: 0.0018626977689564228\n",
      "Epoch: 147, Iteration: 21, Loss: 0.0020830160938203335\n",
      "Epoch: 147, Iteration: 22, Loss: 0.001893407548777759\n",
      "Epoch: 147, Iteration: 23, Loss: 0.0018141597975045443\n",
      "Epoch: 147, Iteration: 24, Loss: 0.0020198961719870567\n",
      "Epoch: 147, Iteration: 25, Loss: 0.0016915119485929608\n",
      "Epoch: 147, Iteration: 26, Loss: 0.0020428639836609364\n",
      "Epoch: 147, Iteration: 27, Loss: 0.00197298894636333\n",
      "Epoch: 147, Iteration: 28, Loss: 0.0021474938839673996\n",
      "Epoch: 147, Iteration: 29, Loss: 0.0017488209996372461\n",
      "Epoch: 147, Iteration: 30, Loss: 0.0018682873342186213\n",
      "Epoch: 147, Iteration: 31, Loss: 0.0016920558409765363\n",
      "Epoch: 147, Iteration: 32, Loss: 0.0019727929029613733\n",
      "Epoch: 147, Iteration: 33, Loss: 0.0019821980968117714\n",
      "Epoch: 147, Iteration: 34, Loss: 0.0018566406797617674\n",
      "Epoch: 147, Iteration: 35, Loss: 0.0016272289212793112\n",
      "Epoch: 147, Iteration: 36, Loss: 0.0016981828957796097\n",
      "Epoch: 147, Iteration: 37, Loss: 0.0018902813317254186\n",
      "Epoch: 147, Iteration: 38, Loss: 0.001919278991408646\n",
      "Epoch: 147, Iteration: 39, Loss: 0.0019599127117544413\n",
      "Epoch: 147, Iteration: 40, Loss: 0.0019472911953926086\n",
      "Epoch: 147, Iteration: 41, Loss: 0.001956748776137829\n",
      "Epoch: 147, Iteration: 42, Loss: 0.0018938028952106833\n",
      "Epoch: 147, Iteration: 43, Loss: 0.0018695677863433957\n",
      "Epoch: 147, Iteration: 44, Loss: 0.001869312021881342\n",
      "Epoch: 147, Iteration: 45, Loss: 0.0016859902534633875\n",
      "Epoch: 147, Iteration: 46, Loss: 0.0018749203300103545\n",
      "Epoch: 147, Iteration: 47, Loss: 0.0015093140536919236\n",
      "Epoch: 147, Iteration: 48, Loss: 0.0019042862113565207\n",
      "Epoch: 147, Iteration: 49, Loss: 0.0018200838239863515\n",
      "Epoch: 147, Iteration: 50, Loss: 0.0018594971625134349\n",
      "Epoch: 147, Iteration: 51, Loss: 0.001893640379421413\n",
      "Epoch: 147, Iteration: 52, Loss: 0.0018458727281540632\n",
      "Epoch: 147, Iteration: 53, Loss: 0.0017876195488497615\n",
      "Epoch: 147, Iteration: 54, Loss: 0.0017652870155870914\n",
      "Epoch: 147, Iteration: 55, Loss: 0.0019335539545863867\n",
      "Epoch: 147, Iteration: 56, Loss: 0.0019521163776516914\n",
      "Epoch: 147, Iteration: 57, Loss: 0.0018279943615198135\n",
      "Epoch: 147, Iteration: 58, Loss: 0.001893053064122796\n",
      "Epoch: 147, Iteration: 59, Loss: 0.0018414643127471209\n",
      "Epoch: 147, Iteration: 60, Loss: 0.0017378471093252301\n",
      "Epoch: 147, Iteration: 61, Loss: 0.0017988240579143167\n",
      "Epoch: 147, Iteration: 62, Loss: 0.0017602518200874329\n",
      "Epoch: 147, Iteration: 63, Loss: 0.0017079699318856\n",
      "Epoch: 147, Iteration: 64, Loss: 0.002147407503798604\n",
      "Epoch: 147, Iteration: 65, Loss: 0.0018515947740525007\n",
      "Epoch: 147, Iteration: 66, Loss: 0.0020346050150692463\n",
      "Epoch: 147, Iteration: 67, Loss: 0.0019646589644253254\n",
      "Epoch: 147, Iteration: 68, Loss: 0.001974854152649641\n",
      "Epoch: 147, Iteration: 69, Loss: 0.0016387607902288437\n",
      "Epoch: 147, Iteration: 70, Loss: 0.0015662296209484339\n",
      "Epoch: 147, Iteration: 71, Loss: 0.0017411601729691029\n",
      "Epoch: 147, Iteration: 72, Loss: 0.0018580680480226874\n",
      "Epoch: 147, Iteration: 73, Loss: 0.00198920164257288\n",
      "Epoch: 147, Iteration: 74, Loss: 0.0019350071670487523\n",
      "Epoch: 147, Iteration: 75, Loss: 0.0014664914924651384\n",
      "Epoch: 147, Iteration: 76, Loss: 0.001652524690143764\n",
      "Epoch: 147, Iteration: 77, Loss: 0.0018479019636288285\n",
      "Epoch: 147, Iteration: 78, Loss: 0.0018636264139786363\n",
      "Epoch: 147, Iteration: 79, Loss: 0.0019116944167762995\n",
      "Epoch: 147, Iteration: 80, Loss: 0.0017930067842826247\n",
      "Epoch: 147, Iteration: 81, Loss: 0.0018847014289349318\n",
      "Epoch: 147, Iteration: 82, Loss: 0.0018147800583392382\n",
      "Epoch: 147, Iteration: 83, Loss: 0.0019280227134004235\n",
      "Epoch: 147, Iteration: 84, Loss: 0.0017731369007378817\n",
      "Epoch: 147, Iteration: 85, Loss: 0.0017054617637768388\n",
      "Epoch: 147, Iteration: 86, Loss: 0.0019113686867058277\n",
      "Epoch: 147, Iteration: 87, Loss: 0.0019668531604111195\n",
      "Epoch: 147, Iteration: 88, Loss: 0.0020386988762766123\n",
      "Epoch: 147, Iteration: 89, Loss: 0.001872375956736505\n",
      "Epoch: 147, Iteration: 90, Loss: 0.0019342561718076468\n",
      "Epoch: 147, Iteration: 91, Loss: 0.0019709612242877483\n",
      "Epoch: 147, Iteration: 92, Loss: 0.0018171581905335188\n",
      "Epoch: 147, Iteration: 93, Loss: 0.001604994642548263\n",
      "Epoch: 147, Iteration: 94, Loss: 0.0019080416532233357\n",
      "Epoch: 147, Iteration: 95, Loss: 0.0016093200538307428\n",
      "Epoch: 147, Iteration: 96, Loss: 0.0018788825254887342\n",
      "Epoch: 147, Iteration: 97, Loss: 0.0019964566454291344\n",
      "Epoch: 147, Iteration: 98, Loss: 0.0020067004952579737\n",
      "Epoch: 147, Iteration: 99, Loss: 0.00189825682900846\n",
      "Epoch: 147, Iteration: 100, Loss: 0.0020051770843565464\n",
      "Epoch: 147, Iteration: 101, Loss: 0.0018261594232171774\n",
      "Epoch: 147, Iteration: 102, Loss: 0.0018904313910752535\n",
      "Epoch: 147, Iteration: 103, Loss: 0.001761260675266385\n",
      "Epoch: 147, Iteration: 104, Loss: 0.0017471113242208958\n",
      "Epoch: 147, Iteration: 105, Loss: 0.0020270049571990967\n",
      "Epoch: 147, Iteration: 106, Loss: 0.0017865742556750774\n",
      "Epoch: 147, Iteration: 107, Loss: 0.0017535585211589932\n",
      "Epoch: 147, Iteration: 108, Loss: 0.0018029722850769758\n",
      "Epoch: 147, Iteration: 109, Loss: 0.0016009605024009943\n",
      "Epoch: 147, Iteration: 110, Loss: 0.0017268689116463065\n",
      "Epoch: 147, Iteration: 111, Loss: 0.0020012513268738985\n",
      "Epoch: 147, Iteration: 112, Loss: 0.0018431266071274877\n",
      "Epoch: 147, Iteration: 113, Loss: 0.001883881282992661\n",
      "Epoch: 147, Iteration: 114, Loss: 0.0017907496076077223\n",
      "Epoch: 147, Iteration: 115, Loss: 0.0019511462887749076\n",
      "Epoch: 147, Iteration: 116, Loss: 0.0020174868404865265\n",
      "Epoch: 147, Iteration: 117, Loss: 0.0019287837203592062\n",
      "Epoch: 147, Iteration: 118, Loss: 0.0019731386564671993\n",
      "Epoch: 147, Iteration: 119, Loss: 0.0018686777912080288\n",
      "Epoch: 147, Iteration: 120, Loss: 0.0019448939710855484\n",
      "Epoch: 147, Iteration: 121, Loss: 0.002018121536821127\n",
      "Epoch: 147, Iteration: 122, Loss: 0.0018533237744122744\n",
      "Epoch: 147, Iteration: 123, Loss: 0.0019384399056434631\n",
      "Epoch: 147, Iteration: 124, Loss: 0.002029387978836894\n",
      "Epoch: 147, Iteration: 125, Loss: 0.0019718720577657223\n",
      "Epoch: 147, Iteration: 126, Loss: 0.0020520698744803667\n",
      "Epoch: 147, Iteration: 127, Loss: 0.001896050525829196\n",
      "Epoch: 147, Iteration: 128, Loss: 0.002042325446382165\n",
      "Epoch: 147, Iteration: 129, Loss: 0.0020740830805152655\n",
      "Epoch: 147, Iteration: 130, Loss: 0.001676198560744524\n",
      "Epoch: 147, Iteration: 131, Loss: 0.0019360043806955218\n",
      "Epoch: 147, Iteration: 132, Loss: 0.0017633242532610893\n",
      "Epoch: 147, Iteration: 133, Loss: 0.0018907035700976849\n",
      "Epoch: 147, Iteration: 134, Loss: 0.001839545089751482\n",
      "Epoch: 147, Iteration: 135, Loss: 0.0020228500943630934\n",
      "Epoch: 147, Iteration: 136, Loss: 0.0017285693902522326\n",
      "Epoch: 147, Iteration: 137, Loss: 0.001966270385310054\n",
      "Epoch: 147, Iteration: 138, Loss: 0.0018653863808140159\n",
      "Epoch: 147, Iteration: 139, Loss: 0.001870376756414771\n",
      "Epoch: 147, Iteration: 140, Loss: 0.0018798138480633497\n",
      "Epoch: 147, Iteration: 141, Loss: 0.0016076049068942666\n",
      "Epoch: 147, Iteration: 142, Loss: 0.0017357622273266315\n",
      "Epoch: 147, Iteration: 143, Loss: 0.0019444350618869066\n",
      "Epoch: 147, Iteration: 144, Loss: 0.0019104480743408203\n",
      "Epoch: 147, Iteration: 145, Loss: 0.0019139980431646109\n",
      "Epoch: 147, Iteration: 146, Loss: 0.0016391610261052847\n",
      "Epoch: 147, Iteration: 147, Loss: 0.0017868010327219963\n",
      "Epoch: 147, Iteration: 148, Loss: 0.001901908777654171\n",
      "Epoch: 147, Iteration: 149, Loss: 0.0019145046826452017\n",
      "Epoch: 147, Iteration: 150, Loss: 0.0015418865950778127\n",
      "Epoch: 147, Iteration: 151, Loss: 0.0018500861478969455\n",
      "Epoch: 147, Iteration: 152, Loss: 0.0019348454661667347\n",
      "Epoch: 147, Iteration: 153, Loss: 0.0018665873212739825\n",
      "Epoch: 147, Iteration: 154, Loss: 0.002114888746291399\n",
      "Epoch: 147, Iteration: 155, Loss: 0.0018927481723949313\n",
      "Epoch: 147, Iteration: 156, Loss: 0.00195254513528198\n",
      "Epoch: 147, Iteration: 157, Loss: 0.0018305669073015451\n",
      "Epoch: 147, Iteration: 158, Loss: 0.0019091624999418855\n",
      "Epoch: 147, Iteration: 159, Loss: 0.0017473711632192135\n",
      "Epoch: 147, Iteration: 160, Loss: 0.0019682194106280804\n",
      "Epoch: 147, Iteration: 161, Loss: 0.0019663937855511904\n",
      "Epoch: 147, Iteration: 162, Loss: 0.0019348119385540485\n",
      "Epoch: 147, Iteration: 163, Loss: 0.0017144111916422844\n",
      "Epoch: 147, Iteration: 164, Loss: 0.001903848140500486\n",
      "Epoch: 147, Iteration: 165, Loss: 0.001607289188541472\n",
      "Epoch: 147, Iteration: 166, Loss: 0.0017340081976726651\n",
      "Epoch: 147, Iteration: 167, Loss: 0.001808650093153119\n",
      "Epoch: 147, Iteration: 168, Loss: 0.0016873496351763606\n",
      "Epoch: 147, Iteration: 169, Loss: 0.0017214652616530657\n",
      "Epoch: 147, Iteration: 170, Loss: 0.0018206089735031128\n",
      "Epoch: 147, Iteration: 171, Loss: 0.0018094927072525024\n",
      "Epoch: 147, Iteration: 172, Loss: 0.0018486874178051949\n",
      "Epoch: 147, Iteration: 173, Loss: 0.0017968049505725503\n",
      "Epoch: 147, Iteration: 174, Loss: 0.002031987998634577\n",
      "Epoch: 147, Iteration: 175, Loss: 0.001934831845574081\n",
      "Epoch: 147, Iteration: 176, Loss: 0.0018071255180984735\n",
      "Epoch: 147, Iteration: 177, Loss: 0.0020696152932941914\n",
      "Epoch: 147, Iteration: 178, Loss: 0.0021116435527801514\n",
      "Epoch: 147, Iteration: 179, Loss: 0.0019771892111748457\n",
      "Epoch: 147, Iteration: 180, Loss: 0.0017234624829143286\n",
      "Epoch: 147, Iteration: 181, Loss: 0.0018505529733374715\n",
      "Epoch: 147, Iteration: 182, Loss: 0.0017306364607065916\n",
      "Epoch: 147, Iteration: 183, Loss: 0.0019795047119259834\n",
      "Epoch: 147, Iteration: 184, Loss: 0.0019800341688096523\n",
      "Epoch: 147, Iteration: 185, Loss: 0.0017697031144052744\n",
      "Epoch: 147, Iteration: 186, Loss: 0.002035718411207199\n",
      "Epoch: 147, Iteration: 187, Loss: 0.0018105360213667154\n",
      "Epoch: 147, Iteration: 188, Loss: 0.0017233124235644937\n",
      "Epoch: 147, Iteration: 189, Loss: 0.0017227130010724068\n",
      "Epoch: 147, Iteration: 190, Loss: 0.0020204056054353714\n",
      "Epoch: 147, Iteration: 191, Loss: 0.0018804867286235094\n",
      "Epoch: 147, Iteration: 192, Loss: 0.0018192478455603123\n",
      "Epoch: 147, Iteration: 193, Loss: 0.002108132466673851\n",
      "Epoch: 147, Iteration: 194, Loss: 0.0018574882997199893\n",
      "Epoch: 147, Iteration: 195, Loss: 0.0020082909613847733\n",
      "Epoch: 147, Iteration: 196, Loss: 0.001912396284751594\n",
      "Epoch: 147, Iteration: 197, Loss: 0.0019209154415875673\n",
      "Epoch: 147, Iteration: 198, Loss: 0.0017031345050781965\n",
      "Epoch: 147, Iteration: 199, Loss: 0.002069375477731228\n",
      "Epoch: 147, Iteration: 200, Loss: 0.001954574603587389\n",
      "Epoch: 147, Iteration: 201, Loss: 0.0021102842874825\n",
      "Epoch: 147, Iteration: 202, Loss: 0.0018965547205880284\n",
      "Epoch: 147, Iteration: 203, Loss: 0.0021147490479052067\n",
      "Epoch: 147, Iteration: 204, Loss: 0.001765321008861065\n",
      "Epoch: 147, Iteration: 205, Loss: 0.0019379114964976907\n",
      "Epoch: 147, Iteration: 206, Loss: 0.0019725365564227104\n",
      "Epoch: 147, Iteration: 207, Loss: 0.0017478372901678085\n",
      "Epoch: 147, Iteration: 208, Loss: 0.0017941197147592902\n",
      "Epoch: 147, Iteration: 209, Loss: 0.001981771318241954\n",
      "Epoch: 147, Iteration: 210, Loss: 0.0018350423779338598\n",
      "Epoch: 147, Iteration: 211, Loss: 0.0017868627328425646\n",
      "Epoch: 147, Iteration: 212, Loss: 0.0017878906801342964\n",
      "Epoch: 147, Iteration: 213, Loss: 0.0019510905258357525\n",
      "Epoch: 147, Iteration: 214, Loss: 0.0019511536229401827\n",
      "Epoch: 147, Iteration: 215, Loss: 0.0017580174608156085\n",
      "Epoch: 147, Iteration: 216, Loss: 0.0018279963405802846\n",
      "Epoch: 147, Iteration: 217, Loss: 0.0018625891534611583\n",
      "Epoch: 147, Iteration: 218, Loss: 0.0020324825309216976\n",
      "Epoch: 147, Iteration: 219, Loss: 0.001790550071746111\n",
      "Epoch: 147, Iteration: 220, Loss: 0.0019912372808903456\n",
      "Epoch: 147, Iteration: 221, Loss: 0.002137518720701337\n",
      "Epoch: 147, Iteration: 222, Loss: 0.001761527033522725\n",
      "Epoch: 147, Iteration: 223, Loss: 0.0017160915303975344\n",
      "Epoch: 147, Iteration: 224, Loss: 0.0019460408948361874\n",
      "Epoch: 147, Iteration: 225, Loss: 0.0020162761211395264\n",
      "Epoch: 147, Iteration: 226, Loss: 0.0018404606962576509\n",
      "Epoch: 147, Iteration: 227, Loss: 0.0018222139915451407\n",
      "Epoch: 147, Iteration: 228, Loss: 0.001850601052865386\n",
      "Epoch: 147, Iteration: 229, Loss: 0.0017680209130048752\n",
      "Epoch: 147, Iteration: 230, Loss: 0.0018797513330355287\n",
      "Epoch: 147, Iteration: 231, Loss: 0.0018647168762981892\n",
      "Epoch: 147, Iteration: 232, Loss: 0.00202853512018919\n",
      "Epoch: 147, Iteration: 233, Loss: 0.001869993400759995\n",
      "Epoch: 147, Iteration: 234, Loss: 0.0018857044633477926\n",
      "Epoch: 147, Iteration: 235, Loss: 0.0017380096251145005\n",
      "Epoch: 147, Iteration: 236, Loss: 0.001908490085043013\n",
      "Epoch: 147, Iteration: 237, Loss: 0.0018087491625919938\n",
      "Epoch: 147, Iteration: 238, Loss: 0.0018204556545242667\n",
      "Epoch: 147, Iteration: 239, Loss: 0.0019262273563072085\n",
      "Epoch: 147, Iteration: 240, Loss: 0.0020274040289223194\n",
      "Epoch: 147, Iteration: 241, Loss: 0.0022197049111127853\n",
      "Epoch: 147, Iteration: 242, Loss: 0.0017517183441668749\n",
      "Epoch: 147, Iteration: 243, Loss: 0.0021650963462889194\n",
      "Epoch: 147, Iteration: 244, Loss: 0.0017745450604707003\n",
      "Epoch: 147, Iteration: 245, Loss: 0.001924817101098597\n",
      "Epoch: 147, Iteration: 246, Loss: 0.0017271812539547682\n",
      "Epoch: 147, Iteration: 247, Loss: 0.0018461095169186592\n",
      "Epoch: 147, Iteration: 248, Loss: 0.001934146392159164\n",
      "Epoch: 147, Iteration: 249, Loss: 0.001608813414350152\n",
      "Epoch: 147, Iteration: 250, Loss: 0.0016376171261072159\n",
      "Epoch: 147, Iteration: 251, Loss: 0.0019428670639172196\n",
      "Epoch: 147, Iteration: 252, Loss: 0.0018600719049572945\n",
      "Epoch: 147, Iteration: 253, Loss: 0.0019507112447172403\n",
      "Epoch: 147, Iteration: 254, Loss: 0.0018789204768836498\n",
      "Epoch: 147, Iteration: 255, Loss: 0.0018888551276177168\n",
      "Epoch: 147, Iteration: 256, Loss: 0.002021296415477991\n",
      "Epoch: 147, Iteration: 257, Loss: 0.0017900685779750347\n",
      "Epoch: 147, Iteration: 258, Loss: 0.002048573922365904\n",
      "Epoch: 147, Iteration: 259, Loss: 0.0018173479475080967\n",
      "Epoch: 147, Iteration: 260, Loss: 0.0018012800719588995\n",
      "Epoch: 147, Iteration: 261, Loss: 0.001848840620368719\n",
      "Epoch: 147, Iteration: 262, Loss: 0.002127428073436022\n",
      "Epoch: 147, Iteration: 263, Loss: 0.0021030523348599672\n",
      "Epoch: 147, Iteration: 264, Loss: 0.001665328862145543\n",
      "Epoch: 147, Iteration: 265, Loss: 0.0017799284541979432\n",
      "Epoch: 147, Iteration: 266, Loss: 0.0019905627705156803\n",
      "Epoch: 147, Iteration: 267, Loss: 0.002070086542516947\n",
      "Epoch: 147, Iteration: 268, Loss: 0.0016962419031187892\n",
      "Epoch: 147, Iteration: 269, Loss: 0.0020453629549592733\n",
      "Epoch: 147, Iteration: 270, Loss: 0.0018510852241888642\n",
      "Epoch: 147, Iteration: 271, Loss: 0.001731631113216281\n",
      "Epoch: 147, Iteration: 272, Loss: 0.001620554132387042\n",
      "Epoch: 147, Iteration: 273, Loss: 0.0018409299664199352\n",
      "Epoch: 147, Iteration: 274, Loss: 0.002026949543505907\n",
      "Epoch: 147 Loss: 0.0018743570938686916\n",
      "Epoch: 148, Iteration: 0, Loss: 0.0019132031593471766\n",
      "Epoch: 148, Iteration: 1, Loss: 0.0018308208091184497\n",
      "Epoch: 148, Iteration: 2, Loss: 0.001942973118275404\n",
      "Epoch: 148, Iteration: 3, Loss: 0.0017112556379288435\n",
      "Epoch: 148, Iteration: 4, Loss: 0.0018367429729551077\n",
      "Epoch: 148, Iteration: 5, Loss: 0.0018629690166562796\n",
      "Epoch: 148, Iteration: 6, Loss: 0.001949625788256526\n",
      "Epoch: 148, Iteration: 7, Loss: 0.0018139423336833715\n",
      "Epoch: 148, Iteration: 8, Loss: 0.0017633107490837574\n",
      "Epoch: 148, Iteration: 9, Loss: 0.0017792298458516598\n",
      "Epoch: 148, Iteration: 10, Loss: 0.0018085261108353734\n",
      "Epoch: 148, Iteration: 11, Loss: 0.0019521655049175024\n",
      "Epoch: 148, Iteration: 12, Loss: 0.0017007318092510104\n",
      "Epoch: 148, Iteration: 13, Loss: 0.0017442971002310514\n",
      "Epoch: 148, Iteration: 14, Loss: 0.0020988397300243378\n",
      "Epoch: 148, Iteration: 15, Loss: 0.0019385034684091806\n",
      "Epoch: 148, Iteration: 16, Loss: 0.0019341821316629648\n",
      "Epoch: 148, Iteration: 17, Loss: 0.002068108879029751\n",
      "Epoch: 148, Iteration: 18, Loss: 0.0020814030431210995\n",
      "Epoch: 148, Iteration: 19, Loss: 0.0017487937584519386\n",
      "Epoch: 148, Iteration: 20, Loss: 0.0018344020936638117\n",
      "Epoch: 148, Iteration: 21, Loss: 0.0017671813257038593\n",
      "Epoch: 148, Iteration: 22, Loss: 0.001966556068509817\n",
      "Epoch: 148, Iteration: 23, Loss: 0.001978596905246377\n",
      "Epoch: 148, Iteration: 24, Loss: 0.0020058522932231426\n",
      "Epoch: 148, Iteration: 25, Loss: 0.002036109333857894\n",
      "Epoch: 148, Iteration: 26, Loss: 0.0017705499194562435\n",
      "Epoch: 148, Iteration: 27, Loss: 0.0016728368354961276\n",
      "Epoch: 148, Iteration: 28, Loss: 0.001996867824345827\n",
      "Epoch: 148, Iteration: 29, Loss: 0.0017520252149552107\n",
      "Epoch: 148, Iteration: 30, Loss: 0.0018082691822201014\n",
      "Epoch: 148, Iteration: 31, Loss: 0.00218178890645504\n",
      "Epoch: 148, Iteration: 32, Loss: 0.0019371259259060025\n",
      "Epoch: 148, Iteration: 33, Loss: 0.0019795247353613377\n",
      "Epoch: 148, Iteration: 34, Loss: 0.0018690647557377815\n",
      "Epoch: 148, Iteration: 35, Loss: 0.0017930814065039158\n",
      "Epoch: 148, Iteration: 36, Loss: 0.001672599115408957\n",
      "Epoch: 148, Iteration: 37, Loss: 0.0018272153101861477\n",
      "Epoch: 148, Iteration: 38, Loss: 0.0018156599253416061\n",
      "Epoch: 148, Iteration: 39, Loss: 0.0016587201971560717\n",
      "Epoch: 148, Iteration: 40, Loss: 0.002051127143204212\n",
      "Epoch: 148, Iteration: 41, Loss: 0.001824552658945322\n",
      "Epoch: 148, Iteration: 42, Loss: 0.0020199178252369165\n",
      "Epoch: 148, Iteration: 43, Loss: 0.0019765221513807774\n",
      "Epoch: 148, Iteration: 44, Loss: 0.0020124814473092556\n",
      "Epoch: 148, Iteration: 45, Loss: 0.0017912557814270258\n",
      "Epoch: 148, Iteration: 46, Loss: 0.002164833014830947\n",
      "Epoch: 148, Iteration: 47, Loss: 0.0018063158495351672\n",
      "Epoch: 148, Iteration: 48, Loss: 0.002072238828986883\n",
      "Epoch: 148, Iteration: 49, Loss: 0.0017696947325021029\n",
      "Epoch: 148, Iteration: 50, Loss: 0.002017435384914279\n",
      "Epoch: 148, Iteration: 51, Loss: 0.0017694541020318866\n",
      "Epoch: 148, Iteration: 52, Loss: 0.0017998996190726757\n",
      "Epoch: 148, Iteration: 53, Loss: 0.001888686092570424\n",
      "Epoch: 148, Iteration: 54, Loss: 0.0017423543613404036\n",
      "Epoch: 148, Iteration: 55, Loss: 0.0017508910968899727\n",
      "Epoch: 148, Iteration: 56, Loss: 0.0017529085744172335\n",
      "Epoch: 148, Iteration: 57, Loss: 0.002101676072925329\n",
      "Epoch: 148, Iteration: 58, Loss: 0.0020778290927410126\n",
      "Epoch: 148, Iteration: 59, Loss: 0.0019494264852255583\n",
      "Epoch: 148, Iteration: 60, Loss: 0.0018435094971209764\n",
      "Epoch: 148, Iteration: 61, Loss: 0.0018801615806296468\n",
      "Epoch: 148, Iteration: 62, Loss: 0.0018860269337892532\n",
      "Epoch: 148, Iteration: 63, Loss: 0.001848271582275629\n",
      "Epoch: 148, Iteration: 64, Loss: 0.0019790716469287872\n",
      "Epoch: 148, Iteration: 65, Loss: 0.002265404909849167\n",
      "Epoch: 148, Iteration: 66, Loss: 0.0018383610295131803\n",
      "Epoch: 148, Iteration: 67, Loss: 0.0018055583350360394\n",
      "Epoch: 148, Iteration: 68, Loss: 0.0020575476810336113\n",
      "Epoch: 148, Iteration: 69, Loss: 0.002105502411723137\n",
      "Epoch: 148, Iteration: 70, Loss: 0.0018944577313959599\n",
      "Epoch: 148, Iteration: 71, Loss: 0.0016764260362833738\n",
      "Epoch: 148, Iteration: 72, Loss: 0.002013922668993473\n",
      "Epoch: 148, Iteration: 73, Loss: 0.0020027090795338154\n",
      "Epoch: 148, Iteration: 74, Loss: 0.0019494621083140373\n",
      "Epoch: 148, Iteration: 75, Loss: 0.0016330605139955878\n",
      "Epoch: 148, Iteration: 76, Loss: 0.0018124295165762305\n",
      "Epoch: 148, Iteration: 77, Loss: 0.0018793628551065922\n",
      "Epoch: 148, Iteration: 78, Loss: 0.0019007377559319139\n",
      "Epoch: 148, Iteration: 79, Loss: 0.0020587523467838764\n",
      "Epoch: 148, Iteration: 80, Loss: 0.0018916948465630412\n",
      "Epoch: 148, Iteration: 81, Loss: 0.0018522485624998808\n",
      "Epoch: 148, Iteration: 82, Loss: 0.0016651450423523784\n",
      "Epoch: 148, Iteration: 83, Loss: 0.0019853576086461544\n",
      "Epoch: 148, Iteration: 84, Loss: 0.002015054924413562\n",
      "Epoch: 148, Iteration: 85, Loss: 0.002061193808913231\n",
      "Epoch: 148, Iteration: 86, Loss: 0.0018167633097618818\n",
      "Epoch: 148, Iteration: 87, Loss: 0.0016818062867969275\n",
      "Epoch: 148, Iteration: 88, Loss: 0.0021319533698260784\n",
      "Epoch: 148, Iteration: 89, Loss: 0.0018099755980074406\n",
      "Epoch: 148, Iteration: 90, Loss: 0.0019340174039825797\n",
      "Epoch: 148, Iteration: 91, Loss: 0.0018322091782465577\n",
      "Epoch: 148, Iteration: 92, Loss: 0.0017599001294001937\n",
      "Epoch: 148, Iteration: 93, Loss: 0.0020382986404001713\n",
      "Epoch: 148, Iteration: 94, Loss: 0.0019127121195197105\n",
      "Epoch: 148, Iteration: 95, Loss: 0.0018063150346279144\n",
      "Epoch: 148, Iteration: 96, Loss: 0.0019397365394979715\n",
      "Epoch: 148, Iteration: 97, Loss: 0.001817416283302009\n",
      "Epoch: 148, Iteration: 98, Loss: 0.001734855817630887\n",
      "Epoch: 148, Iteration: 99, Loss: 0.0018533895490691066\n",
      "Epoch: 148, Iteration: 100, Loss: 0.0017303959466516972\n",
      "Epoch: 148, Iteration: 101, Loss: 0.001991214696317911\n",
      "Epoch: 148, Iteration: 102, Loss: 0.0018426628084853292\n",
      "Epoch: 148, Iteration: 103, Loss: 0.0019152873428538442\n",
      "Epoch: 148, Iteration: 104, Loss: 0.0019203692208975554\n",
      "Epoch: 148, Iteration: 105, Loss: 0.0018707640701904893\n",
      "Epoch: 148, Iteration: 106, Loss: 0.0017110607586801052\n",
      "Epoch: 148, Iteration: 107, Loss: 0.001917240908369422\n",
      "Epoch: 148, Iteration: 108, Loss: 0.0017035522032529116\n",
      "Epoch: 148, Iteration: 109, Loss: 0.002103195758536458\n",
      "Epoch: 148, Iteration: 110, Loss: 0.0018723363755270839\n",
      "Epoch: 148, Iteration: 111, Loss: 0.002060418715700507\n",
      "Epoch: 148, Iteration: 112, Loss: 0.001802995102480054\n",
      "Epoch: 148, Iteration: 113, Loss: 0.0018692032899707556\n",
      "Epoch: 148, Iteration: 114, Loss: 0.0017763548530638218\n",
      "Epoch: 148, Iteration: 115, Loss: 0.0019627055153250694\n",
      "Epoch: 148, Iteration: 116, Loss: 0.001927800360135734\n",
      "Epoch: 148, Iteration: 117, Loss: 0.0020122546702623367\n",
      "Epoch: 148, Iteration: 118, Loss: 0.0020083594135940075\n",
      "Epoch: 148, Iteration: 119, Loss: 0.0017422509845346212\n",
      "Epoch: 148, Iteration: 120, Loss: 0.0018777961377054453\n",
      "Epoch: 148, Iteration: 121, Loss: 0.0017031540628522635\n",
      "Epoch: 148, Iteration: 122, Loss: 0.0017288634553551674\n",
      "Epoch: 148, Iteration: 123, Loss: 0.0018457307014614344\n",
      "Epoch: 148, Iteration: 124, Loss: 0.0021294665057212114\n",
      "Epoch: 148, Iteration: 125, Loss: 0.0022308018524199724\n",
      "Epoch: 148, Iteration: 126, Loss: 0.0019177866633981466\n",
      "Epoch: 148, Iteration: 127, Loss: 0.0019680240657180548\n",
      "Epoch: 148, Iteration: 128, Loss: 0.0019290510099381208\n",
      "Epoch: 148, Iteration: 129, Loss: 0.0019238480599597096\n",
      "Epoch: 148, Iteration: 130, Loss: 0.0018593668937683105\n",
      "Epoch: 148, Iteration: 131, Loss: 0.0021109245717525482\n",
      "Epoch: 148, Iteration: 132, Loss: 0.0018949250224977732\n",
      "Epoch: 148, Iteration: 133, Loss: 0.0018388767493888736\n",
      "Epoch: 148, Iteration: 134, Loss: 0.00162494124379009\n",
      "Epoch: 148, Iteration: 135, Loss: 0.0016086918767541647\n",
      "Epoch: 148, Iteration: 136, Loss: 0.002012550598010421\n",
      "Epoch: 148, Iteration: 137, Loss: 0.001965358853340149\n",
      "Epoch: 148, Iteration: 138, Loss: 0.0017413971945643425\n",
      "Epoch: 148, Iteration: 139, Loss: 0.0019626033026725054\n",
      "Epoch: 148, Iteration: 140, Loss: 0.00177679187618196\n",
      "Epoch: 148, Iteration: 141, Loss: 0.0018942567985504866\n",
      "Epoch: 148, Iteration: 142, Loss: 0.0017652169335633516\n",
      "Epoch: 148, Iteration: 143, Loss: 0.001693596481345594\n",
      "Epoch: 148, Iteration: 144, Loss: 0.001579225528985262\n",
      "Epoch: 148, Iteration: 145, Loss: 0.0017487287987023592\n",
      "Epoch: 148, Iteration: 146, Loss: 0.0017297411104664207\n",
      "Epoch: 148, Iteration: 147, Loss: 0.0018821550766006112\n",
      "Epoch: 148, Iteration: 148, Loss: 0.0016905441880226135\n",
      "Epoch: 148, Iteration: 149, Loss: 0.0018726021517068148\n",
      "Epoch: 148, Iteration: 150, Loss: 0.0020156344398856163\n",
      "Epoch: 148, Iteration: 151, Loss: 0.0017121341079473495\n",
      "Epoch: 148, Iteration: 152, Loss: 0.0019523982191458344\n",
      "Epoch: 148, Iteration: 153, Loss: 0.0017171052750200033\n",
      "Epoch: 148, Iteration: 154, Loss: 0.0018174615688621998\n",
      "Epoch: 148, Iteration: 155, Loss: 0.0019063514191657305\n",
      "Epoch: 148, Iteration: 156, Loss: 0.001919332891702652\n",
      "Epoch: 148, Iteration: 157, Loss: 0.0018075854750350118\n",
      "Epoch: 148, Iteration: 158, Loss: 0.001793845440261066\n",
      "Epoch: 148, Iteration: 159, Loss: 0.0022012614645063877\n",
      "Epoch: 148, Iteration: 160, Loss: 0.0018998541636392474\n",
      "Epoch: 148, Iteration: 161, Loss: 0.0020887679420411587\n",
      "Epoch: 148, Iteration: 162, Loss: 0.0018862339202314615\n",
      "Epoch: 148, Iteration: 163, Loss: 0.0017305995570495725\n",
      "Epoch: 148, Iteration: 164, Loss: 0.0019286007154732943\n",
      "Epoch: 148, Iteration: 165, Loss: 0.0017563740257173777\n",
      "Epoch: 148, Iteration: 166, Loss: 0.0020342692732810974\n",
      "Epoch: 148, Iteration: 167, Loss: 0.0018396545201539993\n",
      "Epoch: 148, Iteration: 168, Loss: 0.0018042201409116387\n",
      "Epoch: 148, Iteration: 169, Loss: 0.001881107804365456\n",
      "Epoch: 148, Iteration: 170, Loss: 0.001974280923604965\n",
      "Epoch: 148, Iteration: 171, Loss: 0.001846757484599948\n",
      "Epoch: 148, Iteration: 172, Loss: 0.001706469338387251\n",
      "Epoch: 148, Iteration: 173, Loss: 0.0016777397831901908\n",
      "Epoch: 148, Iteration: 174, Loss: 0.0019305979367345572\n",
      "Epoch: 148, Iteration: 175, Loss: 0.001948460005223751\n",
      "Epoch: 148, Iteration: 176, Loss: 0.0019092147704213858\n",
      "Epoch: 148, Iteration: 177, Loss: 0.0017313668504357338\n",
      "Epoch: 148, Iteration: 178, Loss: 0.0016680662520229816\n",
      "Epoch: 148, Iteration: 179, Loss: 0.0018923613242805004\n",
      "Epoch: 148, Iteration: 180, Loss: 0.0017533889040350914\n",
      "Epoch: 148, Iteration: 181, Loss: 0.001859719050116837\n",
      "Epoch: 148, Iteration: 182, Loss: 0.001737374346703291\n",
      "Epoch: 148, Iteration: 183, Loss: 0.0019835615530610085\n",
      "Epoch: 148, Iteration: 184, Loss: 0.002165950369089842\n",
      "Epoch: 148, Iteration: 185, Loss: 0.0018803862622007728\n",
      "Epoch: 148, Iteration: 186, Loss: 0.0019614361226558685\n",
      "Epoch: 148, Iteration: 187, Loss: 0.0016599069349467754\n",
      "Epoch: 148, Iteration: 188, Loss: 0.0018902274314314127\n",
      "Epoch: 148, Iteration: 189, Loss: 0.001969539560377598\n",
      "Epoch: 148, Iteration: 190, Loss: 0.001976002473384142\n",
      "Epoch: 148, Iteration: 191, Loss: 0.0018219157354906201\n",
      "Epoch: 148, Iteration: 192, Loss: 0.0020288359373807907\n",
      "Epoch: 148, Iteration: 193, Loss: 0.0018189577385783195\n",
      "Epoch: 148, Iteration: 194, Loss: 0.002101199235767126\n",
      "Epoch: 148, Iteration: 195, Loss: 0.0018073793035000563\n",
      "Epoch: 148, Iteration: 196, Loss: 0.0018331152386963367\n",
      "Epoch: 148, Iteration: 197, Loss: 0.001850702567026019\n",
      "Epoch: 148, Iteration: 198, Loss: 0.0017749456455931067\n",
      "Epoch: 148, Iteration: 199, Loss: 0.0016328823985531926\n",
      "Epoch: 148, Iteration: 200, Loss: 0.0020364364609122276\n",
      "Epoch: 148, Iteration: 201, Loss: 0.0016961024375632405\n",
      "Epoch: 148, Iteration: 202, Loss: 0.0021056397818028927\n",
      "Epoch: 148, Iteration: 203, Loss: 0.0018270828295499086\n",
      "Epoch: 148, Iteration: 204, Loss: 0.0017044136766344309\n",
      "Epoch: 148, Iteration: 205, Loss: 0.0019454166758805513\n",
      "Epoch: 148, Iteration: 206, Loss: 0.001959338318556547\n",
      "Epoch: 148, Iteration: 207, Loss: 0.0018076712731271982\n",
      "Epoch: 148, Iteration: 208, Loss: 0.0018675695173442364\n",
      "Epoch: 148, Iteration: 209, Loss: 0.001926405355334282\n",
      "Epoch: 148, Iteration: 210, Loss: 0.0018913450185209513\n",
      "Epoch: 148, Iteration: 211, Loss: 0.002057682955637574\n",
      "Epoch: 148, Iteration: 212, Loss: 0.0019108311971649528\n",
      "Epoch: 148, Iteration: 213, Loss: 0.001714556012302637\n",
      "Epoch: 148, Iteration: 214, Loss: 0.001690643490292132\n",
      "Epoch: 148, Iteration: 215, Loss: 0.001758871367201209\n",
      "Epoch: 148, Iteration: 216, Loss: 0.002055307850241661\n",
      "Epoch: 148, Iteration: 217, Loss: 0.0020082732662558556\n",
      "Epoch: 148, Iteration: 218, Loss: 0.002039482817053795\n",
      "Epoch: 148, Iteration: 219, Loss: 0.0019922726787626743\n",
      "Epoch: 148, Iteration: 220, Loss: 0.0017512969207018614\n",
      "Epoch: 148, Iteration: 221, Loss: 0.0017986259190365672\n",
      "Epoch: 148, Iteration: 222, Loss: 0.001900008413940668\n",
      "Epoch: 148, Iteration: 223, Loss: 0.0016338585410267115\n",
      "Epoch: 148, Iteration: 224, Loss: 0.001757071353495121\n",
      "Epoch: 148, Iteration: 225, Loss: 0.0017618091078475118\n",
      "Epoch: 148, Iteration: 226, Loss: 0.0018711083102971315\n",
      "Epoch: 148, Iteration: 227, Loss: 0.0017474874621257186\n",
      "Epoch: 148, Iteration: 228, Loss: 0.0017518644453957677\n",
      "Epoch: 148, Iteration: 229, Loss: 0.0019396055722609162\n",
      "Epoch: 148, Iteration: 230, Loss: 0.001955829095095396\n",
      "Epoch: 148, Iteration: 231, Loss: 0.002311254385858774\n",
      "Epoch: 148, Iteration: 232, Loss: 0.0017895770724862814\n",
      "Epoch: 148, Iteration: 233, Loss: 0.0017845897236838937\n",
      "Epoch: 148, Iteration: 234, Loss: 0.0018814483191817999\n",
      "Epoch: 148, Iteration: 235, Loss: 0.001754092052578926\n",
      "Epoch: 148, Iteration: 236, Loss: 0.0016793909016996622\n",
      "Epoch: 148, Iteration: 237, Loss: 0.0017784865340217948\n",
      "Epoch: 148, Iteration: 238, Loss: 0.0018100976012647152\n",
      "Epoch: 148, Iteration: 239, Loss: 0.0017596496036276221\n",
      "Epoch: 148, Iteration: 240, Loss: 0.0019587569404393435\n",
      "Epoch: 148, Iteration: 241, Loss: 0.0018635229207575321\n",
      "Epoch: 148, Iteration: 242, Loss: 0.002075527561828494\n",
      "Epoch: 148, Iteration: 243, Loss: 0.002032550284639001\n",
      "Epoch: 148, Iteration: 244, Loss: 0.0019209458259865642\n",
      "Epoch: 148, Iteration: 245, Loss: 0.0019042107742279768\n",
      "Epoch: 148, Iteration: 246, Loss: 0.0020004184916615486\n",
      "Epoch: 148, Iteration: 247, Loss: 0.0019239967223256826\n",
      "Epoch: 148, Iteration: 248, Loss: 0.0018300837837159634\n",
      "Epoch: 148, Iteration: 249, Loss: 0.0019521338399499655\n",
      "Epoch: 148, Iteration: 250, Loss: 0.002113309223204851\n",
      "Epoch: 148, Iteration: 251, Loss: 0.0016908950638026\n",
      "Epoch: 148, Iteration: 252, Loss: 0.0018712220480665565\n",
      "Epoch: 148, Iteration: 253, Loss: 0.0016942562069743872\n",
      "Epoch: 148, Iteration: 254, Loss: 0.0019217298831790686\n",
      "Epoch: 148, Iteration: 255, Loss: 0.001895787427201867\n",
      "Epoch: 148, Iteration: 256, Loss: 0.0018644032534211874\n",
      "Epoch: 148, Iteration: 257, Loss: 0.0020072534680366516\n",
      "Epoch: 148, Iteration: 258, Loss: 0.0018446453614160419\n",
      "Epoch: 148, Iteration: 259, Loss: 0.002004646696150303\n",
      "Epoch: 148, Iteration: 260, Loss: 0.0018684212118387222\n",
      "Epoch: 148, Iteration: 261, Loss: 0.002116587944328785\n",
      "Epoch: 148, Iteration: 262, Loss: 0.0021140356548130512\n",
      "Epoch: 148, Iteration: 263, Loss: 0.0017231990350410342\n",
      "Epoch: 148, Iteration: 264, Loss: 0.0017027112189680338\n",
      "Epoch: 148, Iteration: 265, Loss: 0.0015829041367396712\n",
      "Epoch: 148, Iteration: 266, Loss: 0.0018511096714064479\n",
      "Epoch: 148, Iteration: 267, Loss: 0.001706320559605956\n",
      "Epoch: 148, Iteration: 268, Loss: 0.0017006656853482127\n",
      "Epoch: 148, Iteration: 269, Loss: 0.0017609363421797752\n",
      "Epoch: 148, Iteration: 270, Loss: 0.001768720569089055\n",
      "Epoch: 148, Iteration: 271, Loss: 0.001850372296757996\n",
      "Epoch: 148, Iteration: 272, Loss: 0.0017794023733586073\n",
      "Epoch: 148, Iteration: 273, Loss: 0.0016951344441622496\n",
      "Epoch: 148, Iteration: 274, Loss: 0.0021198070608079433\n",
      "Epoch: 148 Loss: 0.0018768507558845021\n",
      "Epoch: 149, Iteration: 0, Loss: 0.0017721711192280054\n",
      "Epoch: 149, Iteration: 1, Loss: 0.0020211110822856426\n",
      "Epoch: 149, Iteration: 2, Loss: 0.0016799046425148845\n",
      "Epoch: 149, Iteration: 3, Loss: 0.0019476673332974315\n",
      "Epoch: 149, Iteration: 4, Loss: 0.0018175148870795965\n",
      "Epoch: 149, Iteration: 5, Loss: 0.0018914022948592901\n",
      "Epoch: 149, Iteration: 6, Loss: 0.0018835985101759434\n",
      "Epoch: 149, Iteration: 7, Loss: 0.001938049797900021\n",
      "Epoch: 149, Iteration: 8, Loss: 0.0019200551323592663\n",
      "Epoch: 149, Iteration: 9, Loss: 0.0016761659644544125\n",
      "Epoch: 149, Iteration: 10, Loss: 0.0018904588650912046\n",
      "Epoch: 149, Iteration: 11, Loss: 0.002049563452601433\n",
      "Epoch: 149, Iteration: 12, Loss: 0.001723401015624404\n",
      "Epoch: 149, Iteration: 13, Loss: 0.0017792282160371542\n",
      "Epoch: 149, Iteration: 14, Loss: 0.00169845309574157\n",
      "Epoch: 149, Iteration: 15, Loss: 0.0018969792872667313\n",
      "Epoch: 149, Iteration: 16, Loss: 0.00181770755443722\n",
      "Epoch: 149, Iteration: 17, Loss: 0.0021207667887210846\n",
      "Epoch: 149, Iteration: 18, Loss: 0.0019852234981954098\n",
      "Epoch: 149, Iteration: 19, Loss: 0.001703728805296123\n",
      "Epoch: 149, Iteration: 20, Loss: 0.0021460424177348614\n",
      "Epoch: 149, Iteration: 21, Loss: 0.0018975421553477645\n",
      "Epoch: 149, Iteration: 22, Loss: 0.0018728484865278006\n",
      "Epoch: 149, Iteration: 23, Loss: 0.0016992352902889252\n",
      "Epoch: 149, Iteration: 24, Loss: 0.0018094045808538795\n",
      "Epoch: 149, Iteration: 25, Loss: 0.001906698103994131\n",
      "Epoch: 149, Iteration: 26, Loss: 0.0017211182275786996\n",
      "Epoch: 149, Iteration: 27, Loss: 0.0018611387349665165\n",
      "Epoch: 149, Iteration: 28, Loss: 0.0019890842959284782\n",
      "Epoch: 149, Iteration: 29, Loss: 0.0018293116008862853\n",
      "Epoch: 149, Iteration: 30, Loss: 0.0018920988077297807\n",
      "Epoch: 149, Iteration: 31, Loss: 0.0018218918703496456\n",
      "Epoch: 149, Iteration: 32, Loss: 0.0019159420626237988\n",
      "Epoch: 149, Iteration: 33, Loss: 0.0019404998747631907\n",
      "Epoch: 149, Iteration: 34, Loss: 0.001838678726926446\n",
      "Epoch: 149, Iteration: 35, Loss: 0.00176062504760921\n",
      "Epoch: 149, Iteration: 36, Loss: 0.0019428691593930125\n",
      "Epoch: 149, Iteration: 37, Loss: 0.00163132487796247\n",
      "Epoch: 149, Iteration: 38, Loss: 0.0018772039329633117\n",
      "Epoch: 149, Iteration: 39, Loss: 0.002045668661594391\n",
      "Epoch: 149, Iteration: 40, Loss: 0.001793897245079279\n",
      "Epoch: 149, Iteration: 41, Loss: 0.0021535372361540794\n",
      "Epoch: 149, Iteration: 42, Loss: 0.0018221749924123287\n",
      "Epoch: 149, Iteration: 43, Loss: 0.0019954931922256947\n",
      "Epoch: 149, Iteration: 44, Loss: 0.0019412108231335878\n",
      "Epoch: 149, Iteration: 45, Loss: 0.001874946872703731\n",
      "Epoch: 149, Iteration: 46, Loss: 0.0018469053320586681\n",
      "Epoch: 149, Iteration: 47, Loss: 0.001972789177671075\n",
      "Epoch: 149, Iteration: 48, Loss: 0.001909459475427866\n",
      "Epoch: 149, Iteration: 49, Loss: 0.0014962900895625353\n",
      "Epoch: 149, Iteration: 50, Loss: 0.0019827508367598057\n",
      "Epoch: 149, Iteration: 51, Loss: 0.0017609919887036085\n",
      "Epoch: 149, Iteration: 52, Loss: 0.0015787326265126467\n",
      "Epoch: 149, Iteration: 53, Loss: 0.001953249564394355\n",
      "Epoch: 149, Iteration: 54, Loss: 0.0019327637273818254\n",
      "Epoch: 149, Iteration: 55, Loss: 0.0016117135528475046\n",
      "Epoch: 149, Iteration: 56, Loss: 0.0017531728371977806\n",
      "Epoch: 149, Iteration: 57, Loss: 0.002159504685550928\n",
      "Epoch: 149, Iteration: 58, Loss: 0.001817319542169571\n",
      "Epoch: 149, Iteration: 59, Loss: 0.001730449846945703\n",
      "Epoch: 149, Iteration: 60, Loss: 0.00202229805290699\n",
      "Epoch: 149, Iteration: 61, Loss: 0.0019671504851430655\n",
      "Epoch: 149, Iteration: 62, Loss: 0.0020151929929852486\n",
      "Epoch: 149, Iteration: 63, Loss: 0.0018059349386021495\n",
      "Epoch: 149, Iteration: 64, Loss: 0.0016054578591138124\n",
      "Epoch: 149, Iteration: 65, Loss: 0.0018587440717965364\n",
      "Epoch: 149, Iteration: 66, Loss: 0.001735588302835822\n",
      "Epoch: 149, Iteration: 67, Loss: 0.0018166258232668042\n",
      "Epoch: 149, Iteration: 68, Loss: 0.0016695046797394753\n",
      "Epoch: 149, Iteration: 69, Loss: 0.0021516894921660423\n",
      "Epoch: 149, Iteration: 70, Loss: 0.0017382930964231491\n",
      "Epoch: 149, Iteration: 71, Loss: 0.0017119344556704164\n",
      "Epoch: 149, Iteration: 72, Loss: 0.0017126285238191485\n",
      "Epoch: 149, Iteration: 73, Loss: 0.0018761151004582644\n",
      "Epoch: 149, Iteration: 74, Loss: 0.0019465479999780655\n",
      "Epoch: 149, Iteration: 75, Loss: 0.0019969455897808075\n",
      "Epoch: 149, Iteration: 76, Loss: 0.0016485024243593216\n",
      "Epoch: 149, Iteration: 77, Loss: 0.0019071032293140888\n",
      "Epoch: 149, Iteration: 78, Loss: 0.0017480335664004087\n",
      "Epoch: 149, Iteration: 79, Loss: 0.0019386583007872105\n",
      "Epoch: 149, Iteration: 80, Loss: 0.0015966020291671157\n",
      "Epoch: 149, Iteration: 81, Loss: 0.0018248057458549738\n",
      "Epoch: 149, Iteration: 82, Loss: 0.0018452578224241734\n",
      "Epoch: 149, Iteration: 83, Loss: 0.0019098346820101142\n",
      "Epoch: 149, Iteration: 84, Loss: 0.002071297261863947\n",
      "Epoch: 149, Iteration: 85, Loss: 0.0018070372752845287\n",
      "Epoch: 149, Iteration: 86, Loss: 0.0020083002746105194\n",
      "Epoch: 149, Iteration: 87, Loss: 0.001986311748623848\n",
      "Epoch: 149, Iteration: 88, Loss: 0.0020243781618773937\n",
      "Epoch: 149, Iteration: 89, Loss: 0.0019140481017529964\n",
      "Epoch: 149, Iteration: 90, Loss: 0.0016446304507553577\n",
      "Epoch: 149, Iteration: 91, Loss: 0.0017876836936920881\n",
      "Epoch: 149, Iteration: 92, Loss: 0.0019744504243135452\n",
      "Epoch: 149, Iteration: 93, Loss: 0.0020289053209125996\n",
      "Epoch: 149, Iteration: 94, Loss: 0.0020296701695770025\n",
      "Epoch: 149, Iteration: 95, Loss: 0.0018776431679725647\n",
      "Epoch: 149, Iteration: 96, Loss: 0.002051320392638445\n",
      "Epoch: 149, Iteration: 97, Loss: 0.001938770292326808\n",
      "Epoch: 149, Iteration: 98, Loss: 0.001999568659812212\n",
      "Epoch: 149, Iteration: 99, Loss: 0.001895813038572669\n",
      "Epoch: 149, Iteration: 100, Loss: 0.0019203296396881342\n",
      "Epoch: 149, Iteration: 101, Loss: 0.0018513498362153769\n",
      "Epoch: 149, Iteration: 102, Loss: 0.001989375799894333\n",
      "Epoch: 149, Iteration: 103, Loss: 0.0016435077413916588\n",
      "Epoch: 149, Iteration: 104, Loss: 0.0019680187106132507\n",
      "Epoch: 149, Iteration: 105, Loss: 0.001824371051043272\n",
      "Epoch: 149, Iteration: 106, Loss: 0.0019388538785278797\n",
      "Epoch: 149, Iteration: 107, Loss: 0.001969794509932399\n",
      "Epoch: 149, Iteration: 108, Loss: 0.0016807292122393847\n",
      "Epoch: 149, Iteration: 109, Loss: 0.0020526389125734568\n",
      "Epoch: 149, Iteration: 110, Loss: 0.0020345416851341724\n",
      "Epoch: 149, Iteration: 111, Loss: 0.001715778955258429\n",
      "Epoch: 149, Iteration: 112, Loss: 0.0018170853145420551\n",
      "Epoch: 149, Iteration: 113, Loss: 0.0021339594386518\n",
      "Epoch: 149, Iteration: 114, Loss: 0.0018479094142094254\n",
      "Epoch: 149, Iteration: 115, Loss: 0.0018123578047379851\n",
      "Epoch: 149, Iteration: 116, Loss: 0.0021031296346336603\n",
      "Epoch: 149, Iteration: 117, Loss: 0.0021941233426332474\n",
      "Epoch: 149, Iteration: 118, Loss: 0.0019190971506759524\n",
      "Epoch: 149, Iteration: 119, Loss: 0.0020234135445207357\n",
      "Epoch: 149, Iteration: 120, Loss: 0.0018206874374300241\n",
      "Epoch: 149, Iteration: 121, Loss: 0.0019966140389442444\n",
      "Epoch: 149, Iteration: 122, Loss: 0.0018161232583224773\n",
      "Epoch: 149, Iteration: 123, Loss: 0.0017284646164625883\n",
      "Epoch: 149, Iteration: 124, Loss: 0.0017984184669330716\n",
      "Epoch: 149, Iteration: 125, Loss: 0.0017482105176895857\n",
      "Epoch: 149, Iteration: 126, Loss: 0.0017406062688678503\n",
      "Epoch: 149, Iteration: 127, Loss: 0.0019342211307957768\n",
      "Epoch: 149, Iteration: 128, Loss: 0.0016930033452808857\n",
      "Epoch: 149, Iteration: 129, Loss: 0.0021545730996876955\n",
      "Epoch: 149, Iteration: 130, Loss: 0.0018334672786295414\n",
      "Epoch: 149, Iteration: 131, Loss: 0.0017620609141886234\n",
      "Epoch: 149, Iteration: 132, Loss: 0.0016894624568521976\n",
      "Epoch: 149, Iteration: 133, Loss: 0.0019171267049387097\n",
      "Epoch: 149, Iteration: 134, Loss: 0.001730188261717558\n",
      "Epoch: 149, Iteration: 135, Loss: 0.0017733543645590544\n",
      "Epoch: 149, Iteration: 136, Loss: 0.0020056101493537426\n",
      "Epoch: 149, Iteration: 137, Loss: 0.0019475182052701712\n",
      "Epoch: 149, Iteration: 138, Loss: 0.0017980138072744012\n",
      "Epoch: 149, Iteration: 139, Loss: 0.0019108246779069304\n",
      "Epoch: 149, Iteration: 140, Loss: 0.0019485550001263618\n",
      "Epoch: 149, Iteration: 141, Loss: 0.0018422333523631096\n",
      "Epoch: 149, Iteration: 142, Loss: 0.0016628088196739554\n",
      "Epoch: 149, Iteration: 143, Loss: 0.002013307996094227\n",
      "Epoch: 149, Iteration: 144, Loss: 0.0020487618166953325\n",
      "Epoch: 149, Iteration: 145, Loss: 0.0017800682689994574\n",
      "Epoch: 149, Iteration: 146, Loss: 0.0018486811313778162\n",
      "Epoch: 149, Iteration: 147, Loss: 0.0019753160886466503\n",
      "Epoch: 149, Iteration: 148, Loss: 0.002061249688267708\n",
      "Epoch: 149, Iteration: 149, Loss: 0.0019373955437913537\n",
      "Epoch: 149, Iteration: 150, Loss: 0.0020343055948615074\n",
      "Epoch: 149, Iteration: 151, Loss: 0.0021513639949262142\n",
      "Epoch: 149, Iteration: 152, Loss: 0.001715363934636116\n",
      "Epoch: 149, Iteration: 153, Loss: 0.0016136611811816692\n",
      "Epoch: 149, Iteration: 154, Loss: 0.0016527855768799782\n",
      "Epoch: 149, Iteration: 155, Loss: 0.0016911110142245889\n",
      "Epoch: 149, Iteration: 156, Loss: 0.0016815660055726767\n",
      "Epoch: 149, Iteration: 157, Loss: 0.0019967590924352407\n",
      "Epoch: 149, Iteration: 158, Loss: 0.001783529995009303\n",
      "Epoch: 149, Iteration: 159, Loss: 0.0020130397751927376\n",
      "Epoch: 149, Iteration: 160, Loss: 0.0017071503680199385\n",
      "Epoch: 149, Iteration: 161, Loss: 0.001833008835092187\n",
      "Epoch: 149, Iteration: 162, Loss: 0.0016751773655414581\n",
      "Epoch: 149, Iteration: 163, Loss: 0.0018890156643465161\n",
      "Epoch: 149, Iteration: 164, Loss: 0.0019093987066298723\n",
      "Epoch: 149, Iteration: 165, Loss: 0.0019056805176660419\n",
      "Epoch: 149, Iteration: 166, Loss: 0.0019049628172069788\n",
      "Epoch: 149, Iteration: 167, Loss: 0.0018706789705902338\n",
      "Epoch: 149, Iteration: 168, Loss: 0.0017140032723546028\n",
      "Epoch: 149, Iteration: 169, Loss: 0.001782645471394062\n",
      "Epoch: 149, Iteration: 170, Loss: 0.0018948761280626059\n",
      "Epoch: 149, Iteration: 171, Loss: 0.00220239395275712\n",
      "Epoch: 149, Iteration: 172, Loss: 0.00200827419757843\n",
      "Epoch: 149, Iteration: 173, Loss: 0.0018288979772478342\n",
      "Epoch: 149, Iteration: 174, Loss: 0.002035417128354311\n",
      "Epoch: 149, Iteration: 175, Loss: 0.002031585667282343\n",
      "Epoch: 149, Iteration: 176, Loss: 0.002084905980154872\n",
      "Epoch: 149, Iteration: 177, Loss: 0.0019351253286004066\n",
      "Epoch: 149, Iteration: 178, Loss: 0.0019354374380782247\n",
      "Epoch: 149, Iteration: 179, Loss: 0.001899999799206853\n",
      "Epoch: 149, Iteration: 180, Loss: 0.0019667481537908316\n",
      "Epoch: 149, Iteration: 181, Loss: 0.0018542013131082058\n",
      "Epoch: 149, Iteration: 182, Loss: 0.0016831622924655676\n",
      "Epoch: 149, Iteration: 183, Loss: 0.002002051565796137\n",
      "Epoch: 149, Iteration: 184, Loss: 0.0018909634090960026\n",
      "Epoch: 149, Iteration: 185, Loss: 0.001794849755242467\n",
      "Epoch: 149, Iteration: 186, Loss: 0.0017203516326844692\n",
      "Epoch: 149, Iteration: 187, Loss: 0.0017331575509160757\n",
      "Epoch: 149, Iteration: 188, Loss: 0.0016885935328900814\n",
      "Epoch: 149, Iteration: 189, Loss: 0.0019732420332729816\n",
      "Epoch: 149, Iteration: 190, Loss: 0.0015418987022712827\n",
      "Epoch: 149, Iteration: 191, Loss: 0.0017169039929285645\n",
      "Epoch: 149, Iteration: 192, Loss: 0.00175506342202425\n",
      "Epoch: 149, Iteration: 193, Loss: 0.002119594719260931\n",
      "Epoch: 149, Iteration: 194, Loss: 0.0020736423321068287\n",
      "Epoch: 149, Iteration: 195, Loss: 0.0018657572800293565\n",
      "Epoch: 149, Iteration: 196, Loss: 0.0018335934728384018\n",
      "Epoch: 149, Iteration: 197, Loss: 0.0019327322952449322\n",
      "Epoch: 149, Iteration: 198, Loss: 0.0018300930969417095\n",
      "Epoch: 149, Iteration: 199, Loss: 0.0020412057638168335\n",
      "Epoch: 149, Iteration: 200, Loss: 0.002102506812661886\n",
      "Epoch: 149, Iteration: 201, Loss: 0.0018430788768455386\n",
      "Epoch: 149, Iteration: 202, Loss: 0.0018635897431522608\n",
      "Epoch: 149, Iteration: 203, Loss: 0.0018937976565212011\n",
      "Epoch: 149, Iteration: 204, Loss: 0.002193351974710822\n",
      "Epoch: 149, Iteration: 205, Loss: 0.0015164921060204506\n",
      "Epoch: 149, Iteration: 206, Loss: 0.0018959951121360064\n",
      "Epoch: 149, Iteration: 207, Loss: 0.002082135761156678\n",
      "Epoch: 149, Iteration: 208, Loss: 0.001892471918836236\n",
      "Epoch: 149, Iteration: 209, Loss: 0.0018197520403191447\n",
      "Epoch: 149, Iteration: 210, Loss: 0.0017093734350055456\n",
      "Epoch: 149, Iteration: 211, Loss: 0.0019234049832448363\n",
      "Epoch: 149, Iteration: 212, Loss: 0.0018758174264803529\n",
      "Epoch: 149, Iteration: 213, Loss: 0.0016155955381691456\n",
      "Epoch: 149, Iteration: 214, Loss: 0.0018111056415364146\n",
      "Epoch: 149, Iteration: 215, Loss: 0.0017765830270946026\n",
      "Epoch: 149, Iteration: 216, Loss: 0.001740629319101572\n",
      "Epoch: 149, Iteration: 217, Loss: 0.0018758452497422695\n",
      "Epoch: 149, Iteration: 218, Loss: 0.0020158207044005394\n",
      "Epoch: 149, Iteration: 219, Loss: 0.001963626593351364\n",
      "Epoch: 149, Iteration: 220, Loss: 0.0018263263627886772\n",
      "Epoch: 149, Iteration: 221, Loss: 0.001796539407223463\n",
      "Epoch: 149, Iteration: 222, Loss: 0.0019150243606418371\n",
      "Epoch: 149, Iteration: 223, Loss: 0.0016264510340988636\n",
      "Epoch: 149, Iteration: 224, Loss: 0.0018142135813832283\n",
      "Epoch: 149, Iteration: 225, Loss: 0.0019763025920838118\n",
      "Epoch: 149, Iteration: 226, Loss: 0.0017543141730129719\n",
      "Epoch: 149, Iteration: 227, Loss: 0.0017591590294614434\n",
      "Epoch: 149, Iteration: 228, Loss: 0.0018440504791215062\n",
      "Epoch: 149, Iteration: 229, Loss: 0.0019901711493730545\n",
      "Epoch: 149, Iteration: 230, Loss: 0.0019459251780062914\n",
      "Epoch: 149, Iteration: 231, Loss: 0.0018756925128400326\n",
      "Epoch: 149, Iteration: 232, Loss: 0.0018388342577964067\n",
      "Epoch: 149, Iteration: 233, Loss: 0.0018376590451225638\n",
      "Epoch: 149, Iteration: 234, Loss: 0.001958928070962429\n",
      "Epoch: 149, Iteration: 235, Loss: 0.0016962073277682066\n",
      "Epoch: 149, Iteration: 236, Loss: 0.0019184828270226717\n",
      "Epoch: 149, Iteration: 237, Loss: 0.0018207180546596646\n",
      "Epoch: 149, Iteration: 238, Loss: 0.0019975428003817797\n",
      "Epoch: 149, Iteration: 239, Loss: 0.0015921222511678934\n",
      "Epoch: 149, Iteration: 240, Loss: 0.0020365151576697826\n",
      "Epoch: 149, Iteration: 241, Loss: 0.0021340851671993732\n",
      "Epoch: 149, Iteration: 242, Loss: 0.001967241056263447\n",
      "Epoch: 149, Iteration: 243, Loss: 0.0020013630855828524\n",
      "Epoch: 149, Iteration: 244, Loss: 0.0018462357111275196\n",
      "Epoch: 149, Iteration: 245, Loss: 0.0020616124384105206\n",
      "Epoch: 149, Iteration: 246, Loss: 0.0018098449800163507\n",
      "Epoch: 149, Iteration: 247, Loss: 0.0019848207011818886\n",
      "Epoch: 149, Iteration: 248, Loss: 0.0018645257223397493\n",
      "Epoch: 149, Iteration: 249, Loss: 0.001885398756712675\n",
      "Epoch: 149, Iteration: 250, Loss: 0.0017637525452300906\n",
      "Epoch: 149, Iteration: 251, Loss: 0.001936918473802507\n",
      "Epoch: 149, Iteration: 252, Loss: 0.0021346048451960087\n",
      "Epoch: 149, Iteration: 253, Loss: 0.001839910983107984\n",
      "Epoch: 149, Iteration: 254, Loss: 0.0019159590592607856\n",
      "Epoch: 149, Iteration: 255, Loss: 0.0020148169714957476\n",
      "Epoch: 149, Iteration: 256, Loss: 0.0019921904895454645\n",
      "Epoch: 149, Iteration: 257, Loss: 0.0017169974744319916\n",
      "Epoch: 149, Iteration: 258, Loss: 0.0017248844960704446\n",
      "Epoch: 149, Iteration: 259, Loss: 0.0018754099728539586\n",
      "Epoch: 149, Iteration: 260, Loss: 0.0019466578960418701\n",
      "Epoch: 149, Iteration: 261, Loss: 0.0020426083356142044\n",
      "Epoch: 149, Iteration: 262, Loss: 0.0018132214900106192\n",
      "Epoch: 149, Iteration: 263, Loss: 0.0016845008358359337\n",
      "Epoch: 149, Iteration: 264, Loss: 0.001913082436658442\n",
      "Epoch: 149, Iteration: 265, Loss: 0.0019577257335186005\n",
      "Epoch: 149, Iteration: 266, Loss: 0.002121496247127652\n",
      "Epoch: 149, Iteration: 267, Loss: 0.001763461623340845\n",
      "Epoch: 149, Iteration: 268, Loss: 0.0016361558809876442\n",
      "Epoch: 149, Iteration: 269, Loss: 0.00177907501347363\n",
      "Epoch: 149, Iteration: 270, Loss: 0.0020490619353950024\n",
      "Epoch: 149, Iteration: 271, Loss: 0.001776711200363934\n",
      "Epoch: 149, Iteration: 272, Loss: 0.0019725002348423004\n",
      "Epoch: 149, Iteration: 273, Loss: 0.0019427788211032748\n",
      "Epoch: 149, Iteration: 274, Loss: 0.0020756477024406195\n",
      "Epoch: 149 Loss: 0.001876318178506364\n",
      "Generate the Best Latent Vectors\n",
      "Evaluating Batch: 0\n",
      "Evaluating Batch: 1\n",
      "Evaluating Batch: 2\n",
      "Evaluating Batch: 3\n",
      "Evaluating Batch: 4\n",
      "Evaluating Batch: 5\n",
      "Evaluating Batch: 6\n",
      "Evaluating Batch: 7\n",
      "Evaluating Batch: 8\n",
      "Evaluating Batch: 9\n",
      "Evaluating Batch: 10\n",
      "Evaluating Batch: 11\n",
      "Evaluating Batch: 12\n",
      "Evaluating Batch: 13\n",
      "Evaluating Batch: 14\n",
      "Evaluating Batch: 15\n",
      "Evaluating Batch: 16\n",
      "Evaluating Batch: 17\n",
      "Evaluating Batch: 18\n",
      "Evaluating Batch: 19\n",
      "Evaluating Batch: 20\n",
      "Evaluating Batch: 21\n",
      "Evaluating Batch: 22\n",
      "Evaluating Batch: 23\n",
      "Evaluating Batch: 24\n",
      "Evaluating Batch: 25\n",
      "Evaluating Batch: 26\n",
      "Evaluating Batch: 27\n",
      "Evaluating Batch: 28\n",
      "Evaluating Batch: 29\n",
      "Evaluating Batch: 30\n",
      "Evaluating Batch: 31\n",
      "Evaluating Batch: 32\n",
      "Evaluating Batch: 33\n",
      "Evaluating Batch: 34\n",
      "Evaluating Batch: 35\n",
      "Evaluating Batch: 36\n",
      "Evaluating Batch: 37\n",
      "Evaluating Batch: 38\n",
      "Evaluating Batch: 39\n",
      "Evaluating Batch: 40\n",
      "Evaluating Batch: 41\n",
      "Evaluating Batch: 42\n",
      "Evaluating Batch: 43\n",
      "Evaluating Batch: 44\n",
      "Evaluating Batch: 45\n",
      "Evaluating Batch: 46\n",
      "Evaluating Batch: 47\n",
      "Evaluating Batch: 48\n",
      "Evaluating Batch: 49\n",
      "Evaluating Batch: 50\n",
      "Evaluating Batch: 51\n",
      "Evaluating Batch: 52\n",
      "Evaluating Batch: 53\n",
      "Evaluating Batch: 54\n",
      "Evaluating Batch: 55\n",
      "Evaluating Batch: 56\n",
      "Evaluating Batch: 57\n",
      "Evaluating Batch: 58\n",
      "Evaluating Batch: 59\n",
      "Evaluating Batch: 60\n",
      "Evaluating Batch: 61\n",
      "Evaluating Batch: 62\n",
      "Evaluating Batch: 63\n",
      "Evaluating Batch: 64\n",
      "Evaluating Batch: 65\n",
      "Evaluating Batch: 66\n",
      "Evaluating Batch: 67\n",
      "Evaluating Batch: 68\n",
      "Evaluating Batch: 69\n",
      "Evaluating Batch: 70\n",
      "Evaluating Batch: 71\n",
      "Evaluating Batch: 72\n",
      "Evaluating Batch: 73\n",
      "Evaluating Batch: 74\n",
      "Evaluating Batch: 75\n",
      "Evaluating Batch: 76\n",
      "Evaluating Batch: 77\n",
      "Evaluating Batch: 78\n",
      "Evaluating Batch: 79\n",
      "Evaluating Batch: 80\n",
      "Evaluating Batch: 81\n",
      "Evaluating Batch: 82\n",
      "Evaluating Batch: 83\n",
      "Evaluating Batch: 84\n",
      "Evaluating Batch: 85\n",
      "Evaluating Batch: 86\n",
      "Evaluating Batch: 87\n",
      "Evaluating Batch: 88\n",
      "Evaluating Batch: 89\n",
      "Evaluating Batch: 90\n",
      "Evaluating Batch: 91\n",
      "Evaluating Batch: 92\n",
      "Evaluating Batch: 93\n",
      "Evaluating Batch: 94\n",
      "Evaluating Batch: 95\n",
      "Evaluating Batch: 96\n",
      "Evaluating Batch: 97\n",
      "Evaluating Batch: 98\n",
      "Evaluating Batch: 99\n",
      "Evaluating Batch: 100\n",
      "Evaluating Batch: 101\n",
      "Evaluating Batch: 102\n",
      "Evaluating Batch: 103\n",
      "Evaluating Batch: 104\n",
      "Evaluating Batch: 105\n",
      "Evaluating Batch: 106\n",
      "Evaluating Batch: 107\n",
      "Evaluating Batch: 108\n",
      "Evaluating Batch: 109\n",
      "Evaluating Batch: 110\n",
      "Evaluating Batch: 111\n",
      "Evaluating Batch: 112\n",
      "Evaluating Batch: 113\n",
      "Evaluating Batch: 114\n",
      "Evaluating Batch: 115\n",
      "Evaluating Batch: 116\n",
      "Evaluating Batch: 117\n",
      "Evaluating Batch: 118\n",
      "Evaluating Batch: 119\n",
      "Evaluating Batch: 120\n",
      "Evaluating Batch: 121\n",
      "Evaluating Batch: 122\n",
      "Evaluating Batch: 123\n",
      "Evaluating Batch: 124\n",
      "Evaluating Batch: 125\n",
      "Evaluating Batch: 126\n",
      "Evaluating Batch: 127\n",
      "Evaluating Batch: 128\n",
      "Evaluating Batch: 129\n",
      "Evaluating Batch: 130\n",
      "Evaluating Batch: 131\n",
      "Evaluating Batch: 132\n",
      "Evaluating Batch: 133\n",
      "Evaluating Batch: 134\n",
      "Evaluating Batch: 135\n",
      "Evaluating Batch: 136\n",
      "Evaluating Batch: 137\n",
      "Evaluating Batch: 138\n",
      "Evaluating Batch: 139\n",
      "Evaluating Batch: 140\n",
      "Evaluating Batch: 141\n",
      "Evaluating Batch: 142\n",
      "Evaluating Batch: 143\n",
      "Evaluating Batch: 144\n",
      "Evaluating Batch: 145\n",
      "Evaluating Batch: 146\n",
      "Evaluating Batch: 147\n",
      "Evaluating Batch: 148\n",
      "Evaluating Batch: 149\n",
      "Evaluating Batch: 150\n",
      "Evaluating Batch: 151\n",
      "Evaluating Batch: 152\n",
      "Evaluating Batch: 153\n",
      "Evaluating Batch: 154\n",
      "Evaluating Batch: 155\n",
      "Evaluating Batch: 156\n",
      "Evaluating Batch: 157\n",
      "Evaluating Batch: 158\n",
      "Evaluating Batch: 159\n",
      "Evaluating Batch: 160\n",
      "Evaluating Batch: 161\n",
      "Evaluating Batch: 162\n",
      "Evaluating Batch: 163\n",
      "Evaluating Batch: 164\n",
      "Evaluating Batch: 165\n",
      "Evaluating Batch: 166\n",
      "Evaluating Batch: 167\n",
      "Evaluating Batch: 168\n",
      "Evaluating Batch: 169\n",
      "Evaluating Batch: 170\n",
      "Evaluating Batch: 171\n",
      "Evaluating Batch: 172\n",
      "Evaluating Batch: 173\n",
      "Evaluating Batch: 174\n",
      "Evaluating Batch: 175\n",
      "Evaluating Batch: 176\n",
      "Evaluating Batch: 177\n",
      "Evaluating Batch: 178\n",
      "Evaluating Batch: 179\n",
      "Evaluating Batch: 180\n",
      "Evaluating Batch: 181\n",
      "Evaluating Batch: 182\n",
      "Evaluating Batch: 183\n",
      "Evaluating Batch: 184\n",
      "Evaluating Batch: 185\n",
      "Evaluating Batch: 186\n",
      "Evaluating Batch: 187\n",
      "Evaluating Batch: 188\n",
      "Evaluating Batch: 189\n",
      "Evaluating Batch: 190\n",
      "Evaluating Batch: 191\n",
      "Evaluating Batch: 192\n",
      "Evaluating Batch: 193\n",
      "Evaluating Batch: 194\n",
      "Evaluating Batch: 195\n",
      "Evaluating Batch: 196\n",
      "Evaluating Batch: 197\n",
      "Evaluating Batch: 198\n",
      "Evaluating Batch: 199\n",
      "Evaluating Batch: 200\n",
      "Evaluating Batch: 201\n",
      "Evaluating Batch: 202\n",
      "Evaluating Batch: 203\n",
      "Evaluating Batch: 204\n",
      "Evaluating Batch: 205\n",
      "Evaluating Batch: 206\n",
      "Evaluating Batch: 207\n",
      "Evaluating Batch: 208\n",
      "Evaluating Batch: 209\n",
      "Evaluating Batch: 210\n",
      "Evaluating Batch: 211\n",
      "Evaluating Batch: 212\n",
      "Evaluating Batch: 213\n",
      "Evaluating Batch: 214\n",
      "Evaluating Batch: 215\n",
      "Evaluating Batch: 216\n",
      "Evaluating Batch: 217\n",
      "Evaluating Batch: 218\n",
      "Evaluating Batch: 219\n",
      "Evaluating Batch: 220\n",
      "Evaluating Batch: 221\n",
      "Evaluating Batch: 222\n",
      "Evaluating Batch: 223\n",
      "Evaluating Batch: 224\n",
      "Evaluating Batch: 225\n",
      "Evaluating Batch: 226\n",
      "Evaluating Batch: 227\n",
      "Evaluating Batch: 228\n",
      "Evaluating Batch: 229\n",
      "Evaluating Batch: 230\n",
      "Evaluating Batch: 231\n",
      "Evaluating Batch: 232\n",
      "Evaluating Batch: 233\n",
      "Evaluating Batch: 234\n",
      "Evaluating Batch: 235\n",
      "Evaluating Batch: 236\n",
      "Evaluating Batch: 237\n",
      "Evaluating Batch: 238\n",
      "Evaluating Batch: 239\n",
      "Evaluating Batch: 240\n",
      "Evaluating Batch: 241\n",
      "Evaluating Batch: 242\n",
      "Evaluating Batch: 243\n",
      "Evaluating Batch: 244\n",
      "Evaluating Batch: 245\n",
      "Evaluating Batch: 246\n",
      "Evaluating Batch: 247\n",
      "Evaluating Batch: 248\n",
      "Evaluating Batch: 249\n",
      "Evaluating Batch: 250\n",
      "Evaluating Batch: 251\n",
      "Evaluating Batch: 252\n",
      "Evaluating Batch: 253\n",
      "Evaluating Batch: 254\n",
      "Evaluating Batch: 255\n",
      "Evaluating Batch: 256\n",
      "Evaluating Batch: 257\n",
      "Evaluating Batch: 258\n",
      "Evaluating Batch: 259\n",
      "Evaluating Batch: 260\n",
      "Evaluating Batch: 261\n",
      "Evaluating Batch: 262\n",
      "Evaluating Batch: 263\n",
      "Evaluating Batch: 264\n",
      "Evaluating Batch: 265\n",
      "Evaluating Batch: 266\n",
      "Evaluating Batch: 267\n",
      "Evaluating Batch: 268\n",
      "Evaluating Batch: 269\n",
      "Evaluating Batch: 270\n",
      "Evaluating Batch: 271\n",
      "Evaluating Batch: 272\n",
      "Evaluating Batch: 273\n",
      "Evaluating Batch: 274\n",
      "Performing Clustering...\n",
      "Clusters file generated\n",
      "Writing to Tensorboard\n",
      "the end.\n"
     ]
    }
   ],
   "source": [
    "! python train.py --batch_size 64 \\\n",
    "                  --num_points 2048 \\\n",
    "                  --num_workers 4 \\\n",
    "                  --nepoch 150 \\\n",
    "                  --model_type fxia \\\n",
    "                  --dataset_path /content/drive/My\\ Drive/Projects/dataset/shapenet_pcd \\\n",
    "                  --latent_vector \"\" \\\n",
    "                  --filenames \"\" 2>&1 | tee train_fxia_shapenet.log \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMSQ_TGoynDW"
   },
   "source": [
    "Perform Clustering using Mean Shift Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlW6qY2AB670"
   },
   "outputs": [],
   "source": [
    "! cat train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "colab_type": "code",
    "id": "ylc38h9jyxcS",
    "outputId": "64cabd03-934e-4758-8574-d91fc98f28b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Input Arguments : Namespace(batch_size=0, dataset_path='/content/drive/My Drive/Projects/dataset/shapenet_pcd', filenames='/content/drive/My Drive/Projects/clustering/clustering_3d_data/saved_models_bkp/best_filenames_142.pth', latent_vector='/content/drive/My Drive/Projects/clustering/clustering_3d_data/saved_models_bkp/best_latent_vector_142.pth', load_saved_model='', model_type='fxia', nepoch=0, num_points=0, num_workers=0, only_clustering=True, start_epoch_from=0)\n",
      "Random Seed:  7133\n",
      "Performing Clustering...\n",
      "Clusters file generated\n",
      "Writing to Tensorboard\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 207, in <module>\n",
      "    points = pointutil.normalize(points)\n",
      "  File \"/content/drive/My Drive/Projects/clustering/clustering_3d_data/util/pointutil.py\", line 7, in normalize\n",
      "    dist = np.max(np.sqrt(np.sum(iPoints ** 2, axis = 1)),0)\n",
      "  File \"<__array_function__ internals>\", line 6, in amax\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 2668, in amax\n",
      "    keepdims=keepdims, initial=initial, where=where)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 90, in _wrapreduction\n",
      "    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "ValueError: zero-size array to reduction operation maximum which has no identity\n"
     ]
    }
   ],
   "source": [
    "! python train.py --batch_size 0 \\\n",
    "                  --num_points 0 \\\n",
    "                  --num_workers 0 \\\n",
    "                  --nepoch 0 \\\n",
    "                  --model_type fxia \\\n",
    "                  --dataset_path /content/drive/My\\ Drive/Projects/dataset/shapenet_pcd \\\n",
    "                  --only_clustering \\\n",
    "                  --latent_vector \"/content/drive/My Drive/Projects/clustering/clustering_3d_data/saved_models_bkp/best_latent_vector_142.pth\" \\\n",
    "                  --filenames \"/content/drive/My Drive/Projects/clustering/clustering_3d_data/saved_models_bkp/best_filenames_142.pth\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "jWhF8Uw3m0_B",
    "outputId": "cc62d825-b4bb-4627-fff8-82987c565191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gputil\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
      "Building wheels for collected packages: gputil\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=f5e49bb460be304f5bf136207c526c7f83b4f4a92853ca0f8f37de6732480306\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
      "Successfully built gputil\n",
      "Installing collected packages: gputil\n",
      "Successfully installed gputil-1.4.0\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Gen RAM Free: 12.6 GB  | Proc size: 160.4 MB\n",
      "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
     ]
    }
   ],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnt guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KihVXbNlm_2g"
   },
   "outputs": [],
   "source": [
    "#if utilization is greater than 0% try to kill your machine, using the code below.\n",
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "colab_type": "code",
    "id": "-5k5D5pS4xsP",
    "outputId": "c765f300-5ea5-43f5-977b-e899ac77e96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwx------ 3 root root 4096 May 22 10:03 chamfer_distance\n",
      "drwx------ 3 root root 4096 May 22 10:03 dataprep\n",
      "-rw------- 1 root root 3603 May 22 09:58 infer.py\n",
      "drwx------ 3 root root 4096 May 22 10:03 model\n",
      "drwx------ 2 root root 4096 May 22 10:37 saved_models\n",
      "-rw------- 1 root root 3972 May 22 10:33 train.py\n"
     ]
    }
   ],
   "source": [
    "! ls -l  | tee /content/sample.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m31uYmL4zG1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPcWhanxAZuRnlZmyrJLmjh",
   "collapsed_sections": [
    "-paxNdU0Z-J7"
   ],
   "include_colab_link": true,
   "name": "Clustering_3D.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
